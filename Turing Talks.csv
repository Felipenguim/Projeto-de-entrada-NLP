url,title,subtitle,author,claps,reading_time,date,text
https://medium.com/turing-talks/turing-talks-1-o-que-%C3%A9-o-teste-de-turing-ee656ced7b6?source=collection_home---------110----------------------------,O que é o Teste de Turing?,Entenda a lógica por trás do teste proposto pelo grande matemático e cientista da computação Alan Turing!,Rodrigo Magaldi,534,5,2019-02-17,"Consideremos a seguinte questão: as máquinas conseguem pensar? É com essa instigante indagação que o matemático Alan Turing inicia o trabalho Computing Machinery and Intelligence (Máquinas Computacionais e Inteligência, em tradução livre), escrito por ele em 1950. Turing defende a criação de um teste hipotético para analisar se um sistema computacional é ou não inteligente como um ser humano, conhecido popularmente como Teste de Turing. Mas, antes de tentar responder a esse pensamento mirabolante do pai da ciência da computação, vamos entender um jogo que o próprio Turing propõe — O Jogo da Imitação. Suponhamos que haja três jogadores: Vamos agora aos objetivos: É preciso que a forma de comunicação utilizada não dê qualquer indicação de gênero (como o uso da voz). Pode ser utilizada, por exemplo, uma máquina de escrever para redigir as respostas. Interessante, não? Um homem deve ser capaz de imitar o comportamento e as respostas de uma mulher, e o contrário também seria possível. Voltemos, então, à pergunta feita por Turing. Será que elas conseguem? Vamos investigar juntos. Em que você está pensando agora? Talvez você esteja tentando acompanhar o texto, tentando entender onde eu quero chegar com tudo o que estou falando. Ou, então, talvez você esteja pensando num prazo importante que você tem para a entrega de um trabalho. Mas, se você estiver realmente muito empenhado na leitura, deve estar pensando, quase que filosoficamente: O que é pensar? Como podemos definir este comportamento para uma máquina? Alguns dicionários apostam em algumas definições: Mas essas, com toda certeza, não são todas as definições possíveis. De fato, é um trabalho árduo tentar entender profundamente o que significa pensar. Que perda de tempo, definir o que é pensar, pensa você. Genial! Alan Turing pensou o mesmo! Em vez de perder tempo numa discussão moral-filosófica acerca do que é pensar, Turing seguiu outro rumo e se perguntou: Uma máquina consegue imitar o comportamento humano? Já entendeu onde eu quero chegar, né? Voltamos, então, ao Jogo da Imitação! Mudemos apenas alguns fatores: Temos, agora, o Teste de Turing: um teste capaz de concluir se um computador (jogador A) é inteligente. Conseguindo enganar o interrogador e convencê-lo de que é humana, a máquina poderia ser considerada inteligente. O teste proposto traz uma das ideias mais primitivas acerca do que é inteligência artificial (IA), objeto de estudo do Grupo Turing. Trata-se um algoritmo conversacional, baseado em uma área específica dentro do campo de IA, chamada de Processamento de Linguagem Natural. O nome faz bastante sentido — o computador deve entender exatamente o que está sendo demandado dele, mesmo sem instruções diretamente programadas, e responder de forma a imitar a linguagem natural de uma conversação humana. É essa a lógica por detrás dos chamados chatbots que existem de forma bastante comum atualmente. Você sabe, aqueles assistentes virtuais que todo site de uma grande loja de venda on-line tem, que se disponibiliza para tirar suas dúvidas acerca do site, da compra, da entrega e tudo o que você conseguir imaginar. O uso de chatbots aumenta muito a eficiência do setor de atendimento das empresas, pois o gasto de recursos das empresas é drasticamente reduzido quando não é necessário o uso de empregados para responder a perguntas simples que eventuais clientes possam ter. O Teste de Turing pode parecer, à primeira vista, uma boa maneira de classificar máquinas como inteligentes ou não, mas na verdade é uma grande simplificação para o que se entende como Inteligência Artificial hoje em dia. Antes de tudo, precisamos nos lembrar da pergunta que Turing tentou responder. O teste não conclui se máquinas podem pensar, mas sim se podem ser confundidas com pessoas, isto é, mimicar uma conversação humana. Assim, a avaliação do teste depende quase que inteiramente das perguntas feitas ao computador dentro do período de tempo estipulado para tal (em seu trabalho, Turing nunca especificou um limite de tempo, mas com o advento dos chatbots foram criadas competições que tiveram de ser regulamentadas). Além disso, se está avaliando a inteligência da máquina como sua capacidade de comunicar-se com seres humanos como se fosse um deles, apenas. No entanto, o estudo de inteligência artificial não se resume e não se limita ao processamento de linguagem natural: existe o processamento de imagens, vídeos, sons, e até aprendizado por reforço — áreas de estudo também muito importantes e que permitem resultados muito impressionantes. Por exemplo, é mais inteligente um programa que é capaz de conversar com um humano naturalmente ou um que consegue vencer o campeão mundial de xadrez, resultado este alcançado com aprendizado por reforço? Ainda temos um longo caminho pela frente na tentativa de gerar algoritmos conversacionais 100% capazes de se passarem por humanos. Devemos pensar, também, se queremos chegar lá — quais são as implicações éticas que isso poderia gerar? O que todos sabemos é que aquilo que já temos nos dias de hoje é bastante impressionante, e que a inteligência artificial não para de nos surpreender. É um campo de estudo muito amplo e que tem muitas frentes. Mas não se apavore: podemos aprender juntos! Nas próximas semanas, continuam as Turing Talks com mais assuntos para você ficar por dentro do mundo da IA! E, se você estiver interessado mesmo em saber mais, siga nossa página no Facebook e fique ligado no que postamos por lá também. Por hoje é só, e aquele abraço!"
https://medium.com/turing-talks/turing-talks-2-o-que-%C3%A9-machine-learning-b7e7654a86f2?source=collection_home---------109----------------------------,O que é Machine Learning?,,Rodrigo Estevam,441,5,2019-04-28,"Olá grandes entusiastas de Inteligência Artificial! Eu sou o Rodrigo e te acompanharei hoje no nosso Turing Talks #2, em continuação a nossa série de posts sobre IA (leiam o post anterior, é bem legal). Hoje a minha missão é fazer vocês entenderem o básico sobre o que é Machine Learning e quem sabe dar um pontapé inicial para fazer vocês pesquisarem a fundo sobre esse assunto e começar a estudar os algoritmos. Machine Learning (Aprendizado de Máquina, em português) é um ramo de IA que estuda a automatização da criação de modelos com base na análise de dados. Ou seja, a máquina aprende sozinha, sem que ela seja explicitamente programada a priori, a fazer sua “tarefa” analisando os dados que ela tem a seu dispor. Eu defini como “tarefa” algo que é muito abrangente; dependendo dos tipos de dados e como eles estão organizados é possível fazer muitas coisas diferentes. Machine Learning está inserido no seu cotidiano mais do que você pensa! A Siri e a Google Assistent, os filtros bonitinhos de cachorrinho do Instagram e até o catálogo de recomendações do Netflix, utilizam algoritmos de aprendizado de máquina. Para nos ajudar a entender todas essas possibilidades de atuação, estruturarei esta explicação por tipo de aprendizado: aprendizado supervisionado, não supervisionado e por reforço. Primeiramente, para que esse tipo de aprendizado seja possível é preciso ter uma base de dados rotulada (labeled dataset), que é uma “tabela” com colunas contendo características (features) e uma sendo o nosso objetivo (target). Prometemos fazer mais posts sobre Data Science futuramente, assim vocês entenderão melhor como é um dataset. Uma boa analogia para entender esse tipo de aprendizado é pensar que a máquina tem um professor. Como um professor da vida real, ele resolve alguns exemplos para que ela aprenda e consiga generalizar para todos os tipos de situações. Como o nosso dataset é rotulado, ele já vem com as respostas do que a gente quer prever para alguns casos. O que o modelo deve fazer é aprender com esses casos, para que generalizando, ela consiga predizer/classificar a resposta de um caso que ele ainda não tenha visto. Dizemos que é uma classificação quando o nosso objetivo é classificar um caso dentre algumas opções. Um bom exemplo disso é uma aplicação em um dataset bem famoso para iniciantes. No IRIS (http://archive.ics.uci.edu/ml/datasets/iris) é fornecida para cada exemplo características de sua estrutura: comprimento e largura da pétala e da sépala, além de sua espécie que pode ser versicolor, setosa ou virginica. Com isso é possível criar modelos classificatórios que predizem a espécie da flor com base nas suas características. Outro grande exemplo são as classificações de imagens! Elas são muito usadas em sistemas de segurança e até nos filtros de Instagram que citamos acima (para a detecção do rosto de pessoas). Mas e quando o que tentamos acertar não é uma categoria e sim um valor contínuo? Nesse caso precisamos de um modelo de regressão. É o caso de problemas como acertar o preço de uma casa com base em onde é localizada e sua estrutura ou encontrar a idade de uma pessoa usando informações sobre a sua fisionomia. O que é feito é achar uma função que melhor aproxima aquela descrita pelos dados. Mas como nem sempre os dados descrevem uma função “simples” ou “bonita”, é muito difícil criar um modelo perfeito para esse tipo de problema. No aprendizado não supervisionado não se tem os rótulos para a base, ou seja, ele ocorre sem que o algoritmo tenha a resposta para os casos. São usados para clusterização e redução de dimensionalidade. Na clusterização, o que é feito é agrupar esses casos em grupos que pareçam ser mais semelhantes (clusters), com base nas características que eles apresentam. No algoritmos de redução de dimensionalidade a ideia é diminuir a complexidade dos dados escolhendo uma melhor forma de representá los, assim uma análise futura dos dados fica bem mais simples. O algoritmo da imagem abaixo utiliza métodos de álgebra linear para escolher eixos coordenados que melhor representam os dados. O aprendizado por reforço é bem diferente dos que citamos acima, pois nele não é necessário um dataset para treinar o modelo. A ideia é que o modelo seja um agente que aprende ao interagir com um ambiente, o qual lhe fornece uma resposta (recompensa) para cada ação tomada. Então o algoritmo começa indo mal e, percebendo isso, com o tempo vai descobrindo maneiras de se sair melhor, ou seja, de obter uma recompensa maior. Assim ele vai sempre tentando melhorar para maximizar seu ganho. Exemplos de IAs que usam aprendizado por reforço são aquelas que jogam jogos. Uma que ficou muito famosa foi a AlphaGo que conseguiu ganhar do campeão mundial de Go (jogo de tabuleiro) . Aliás, há um documentário no Netflix sobre ela. Outro campo em que esse tipo de aprendizado é muito utilizado é na robótica. Tarefas simples para nós seres humanos, como andar, pegar objetos ou até virar uma panqueca na frigideira, são bem complicadas de serem feitas com programação “clássica”. Então, por meio de simulações ou tentativas no mundo real, é possível treinar esses robôs de maneira que eles aprendam tentando. Há tanto mais a ser explorado em machine learning que seria cansativo citar tudo nesse texto, mas pretendemos continuar essa série explorando esse tema tão interessante. Por hora, espero que tenham entendido o básico do assunto e gostado desse pequeno texto. Se você estiver mesmo interessado em saber mais sobre, além de continuar nos acompanhando aqui, siga nossa página no Facebook e fique ligado nas postagens. Ficamos por aqui, Grande abraço!"
https://medium.com/turing-talks/turing-talks-3-carcinoma-hepatocelular-128a20697854?source=collection_home---------108----------------------------,Aplicações | Carcinoma Hepatocelular,"Como a Inteligência Artificial pode ajudar na área da saúde, predizendo resultados e escolhendo o melhor tratamento.",Felipe Augusto de Moraes Machado,600,5,2019-05-12,"Imagine a seguinte situação: você possui carcinoma hepatocelular (um tipo de câncer que ocorre no fígado) e quer tratá-lo. Um dos tratamentos é por Radiofrequência, no qual colocam em você uma agulha (que custa em torno de 5 mil reais) que emite algumas ondas em certas frequências, destruindo todas as células do carcinoma (e outras que estiverem no caminho). E, depois de toda essa dor e esse custo, o câncer volta. Como podemos prevenir isso? Como podemos predizer o resultado desse tratamento? Esse tipo de caso é bem comum, e é um problema bem complexo para tentarmos resolver de alguma forma convencional. Por isso, uma das saídas que alguns médicos estão escolhendo é usar Inteligência Artificial para ajudá-los. Nós possuímos um projeto do Carcinoma citado acima para predizer esse tratamento. Uma das tarefas que uma IA consegue realizar é o reconhecimento de padrões. Com uma quantidade grande de imagens de ressonância magnética, por exemplo, podemos criar um modelo de IA que seja capaz de predizer qual o resultado de algum tratamento. Infelizmente, na prática, não é tão simples assim. Um exame de ressonância magnética é composto por diversas imagens, por exemplo, 100 imagens de 512 x 512 pixeis (imagens são matrizes com valores da intensidade luminosa em cada pixel), contendo assim 26.214.400 pixeis. Se o modelo possuir apenas um parâmetro para cada pixel, teremos 26.214.400 parâmetros, um modelo muito complexo que precisa de muitos dados para funcionar, e normalmente não temos essa quantidade de casos necessários. Como proceder? Bom, se não podemos usar as imagens diretas em algum modelo, que tal utilizarmos outra abordagem? Um outro método bem conhecido é procurar características da imagem, por exemplo: tirar a média dos valores da imagem, o desvio padrão, contraste, correlação, entre outros. Essas características são chamadas de features e elas ajudam a explicar a imagem usando poucos números (lembre que um exame de 100 x 512 x 512 tem ao todo 26.214.400 números). No nosso projeto Carcinoma, calculamos 4455 features usando uma Biblioteca da linguagem R chamada RIA. Reduzir de 26.214.400 para 4455 reduz o número de parâmetros e a complexidade do nosso modelo, mas ainda assim é um número alto. Após calcular os features da imagem, devemos selecionar os features que explicam melhor o problema: essa etapa chamamos de Feature Selection. Há diversas formas de selecionar, mas o mais comum é usando AUC (“area under the ROC curve”). Esse AUC é um valor entre 0 e 1, e ele indica o quão importante é um feature para o problema (quanto mais próximo de 1, mais importante ele é, 0.5 indica que o feature não é relevante). Selecionando os features com maior importância (com AUC > 0.85, por exemplo), reduzimos a quantidade de features para 20, 10 ou até menos. Depois desses processos, obteremos um conjunto de dados com dimensão bem menor, formado apenas por features relevantes ao problema. Agora que possuímos dados pequenos, vamos para a parte final da nossa tarefa: criar um modelo. Agora temos que decidir o que queremos fazer: um modelo que classifique se o tratamento vai funcionar ou um modelo que retorne a probabilidade do tratamento funcionar. São dois tipos de modelo diferente para análises diferentes, o primeiro é de classificação enquanto o segundo é de regressão. Embora existam diversos modelos para ambas tarefas, vamos falar sobre os mais comuns nessa área: Regressão Logística e Random Forest (esse funciona tanto para regressão quanto pra classificação). A Regressão Logística é uma regressão que nos retorna a probabilidade de um evento ocorrer baseado nos dados de entrada. Ele é muito usado em artigos médicos pois seus parâmetros podem ser usados para entender a importância dos features entre eles (a média pode ser 10x mais importante que o desvio padrão, por exemplo). Random Forest é outra ferramenta muito utilizada, pois ela possui um funcionamento não-linear, embora seja um pouco mais complexa (possui mais parâmetros) comparado à regressão logística. Ela utiliza diversas árvores de decisão (uma para cada feature) para classificar um caso ou para retornar um valor contínuo (regressão). As árvores informam também qual a importância de cada feature no problema. Após criar o modelo, é necessário testá-lo e verificar se o seu funcionamento cumpre suas necessidades. Quando estamos trabalhando com classificador, podemos analisar o seu desempenho baseado em algumas métricas: recall/sensitivity, specificity e precision. Essas métricas são calculadas a partir da quantidade de Falsos Positivos (casos negativos classificados como positivos), Falsos Negativos (casos positivos classificados como negativos), Verdadeiros Positivos e Verdadeiros Negativos. No nosso problema, queremos saber se o tratamento vai funcionar (caso positivo) ou se ele não vai destruir o carcinoma permanentemente (caso negativo), sendo fornecido outro tipo de tratamento nesse segundo caso. Então o nosso modelo pode apresentar alguns Falsos Negativos (o paciente só mudará de tratamento), mas ele não deve apresentar Falsos Positivos (o modelo falar que o tratamento vai funcionar sendo que na verdade não vai). Em outras palavras, para esse problema, devemos ter um Precision alto. Nesse texto, abordamos uma forma convencional de como usar IA nessa área médica. Ela é a única? Claro que não, mas é a mais utilizada. É uma área interdisciplinar nova, então existe muito ainda a ser explorado, muito a ser criado. Cada vez mais se encontra artigos utilizando IA para ajudar em tratamento ou para entender mais sobre um certo problema. Então, temos que continuar a pesquisar, testar e aprender mais sobre para conseguirmos ter resultados melhores! E esse foi o Turing Talks dessa semana. Se você achou interessante o tema dessa semana, siga a nossa página no facebook e acompanhe nossas publicações, vai encontrar só eventos/projetos legais. Isso é tudo por hoje pessoal, um grande abraço!"
https://medium.com/turing-talks/turing-talks-4-python-parte-1-29b8d9efd0a5?source=collection_home---------107----------------------------,Programação | Python — Parte 1,,Lucas Fentanes Machado,731,9,2019-05-19,"Uma introdução a uma das linguagens de programação mais utilizadas no mundo Provavelmente você já ouviu falar sobre Python. Em alguma notícia, post ou em alguma aula ou videoaula, por exemplo. Ainda mais agora, com o enorme crescimento de áreas como Data Science e Machine Learning. Neste meio, o Python figura como uma das principais e mais usadas ferramentas para desenvolvimento de tecnologias deste tipo. Além disso, é uma linguagem de fácil compreensão e com uma sintaxe muito mais leve do que, por exemplo, o C. O Python é uma linguagem de programação de alto nível interpretada (e não compilada), que dá suporte a coisas como orientação a objetos e até programação funcional, ou seja, o programador pode escolher qual paradigma de programação irá usar, diferentemente de linguagens como Java ou C. Isso fornece uma enorme liberdade ao programador para variar entre vários “estilos”. O objetivo deste post não é descrever ou realizar uma comparação generalista entre linguagens de programação, mas sim servir como uma introdução ao seu aprendizado de Python. Este texto poderá servir como uma ótima ferramenta para a leitora ou leitor que deseja aprender esta linguagem, entretanto não sabe por onde começar. É recomendado um conhecimento básico de lógica de programação e talvez alguma pequena experiência com outra linguagem no passado. (Caso o leitor não possua nenhum dois dois, sugiro que leia mesmo assim). Para que o tutorial não ficasse tão longo, foi divido em duas partes. Partiremos de conceitos básicos como quais são os tipos de variáveis, como printar coisas na tela, como fazer loops (se você ainda não sabe o que é isso, não se preocupe, explicaremos na próxima parte do tutorial), até outros mais complexos, como bibliotecas e frameworks muito úteis para aplicações. Bom, vamos ao tutorial! IMPORTANTE: Todo este tutorial está baseado na versão 3 do Python. Algumas funções esclarecidas aqui tem comportamentos diferentes na versão 2 do Python. Aqui está um tutorial para instalação do Python 3 (tanto para Windows quanto para Linux): https://realpython.com/installing-python/. Para melhor entendimento dos pontos listados acima, aqui vai uma boa comparação: Escrever em C é como escrever: If the (integer) time is past (integer) 9 put on some (adjective) work (noun) clothes. Escrever em Python é como escrever: If the time is past 9 put on some work clothes. Agora que entendemos as características mais básicas do Python, vamos para a parte mais técnica. Um dos elementos mais básicos presentes nas linguagens de programação de alto nível é a variável. A variável tem o objetivo de guardar algum valor que seja relevante ao programador e trata-se de uma posição reservada na memória para este fim. Este valor pode ser de diversos tipos como: O Python possui uma função nativa que retorna o tipo de uma variável: type(). Podemos usá-la para verificar o tipo das varáveis com as quais estamos trabalhando. Existem diversos outros tipos que você pode encontrar em Python e em linguagens tipadas como C, Java ou C++, porém esses são os mais importantes. Como se pode ver, em Python não fica explícito o tipo da variável. Isso significa que ao declarar uma variável não é necessário que se explicite seu tipo, mas é importante que você tenha noção da existência dessa tipagem. Agora que você sabe basicamente o que é uma variável, vamos entender o que pode ser feito com elas. A variável, como dito antes, tem o objetivo de guardar algum valor. Em Python, para se atribuir um valor a uma variável, seguimos a seguinte sintaxe: [nome da variável] = [valor a ser guardado] Aqui está um outro exemplo: Importante: O símbolo “=” (igual) não tem o significado de igualdade matemática, mas indica um comando para guardar o valor especificado na posição de memória representada pela variável. É importante ressaltar que ao realizarmos essa operação, o valor antigo da variável é perdido. Em Python, como em outras linguagens, podemos realizar operações aritméticas sobre os valores guardados nas variáveis. Primeiro, vamos falar das operações aritméticas. As operações aritméticas nativas do Python são: 2. Subtração: 3. Divisão: Se quero dividir x1 por x2 e guardar em x3: 4. Multiplicação: 5. Exponenciação: Se quero elevar x1 a x2 e guardar em x3: 6. Resto da divisão: Se quero guardar em x3 o resto da divisão de x1 por x2: 7. Divisão inteira: Se quero guardar em x3 a divisão inteira de x1 por x2: Os símbolos ==, !=, >, >=, < e <= são utilizados em afirmações, que podem ser verdadeiras ou falsas. Se a afirmação for verdadeira, o programa devolve True, senão ele devolve False, seguindo a lógica abaixo: Exemplos de uso das operações booleanas: Essas afirmações de True ou False podem ser modificadas e relacionadas usando operações lógicas, sendo elas and, or e not (algumas operações podem ser interpretados como expressões algébricas para facilitar o entendimento, sendo False igual a 0 e True igual a qualquer número natural, de preferência 1). A função print(), como você já viu nos exemplos anteriores, é capaz de imprimir na tela o que é colocado entre seus parênteses. Note que não é necessário converter o valor entre parênteses para string. Aqui vai mais um exemplo: Observe que para você printar alguma palavra ou letra é necessário declará-la como uma string, isto é, colocá-la entre aspas duplas (“) ou simples (‘). Caso contrário, a palavra será interpretada como uma variável. A função input() lê um texto digitado pelo usuário. Ao encontrar uma chamada da função input(), o programa para na respectiva linha e espera até que o usuário digite enter. Desta maneira, é possível criar programas que dependem da interação do cliente. Esta função sempre retorna uma string (texto). Para ler um número inteiro ou decimal, é necessário convertê-lo para o tipo apropriado usando as funções int(), float(), respectivamente. As estruturas condicionais são extremamente úteis quando queremos que nosso programa execute instruções diferentes para diferentes situações. Para isso, usaremos o conhecimento adquirido até agora, nesse tutorial, a respeito das Operações Booleanas e Operações Aritméticas. Por exemplo, se quisermos fazer um programa que diga se um dado número, numa variável numero, é divisível ou não por 3, podemos fazer o seguinte: Agora que você já sabe mais ou menos como essas estruturas são usadas, vamos detalhar melhor. As condicionais são compostas por 3 tipos: Uma lista é um estrutura de dados nativa do Python. Ela é, basicamente, um conjunto de elementos ordenados, sendo cada elemento um dado de um determinado tipo (int, string ou float, por exemplo). Não é necessário que todos os elementos de uma lista tenham o mesmo tipo. Em Python, a lista é um objeto do tipo list. Declaração de uma lista: Uma outra característica das listas é que todos os elementos são identificados por um índice. Dessa maneira, podemos pegar o valor de algum elemento e/ou alterá-lo por meio de seu índice. Para se chamar um termo específico ou conjunto de termos de uma lista é necessário colocar [] logo após a lista com o índice do elemento entre os colchetes. Abaixo temos alguns exemplos: Note que o índice começa com zero para o primeiro valor e é somado 1 para cada valor a sua direita. No caso de números negativos, o último valor da lista tem índice -1, sendo subtraído 1 para cada valor a sua esquerda. A figura abaixo mostra uma abstração da lista, com os índices representados nas duas formas citadas acima. Para chamar apenas uma parte da lista se deve usar [a:b], onde a é o índice do primeiro valor do corte e b é o índice do primeiro valor que não deve ser incluído (ou seja, o índice do último valor do corte mais 1). Caso essas partes fiquem vazias será completo com o resto da lista. É possível mudar o valor de uma lista como se muda uma variável normalmente. Importante: Uma utilidade importantíssima de se usar o [:] é para realizar uma cópia de uma lista. Fazer uma cópia de uma lista significa criar um novo objeto lista, em outro endereço da memória, porém com exatamente as mesmas características. Por exemplo, ao fizermos o seguinte: Estamos apenas copiando o endereço da lista_a para a lista_b, e não criando realmente um novo objeto. Isso significa que lista_a e lista_b são apenas dois “apelidos” para o mesmo objeto, no mesmo endereço de memória. Dessa maneira, se alterarmos a lista_a, a lista_b também sofrerá alteração, porque estão referenciando o mesmo objeto. Isso ficará claro no exemplo abaixo: Porém, às vezes desejamos criar uma lista exatamente igual a alguma que já temos, mas desejamos alterá-las independentemente. Para isso, usamos o [:]: Como vemos no exemplo acima, a alteração não afetou as duas listas criadas, diferentemente de quando fizemos apenas uma atribuição simples. Existem diversas funções em Python para se trabalhar com listas. Abaixo estão algumas delas. lista.append(): adiciona um certo termo ao final de uma lista. Apenas pode adicionar um termo de cada vez. Se for necessário adicionar mais de um termo, pode-se usar um loop (que será visto no próximo post). lista.pop(): apaga o termo que está entre os parênteses da lista e devolve o termo apagado (caso os parênteses fiquem vazios, o último termo será apagado). Abaixo temos alguns exemplos: Uma abstração dessa função é pensar na lista como uma pilha, onde os elementos estão empilhados. O último elemento da lista é o último elemento que está empilhado. Portanto, podemos apenas desempilhar o termo que está no topo (lista.pop()) ou empilhar um novo termo (lista.append()). lista.sort(): organiza os termos da lista em ordem crescente, ou decrescente. Esta função possui alguns argumentos muito úteis, como reversee key . O reverse, que por default tem valor False , define se a lista será organizada em ordem crescente (reverse=False) ou decrescente (reverse=True). Já o argumento key define o critério de organização. Esses conceitos ficarão mais claros nos exemplos a seguir (a palavra def serve para definir uma função e será explicada melhor no próximo post). len(lista): Retorna o tamanho de uma lista. Se você chegou até aqui, parabéns pela força de vontade! Como já mencionamos, o tutorial foi divido em duas partes. Nessa semana, vimos alguns conceitos muito importantes de programação e, na próxima semana, traremos alguns conceitos mais avançados. Entretanto, você já possui capacidade suficiente para conseguir ler e entender algumas documentações e conceitos mais complicados caso queira ir atrás. Caso tenha encontrado algum erro, achou que algo não ficou muito claro ou que algo ficou de fora ou mal explicado, não deixe de comentar, feedbacks são sempre bem vindos! Nos próximos posts, traremos mais alguns conceitos de programação em python importantes para a área de inteligência artificial. No entanto, depois disso, voltaremos com outros posts mais voltados especificamente a machine learning e ciência de dados. Se você tiver interesse no tema, siga a nossa página no Facebook e acompanhe nossas publicações."
https://medium.com/turing-talks/turing-talks-5-python-parte-2-97198bae699e?source=collection_home---------106----------------------------,Programação | Python — Parte 2,Mais detalhes sobre uma das linguagens de programação mais utilizadas no mundo,Fernando Matsumoto,546,10,2019-05-26,"Na semana passada, passamos o básico da linguagem de programação Python. Introduzimos conceitos básicos como variáveis, atribuições e operações, assim como print, input e listas. Nesta semana, o nosso objetivo é terminar essa introdução, introduzindo conceitos como loops e funções. Poderemos então utilizar esses novos conhecimentos para explorar as áreas de Data Science e Machine Learning. Existe uma função range(a,b,c) em Python que cria uma sequência de números automaticamente¹, sendo apenas necessário identificar o início a, o fim b e o passo c. Os números começam em a e aumentam até atingirem b (note que b não é incluído na sequência). O passo c é a diferença entre números consecutivos nessa sequência. Por exemplo: Mais formalmente, a função range pode ser definida da seguinte maneira: range(a,b,c) ≡ [a, a+1c, a+2c, a+3c, … ,❋] , ❋<b, ❋+c≥b. Saiba que colocar apenas dois termos em range() fará com que o passo seja automaticamente igual a 1 (range(a,b) == range(a,b,1)), ou seja, range(a,b) = range(a,b,1) ≡ [a, a+1, a+2, …, b-1]. Também é bom saber que dar à função apenas um número fará uma lista que começa em zero, tem passo igual a 1 e tem comprimento igual ao valor dado à função (range(a) == range(0,a,1)). Então: range(a) = range(0, a, 1) ≡ [0, 1 ,2, …, a-1] ¹ Essa sequência de números se comporta como se fosse uma lista, mas há algumas diferenças. Por exemplo, não se pode adicionar números a essa sequência usando a função .append(). Para transformar essa sequência em uma lista, basta usar list(range(a,b,c)). Em Python, às vezes, é necessário realizar diversas operações repetitivas, como imprimir uma sequência de números consecutivos. Para esse tipo de tarefa, são utilizados loops. Temos duas maneiras de executar um loop em Python. Podemos fazer utilizando um for, ou por meio de um while. While loop: repete um bloco de código enquanto uma condição for verdadeira. For loop: passa por todos os elementos de uma sequência, na ordem em que eles aparecem. No exemplo abaixo, o bloco dentro do for é executado uma vez para cada elemento da lista a. Você também pode iterar utilizando a função range: Caso seja necessário modificar a lista sobre a qual você está iterando, é necessário fazer uma cópia da lista. Isso pode ser feito utilizando a notação lista[:] vista acima. Os dicionários são uma outra estrutura de dados nativa do Python. Esta estrutura é basicamente composta por elementos que possuem dois componentes: uma chave e um valor. Eles são a implementação, em Python, de hash tables. São extremamente úteis para diversas aplicações. Um dicionário possui a seguinte estrutura: dicionario = {key0:value0, key1:value1, key2:value2,..., keyN:valueN} Como as listas, ele pode ser criado vazio ou já possuindo um ou mais elementos, como está mostrado no exemplo abaixo: Para acessar os valores de um dicionário, usamos uma sintaxe parecida com as listas, utilizando os colchetes ([]), entretanto entre eles não vai o índice do elemento, e sim a sua chave. Além disso, podemos iterar em um dicionário, utilizando o método keys() , que retorna um iterador de todas as chaves do respectivo dicionário. Um iterador é simplesmente um objeto no qual podemos iterar sobre, ou seja, pode ser uma lista ou uma tupla (será explicado adiante), por exemplo. Utilizando os dicionários criados no exemplo anterior, vamos demonstrar estas funcionalidades: Neste exemplo, apareceu algo que ainda não tínhamos visto, a função format() . Ela é muito útil para formatar strings, colocando valores de variáveis nos textos. Ela funciona da seguinte maneira: Ao criarmos a string, colocamos entre chaves ({}) os valores que serão substitúidos por variáveis. No exemplo acima, temos 'Chave do elemento: {c}' , isto é, {c} será substituído por alguma variável que queremos mostrar. Depois, para efetivamente substituirmos os valores, bastante aplicarmos a função format() , e referenciarmos os valores entre chaves em seus argumentos, como fazemos em format(c=key) . Poderíamos fazer 'A chave do elemento é {chave}.format(chave=key), por exemplo. (No Python 3.6 , há também as f-strings). Uma aplicação desta estrutura poderia ser: Quero utilizar uma estrutura de dados para guardar diversas músicas, e para cada música, quero guardar o nome, a banda e o ano. O dicionário cai como uma luva, pois facilita a leitura, é intuitivo e é extremamente prático recuperarmos os dados guardados. A estrutura poderia ser facilmente construída da seguinte maneira: Sim, o dicionário pode ser utilizado em conjunto com listas! Use sua criatividade para criar seus mixes de estruturas de dados de forma que seus dados possam ser acessados de forma eficiente e até para facilitar o entendimento do seu código para o outros programadores. A clareza do seu código é extremamente importante! (Procure sobre code readability). Por fim, o dicionário é uma ótima estrutura de dados para armazenar dados. Em Python, é facilmente transformado num JSON, que é muito usado para envio e recebimento de dados. Como a última estrutura de dados apresentada neste tutorial, temos a tupla. A tupla é muito parecida com uma lista, porém é imutável. Visualmente, a única diferença é que uma lista é criada utilizando-se colchetes ([]), já a tupla é criada utilizando-se parênteses (()). Quando dissemos que esta estrutura de dados é imutável, isso quer dizer que, depois de criada, não pode ser mais alterada. Não é possível adicionar, remover ou alterar elementos. Ela é uma estrutura mais rígida e segura, cujo objetivo é armazenar dados que devem ser manter inalterados ao longo do andamento do programa. Ela é usada, por exemplo, no funcionamento dos *args como você verá em breve. Abaixo temos um exemplo de criação de tupla, seguido de tentativas de alterá-la. Uma função é um bloco de código organizado e reutilizável que realiza uma certa ação. Para se criar uma função em Python, deve-se seguir a estrutura: Primeiro, usaremos apenas argumentos posicionais. Portanto, a ordem que os parâmetros da função são inseridos deve ser a mesma que foram declarados no argumento da função. Após a função ser criada, podemos chamá-la informando os argumentos e ela retornará o valor da expressão colocada no return. Vejamos alguns exemplos: Exemplo 1 Neste primeiro exemplo, estamos criando uma função que printa o texto passado como argumento. Exemplo 2 Neste outro exemplo, estamos criando uma outra função, que soma os dois argumentos passados a ela. Todos argumentos em Python são passados por referência. Isso quer dizer que se você mudar um dos parâmetros dentro de uma função, a mudança também reflete na chamada na função. Exemplo: Neste caso, mantivemos a referência do objeto passado e simplesmente anexamos valores no mesmo objeto. Isso produziria o resultado: Os argumentos de uma função devem ser passados na ordem posicional correta. O número de argumentos na chamada da função deve ser igual ao número na definição da função (novamente, isso se deve pois estamos utiliznado argumentos posicionais. Na pŕoxima parte, explicaremos sobre outros tipos de argumentos). As variáveis do programa podem não estar disponíveis em todas localizações do programa. Isso depende de onde a variável foi declarada. As variáveis podem ser locais ou globais. Variáveis definidas dentro de uma função tem escopo local,e aquelas criadas fora da função tem escopo global. Dessa maneira, variáveis locais só podem ser acessadas dentro da função em que foram declaradas, enquanto as globais podem ser referenciadas por quaisquer funções. Existem três outros tipos de argumentos que podemos utilizar em Python, além dos argumentos posicionais mostrados acima: são os argumentos padrão, *args e os **kwargs. Argumentos padrão são opcionais e têm um valor padrão caso não sejam passados. Por exemplo, para calcular o peso de um objeto, é necessário saber a massa e a aceleração da gravidade, que é aproximadamente g=9.80665 na Terra. Entretanto, sabemos que a aceleração da gravidade muda conforme a latitude, longitude e altura na Terra, mesmo que possa ser uma pequena variação. Portanto, daremos ao usuário a liberdade de definir a sua própria aceleração da gravidade, isto é, a que ele julga mais precisa para calcular seu peso em sua dada situação. Dessa forma, podemos definir uma função peso, que recebe g como um argumento padrão. Note que o argumento g pode ser passado tanto de forma posicional (peso(80, 10)), quanto nomeada (peso(80, g=10)) O *args permite que passemos uma lista de argumentos para uma função, com número indefinido de termos. Dentro da função, recebemos exatamente a estrutura de dados n-tupla e podemos iterar nela a fim de executar os procedimentos da função. Imagine um caso em que precisamos de uma função que receba uma quantidade indefinida de argumentos e execute alguma ação. Podemos pensar numa função que recebe um ponto p0, cujas coordenadas são (x0, y0), e depois retorna um lista contendo as distâncias euclidianas entre p0 e uma série de pontos p1,…, pn, com n ≥ 1. Primeiro, usaremos um argumento posicional p0 para receber o ponto p0 e depois utilizaremos o *args para capturar os outros pontos. Além disso, para deixar nosso código mais limpo, podemos declarar uma função auxiliar distance() que apenas calcula a disitância euclidiana entre dois pontos. Assim, o código fica: Lembrando novamente dos dicionários, utilizar **kwargs numa função é como se passássemos os argumentos para uma função em forma de dicionário. Com este tipo de argumento, não precisamos nos preocupar com a ordem dos argumentos (os argumentos são sempre nomeados) e nem em saber todos os possíveis argumentos. Podemos passar apenas os que nos interessam e deixar os outros indefinidos. Somando-se a isso, poderíamos permitir que o usuário passe um mesmo argumento utilizando nomes diferentes (o nome completo e sua abreviação, por exemplo). Basicamente, quando uma função recebe **kwargs, ela recebe um dicionário de argumentos. Podemos verificar isso no exemplo abaixo: É muito comum que ele seja usado da seguinte forma: O primeiro argumento da função é o argumento mais importante, isto é, a entidade principal que precisará ser usada pela função para que ela possa realizar seu objetivo; já os outros são como configurações, especificações que passamos para a função que podem mudar seu comportamento. Para deixar tudo isso mais claro, vamos modificar o exemplo da função peso(). A nova função poderá receber a aceleração da gravidade tanto como g, quanto como gravidade. Com essas especificações, teríamos o seguinte código para a desejada função: Até aqui, já falamos sobre diversas estruturas de dados, funções nativas importantes, como criar funções, como criar loops entre outras coisas. Agora, vamos falar das Exceptions. Imagine que você tem uma função que soma dois números, mas você passa como argumentos um inteiro e um texto. Isso acarretará num erro, uma Exception. Mais especificamente, receberemos um erro deste tipo: TypeError: unsupported operand type(s) for +: 'int' and 'str' Uma Exception leva a interrupção da execução do código: o programa será terminado. A exceção acima é do tipo TypeError, que indica que as variáveis utilizadas têm tipo errado. Existem vários outros tipos de exceções, como ValueError, que indica que algum argumento da função tinha um valor inválido. Porém, podemos tratar essas exceções, de modo que quando ocorrer o erro, o comportamento do programa seja diferente. Por exemplo, poderíamos fazer com que, ao invés do programa retornar o erro acima, retorne a seguinte mensagem: Por favor, digite dois números! Por outro lado, podemos criar Exceptions, ou seja, podemos fazer com que o programa termine com um erro. Para exemplificar como brincar com as Exceptions, utilizaremos o exemplo do programa que calcula a distância entre p0 e uma série de pontos, copiado abaixo para facilitar a consulta: Queremos garantir algumas condições para o funcionamento da função: A fim de satisfazer essas condições, nosso código fica: Para te incentivar a correr atrás de mais conhecimento, listarei aqui algumas coisas que é possível fazer com Python: Agora que já vimos o básico de Python, podemos usar bibliotecas como as listadas acima (pandas, numpy, …) para entender melhor como funciona a ciência de dados e o aprendizado de máquina. A partir da próxima semana, começaremos a direcionar nosso foco nessa direção. Se quiser saber mais, siga a nossa página no Facebook e fique ligado nos posts."
https://medium.com/turing-talks/turing-talks-6-data-science-libraries-6c2599838b3e?source=collection_home---------105----------------------------,Data Science| Bibliotecas de Data Science,Primeiros passos para adentrar no mundo de IA na prática,Guilherme Fernandes,845,6,2019-06-02,"Quer começar a desenvolver projetos com Machine Learning e IA? Neste Turing Talks apresentaremos ferramentas essenciais pra você colocar a mão na massa e também mostraremos como usá-las. Sem mais delongas, vamos ao que interessa. O primeiro passo é baixar o Anaconda. Essa distribuição de Python é indispensável para quem se interessa por ciência de dados, uma vez que lhe dá acesso a poderosas bibliotecas para data science e machine learning. Além disso é gratuita e contém todas as bibliotecas que abordaremos ao longo desse Turing Talks. Para ter acesso a todo esse conteúdo só precisa seguir o guia de instalação disponível no link: https://www.anaconda.com/distribution/. Fique atento para fazer o download referente ao seu sistema operacional. Uma ferramenta muito utilizada na área de Data Science (e outras) são os Jupyter notebooks. Eles permitem que você combine código com texto e imagens. Dessa forma, eles podem ser usados, por exemplo, para mostrar uma análise de dados (com explicações, gráficos e outras visualizações) ou ensinar algum assunto para os seus leitores. Para entender melhor o que é um notebook, veja o exemplo abaixo. A formatação das células de texto é feita por meio de markdown. Em markdown, a formatação em itálico e negrito é feita utilizando asteriscos: *itálico*, **negrito**. Para escrever o título de uma seção, a linha deve comecar com #. Para mais sobre markdown, veja o link abaixo. Para utilizar os notebooks, você pode seguir esse tutorial de instalação. Para acessar o jupyter e criar notebooks, basta ir no menu iniciar e abrir o Jupyter Notebook (no Windows) ou rodar o comando no terminal (em qualquer sistema operacional). Você também pode editar os notebooks no Google Colab e salvá-los no Google Drive. A interface é um pouco diferente da descrita acima, mas a funcionalidade é a mesma. Mais informações NumPy (Numerical Python) é a biblioteca que você precisa saber para fazer ciência com seus dados, principalmente porque fornece uma estrutura de dados multidimensional com muitos benefícios em relação às listas de python, como a velocidade das operações em arrays e o menor uso de memória. Isso ocorre porque Numpy é em sua maioria escrita em C, linguagem que faz melhor aproveitamento da arquitetura da sua máquina. Além disso, Numpy fornece diversas operações em arrays (como as de álgebra linear) que facilitam muito a manipulação dessa estrutura de dados. E como se isso já não fosse suficiente, ela conta com uma quantidade imensa de algoritmos científicos já implementados de forma otimizada, como: transformada discreta de Fourier, simulações estocásticas e muito mais. Para utilizarmos essa biblioteca, inserimos a seguinte linha de comando no inicio do código e acessamos suas funcionalidades pela abreviação np. Como criar uma estrutura de dados (array) Numpy? Primeiro é necessário importarmos o numpy. Com isso, podemos converter uma lista em um array: Algumas funções elementares Operações básicas Todas as funções do módulo random possuem um argumento size que pode ser uma tupla ou um inteiro que define a dimensão da matriz, ou número de elementos do array. Mais informações Pandas é a biblioteca de python mais famosa para análise de dados, ela fornece uma performance altamente otimizada, uma vez que seu código fonte é escrito em python, com numpy e linguagem C. Para utilizarmos essa biblioteca, é necessário inserir a seguinte linha de comando no inicio do código. Acessamos suas funcionalidades pela abreviação pd. Existem dois tipos de estruturas de dados que utilizaremos para manipular os dados inputados: Series e DataFrames. É uma estrutura de dados unidimensional que armazena valores de uma mesmo tipo (int, float, …) e associado a cada valor existe um índice único. É uma estrutura de dados análoga a uma tabela com linhas e colunas, em que as colunas têm nomes e as linhas índices. Em um DataFrame, cada coluna é uma série. Desse modo, podemos pensar que cada linha do DataFrame representa alguma observação (um indivíduo, um objeto, …) e que cada coluna representa uma característica dessa observação. No dataset de flores que veremos abaixo, cada linha representa uma flor e cada coluna representa uma característica dessa flor. Como criar um DataFrame? Como analisar e visualizar uma DataFrame? Para responder essa pergunta apresentaremos mais uma forma de criar um DataFrame, que é importando um arquivo csv. Em um arquivo csv, cada linha do arquivo corresponde a uma linha da tabela e as colunas são separadas por vírgulas. O primeiro DataFrame criado acima, em csv, seria: Para entender melhor como os DataFrames são utilizados, usaremos como exemplo um dataset muito conhecido pelos cientistas de dados — o Iris (https://archive.ics.uci.edu/ml/datasets/iris). Esse dataset contém algumas informações quantitativas sobre a pétala de três tipos de flores. E esses dados são utilizados para classificar os tipos de flores. Como selecionar índices ou colunas? A principal diferença entre os métodos loc e iloc é que com iloc acessamos somente índices numéricos, mesmo que a indexação de uma DataFrame não seja numérica Como filtrar uma DataFrame? Mais informações Para utilizar a biblioteca matplotlib, é necessário importá-la usando: O comando %matplotlib inline garante que os gráficos sejam mostrados corretamente. Em geral, o código usado para gerar um plot segue o esquema: Mais informações Nessas últimas semanas, vimos várias coisas de programação, mas nada de IA em si. Sabemos que foi cansativo, mas nas próximas semanas começaremos a usar esse conhecimento para explorar o mundo de data science e machine learning. Para nos acompanhar nessa jornada, siga nossa publicação aqui no Medium. E, se você estiver interessado em saber mais, siga nossa página no Facebook e fique ligado no que postamos por lá também."
https://medium.com/turing-talks/turing-talks-7-data-cleaning-c770969dd935?source=collection_home---------104----------------------------,Data Science | Data Cleaning,“War is ninety percent information”Napoleão Bonaparte,Felipe Sibuya,358,5,2019-06-09,"Quando iniciamos os estudos de Data Science, pensamos que a análise de dados ou a implementação de modelos é a parte mais demorada para ser realizada. Na verdade, não é gasto nem 30% do tempo de desenvolvimento de um projeto nessas tarefas. E se eu disser que mais de 60% do tempo é direcionado para simplesmente limpar dados? A princípio, parece simples o trabalho de manipular e limpar dados. Mas quando começamos a tocar neles, percebemos a complexidade de tratar dados faltantes, dados em diversos formatos, valores muito discrepantes, erros de digitação, diversos encodings, … Antes de sequer plotar o primeiro gráfico, ou menos ainda escolher entre um Random Forest ou Boosting, é necessário tratar esses problemas comuns de dados. Caso não seja feita uma boa limpeza de dados, seu modelo corre o risco de apresentar falhas, além de problemas de implementação. Sem mais delongas, hoje no Turing Talks iremos apresentar alguns meios de realizar Data Cleaning. Após obter seus dados, é necessário entender como eles estão organizados. Para realizar tal tarefa, utilizaremos 3 métodos da biblioteca pandas que foram vistos no último Turing Talks: df.head(), df.info() e df.describe(). Esses métodos te permitem obter informações básicas, mas essenciais para realizar nossa tarefa de limpar nossos dados. Por isso, recordaremos rapidamente como usá-los. Esse método mostra as 5 primeiras linhas de seu dataset, permitindo um entendimento mais básico de como os dados estão organizados. O objetivo de usar este método é entender quais são os types, quantos dados não nulos há, o nome das colunas, quantas linhas tem nos dados e o quanto de memória é utilizado. Ao utilizar este método, serão exibidos uma tabela sobre cada coluna, numérica ou de strings, alguns dados estatísticos, como moda, quartis e quantidade de termos. Com estes dados, podemos compreender qual o comportamento de nossos dados, além de entender a sua distribuição. Um problema muito comum em grandes bases de dados é a falta de alguns dados em algumas linhas e colunas. A princípio, caso o número de dados faltantes seja baixo, isso não representa grandes problemas para a utilização de modelos, mas mesmo assim é necessário realizar o tratamento destes. Existem várias opções para realizar tal operação. Para o Turing Talks de hoje, irei apresentar 2: o completamento (fillna) e a retirada (dropna). Ao realizar a extração de dados, é muito comum encontrar alguns erros de duplicação de dados. Esses erros podem ocorrer devido à problemas na mineração, falhas na entrada de dados, erros do servidor… Independente da causa desses problemas, é necessário verificar e retirar valores duplicados. Para realizar tal tarefa, podemos utilizar o método df.drop_duplicates(). Datas são essenciais para acompanhar séries temporais, análise de desenvolvimento de países, vendas de uma loja, desempenho da bolsa de valores, … Quando trabalhamos com esse tipo de dado no python, é possível manipular datas com o formato datetime. Muitas vezes quando iniciamos a leitura das bases de dados, as datas são reconhecidas como strings, devido aos diversos formatos e calendários em que podemos representá-las. Apesar disso, é possível manipular as datas para serem reconhecidas como datetime. Após extrair os dados, às vezes acabamos coletando muito mais dados que o necessário, ou colunas com nomes codificados ou complicados para o entendimento. Ao invés de pensar em extrair novamente esses dados ou alterá-los no local de extração, podemos realizar tais operações no pandas. Para demonstrar como retirar uma coluna inteira e como alterar o nome de uma coluna, utilizaremos o dataset Adult. Quando começamos a avaliar as bases de dados, é muito comum encontrar os dados com erros ou em um formato pouco conveniente. Ao invés de pensar em alterar os valores no banco de dados, podemos realizar algumas mudanças diretamente no pandas. Seja para corrigir erros de digitação mais simples ou para converter uma coluna de Fahrenheit (°F) para Celsius (°C), podemos realizar pequenas operações nas colunas dos DataFrames. Para demonstrar algumas das possibilidades de alteração de valores nas colunas, iremos limpar uma base de dados hipotética de uma pesquisa em uma escola. Nem todos os dados podem ser encontrados em um único banco de informações. Mas e se você precisar analisar dados de diversos bancos, como fará para ligar eles entre si? No pandas há duas ferramentas que te permitem ligar DataFrames diferentes, seja adicionando mais linhas ou adicionando mais colunas aos seus dados. Para realizar essas duas operações, são utilizados o df.concat() e o df.merge() em uma base de dados hipotética de uma loja de doces. Nunca antes geramos tantos dados quanto hoje. Mas antes de analisar e tomar decisões em cima desses, não podemos esquecer que a sua confiabilidade e qualidade é muito mais importante que sua quantidade. No momento que iniciamos os trabalhos com bases de dados para aplicar IA, sempre devemos nos preocupar com os dados obtidos. Realizar a limpeza deles não é uma opção, é uma obrigação para desenvolver bons algoritmos e ter bons resultados. Esperamos ter contribuído com o seu aprendizado de Inteligência Artificial com esse pequeno resumo de alguns dos desafios encontrados no trabalho com dados. Se você quiser saber mais, acompanhe o Turing Talks e siga a nossa página no Facebook."
https://medium.com/turing-talks/turing-talks-8-algoritmos-gen%C3%A9ticos-a791c25bd7ba?source=collection_home---------103----------------------------,Algoritmos Genéticos,Um primeiro modelo de Machine Learning baseado na seleção natural,Luísa Mendes Heise,838,13,2019-06-16,"Esta semana faremos uma pausa em nossos estudos de Data Science para ver uma aplicação de Machine Learning na área de jogos. Para isso, usaremos um tipo de algoritmo baseado na biologia: os algoritmos genéticos. É a mesma ideia que apresentamos em nosso workshop em maio desse ano. Para acompanhar esse artigo você precisa saber o básico de Python e Numpy. Caso você não saiba, fique tranquilo, nós temos alguns artigos sobre isso aqui no Turing Talks: Python (parte 1 e parte 2) e Numpy. Algoritmos genéticos (AGs) são uma classe de algoritmos de otimização inspirada no processo evolutivo biológico. Eles recebem esse nome por se relacionarem intimamente a como espécies se adaptam ao ambiente ao longo de diversas gerações. Em linhas gerais, esses algoritmos criam diversas soluções aleatoriamente para um problema, das quais serão selecionadas as que apresentarem melhor performance (ou fitness, em inglês). Mais soluções são geradas a partir das selecionadas, e esse processo é repetido diversas vezes até que se encontre uma solução satisfatória. Esse processo é análogo à seleção natural, pensando-se em uma população de soluções (cada solução seria um indivíduo). Como o algoritmo é baseado num conceito biológico, em geral, utilizamos alguns jargões da biologia quando falamos de aspectos do AG. Na tabela a seguir, você pode verificar as analogias entre alguns termos utilizados na biologia e em AGs: O exemplo que será explanado neste artigo é o de treinar o jogo Dino Run do Google (aquele que aparece no Google Chrome quando a sua internet cai), entretanto existem outros exemplos, como reproduzir imagens 2D em 3D (um tópico mais avançado do que o que será abordado aqui, mas você pode dar uma olhada neste video e neste artigo). A ideia do algoritmo genético é selecionar as melhores soluções de um problema e fazer com que elas se perpetuem. Podemos definir como um pseudocódigo do AG o seguinte: Tipicamente, esses passos são divididos em funções dentro do código, como “reprodução”, “crossing over”, “população aleatória”, “fitness” e etc. Vamos dividir mais ou menos dessa forma o nosso estudo. Muitos problemas em algoritmos genéticos são unidimensionais, ou seja, a função de decisão é binária: tomar uma ação ou outra. Vamos pensar num exemplo bem bobo com essa propriedade: digamos que queiramos fazer um AG que nos diga se um determinado personagem deve ficar sentado ou de pé. Nesse caso, existem apenas duas escolhas possíveis, de forma que a fronteira de decisão poderia ser unidimensional. Ou seja, o nosso algoritmo iria ter algumas variáveis de entrada, uma função que utilizasse essas variáveis com determinados pesos e iria nos retornar um valor (um número real). Dependendo de quanto fosse esse valor, nós tomaríamos uma determinada ação, como por exemplo: ficar de pé se f(x) < 0 e ficar sentado se f(x) ≥ 0. Assim, nossa fronteira seria o 0, e poderíamos representar numa linha em quais valores iríamos tomar determinadas ações. Resumindo em diagramas essa lógica: Um ponto que pode gerar dúvida é: como eu escolho essa fronteira? A ideia do AG é que ele se adapte às suas escolhas iniciais conforme o algoritmo roda. Então, simplificadamente: pode escolher qualquer fronteira. O que eventualmente pode mudar com a sua escolha é a sua taxa e limites de convergência, que podem ser prejudicados ou melhorados (isso não só para sua fronteira, mas principalmente para a função de fitness… mas isso discutiremos daqui a pouco). No caso do Dino Run nós temos três possíveis ações (Pular, Agachar, Não fazer nada), e, por isso, é melhor que o nosso indivíduo seja multidimensional. O que queremos é que cada ação tenha sua própria função, que indique o quão boa essa ação é. Por exemplo: se, em um determinado instante, o jogador deve pular, o valor da ação pular será maior que o valor de agachar ou não fazer nada. Assim, nossa função de decisão não nos retornará um número, mas sim um vetor com três números (um para cada ação). O que isso implica é que não vai existir uma fronteira de decisão unidimensional, que nem a que foi mostrada, mas sim, cada ação terá o seu próprio valor, sem limitações. Nós vamos escolher a ação cuja função nos dê o maior output dentre os três. Por que vamos fazer isso? Bom, de fato, nós bem que podíamos esquartejar nossa pobre reta real em vários pedacinhos e arbitrar intervalos para determinadas ações, entretanto, isso poderia prejudicar bastante nossa convergência. Um último detalhe antes de mergulharmos na implementação, que você deve estar se perguntando há um tempo, é: e as nossas variáveis de entrada??? Bom, em geral, as nossa variáveis de entrada são parâmetros que nos dizem coisas sobre o nosso problema. No caso do dinossauro, poderia ser a distância até o próximo obstáculo, a velocidade, a altura do próximo obstáculo, etc. Lembrando que o jogo é dinâmico, ou seja, essas variáveis estão em constante mudança, a cada tomada de decisão, rodamos mais uma decisão com outros valores. Esse vetor com essas variáveis é chamado de “estado do jogo”. Agora que discutimos toda essa teoria, vamos começar arrumando a casa, importando o jogo, as bibliotecas que vamos usar (numpy) e escolhendo nossos parâmetros globais: Obs: a biblioteca que estamos usando é a chrome-trex-rush, que pode ser instalada com o seguinte comando: Se você quiser instalar pelo Jupyter notebook, é só colocar um ponto de exclamação antes da linha acima ( !pip install ...). Como dito no pseudocódigo anterior, uma das coisas necessárias para o algoritmo genético é uma população aleatória para começar. Para isso, vamos criar vários indivíduos com pesos. PERA, CALMA, O QUÊ? Mas você nem me explicou direito o que que é isso! Calma leitor, vamos dar uma aprofundada antes de montarmos o código em si. Cada indivíduo possui seu próprio código genético, que determina suas características e desempenho no ambiente em que vive. Da mesma forma, uma inteligência artificial possui parâmetros que influenciam sua tomada de decisão e, consequentemente, seu desempenho. Considere, por exemplo, um código que pretende investir no mercado financeiro de forma a maximizar seu capital. A IA pode levar em conta desde o valor de um ativo até notícias sobre o proprietário de uma empresa para decidir se investirá ou não no negócio. O quanto esse programa valoriza cada elemento da realidade em sua tomada de decisão é chamado de peso, e determina seu sucesso. Como e quais fatores são valorizados nesse processo são descritos por uma função de tomada de decisão. Nesse paralelo, a função de tomada de decisão de um algoritmo seria análogo ao genoma de uma espécie, enquanto os pesos seriam associados às diferenças que cada organismo da espécie tem em seu DNA. Se definíssemos uma função linear simples (em que multiplicamos cada variável por um peso e depois somamos tudo), uma IA gerada por esse algoritmo genético que tivesse 3 pesos teria a função de decisão: f = a⋅p[1] + b⋅p[2] + c⋅p[3], em que p[1], p[2] e p[3] são elementos do vetor de estado e a, b e c são os pesos do indivíduo. (Spoiler: essa é a função que vamos adotar no Dino Run) Note que os pesos não precisam necessariamente multiplicar variáveis da função geral de tomada de decisão, ou seja, o uso da função linear é apenas uma simplificação. O importante é que cada peso altere cada variável de alguma forma, definida pelo projetista. Bom, voltando ao Dino Run, nós vamos utilizar a biblioteca numpy para nos ajudar a gerar uma matriz de pesos aleatórios (explicaremos melhor como funciona essa matriz na próxima parte do post): Okay, agora que já montamos nossos indivíduos iniciais, como vamos fazer com que eles tomem uma decisão? A ideia aqui é dividir esse processo em duas funções. A primeira delas, vai aplicar aquelas nossas funções de decisão em si, nos retornando um valor para cada ação, ou seja, um vetor com três números. Lembrando que nesse caso a nossa função de decisão é uma função linear, mas ela não precisaria ser assim. Essa função pode ser o quão complexa ou simples nós quisermos, desde uma função linear (como a que utilizaremos) até uma rede neural (que veremos mais a frente), por exemplo. A segunda função que vamos utilizar vai pegar esse vetor e nos retornar o índice do maior número entre os três. Por que pegamos esse índice? Bom, no jogo, cada uma das ações é aplicada com um número (0,1 ou 2), que corresponde exatamente a um dos índices do vetor de decisão. E como eu garanto que eu estou pegando o índice certo da ação certa? Eu preciso saber qual ação corresponde a qual índice? Na verdade não, como já dito anteriormente, o AG vai se adaptar às suas escolhas iniciais. Então relaxa, não importa qual índice representa qual ação. A multiplicação matricial da matriz do indivíduo pelo vetor do estado nos retornará os valores de cada ação. Por exemplo, se o estado tiver 5 componentes (s0, s1, s2, s3, s4) e as letras coloridas (de a a o) forem os pesos do indivíduo, os valores das ações seriam: Matricialmente, a equação acima fica: Acima usamos um estado com apenas 5 componentes. No Dino Run, o estado tem 10 componentes, ou seja, os pesos estarão organizados em uma matriz 3×10. Desse modo, a função que calcula o valor das ações fica assim: Essa aqui não tem segredo, a gente vai usar uma função do numpy que faz justamente isso: dado um vetor, ela retorna o índice do maior valor. Antes de entrarmos na reprodução dos nossos indivíduos, vamos esclarecer alguns conceitos. Na natureza, indivíduos mais aptos sobrevivem e se reproduzem com mais frequência, proliferando seu código genético na população. No longo prazo, isso gera populações mais e mais adaptadas ao seu ambiente a cada geração. No entanto, linhas de código não estão sujeitas a adversidades como fome ou incapacidade de encontrar um parceiro. Assim, não se espera observar um fenômeno equivalente a não ser que seja introduzido um processo artificial de seleção desses códigos. No contexto da computação, esse processo é introduzido por uma função de avaliação de desempenho. Essa função é responsável por determinar o que caracteriza o sucesso de uma IA em uma certa tarefa. Consideremos, por exemplo, que o desempenho da nossa IA investidora seja definido exclusivamente por seu capital acumulado ao longo do tempo. Se criássemos diversas IAs para investirem no mercado, com parâmetros de tomada de decisão gerados aleatoriamente, é um fato que haveria exemplares com desempenho melhor do que os demais. Seria possível que uma dessas IAs (ou indivíduos) tivesse performance boa o suficiente para ser utilizada. No entanto, a probabilidade desse evento é usualmente muito baixa. Assim, podemos repetir o processo descrito até que se encontre uma IA satisfatória. Porém, essa busca pode ser muito demorada, o que é remediado por alguns recursos dos algoritmos genéticos. Buscar membros de alta performance (para uma dada tarefa) em um grupo cujos indivíduos têm características aleatórias é contraproducente: espera-se que a grande maioria deles, senão sua totalidade, tenha péssimo desempenho. Apesar disso, ainda existirá uma diferença entre a performance de cada um, o que é crucial para o funcionamento de algoritmos genéticos. Um AG seleciona somente os melhores indivíduos que criou para uma próxima geração (baseado na sua função de avaliação), da mesma forma como somente os organismos mais aptos a um habitat conseguem passar pelo crivo da natureza, gerando proles que viverão a próxima geração da espécie. A partir dessa seleção, o AG descarta os demais indivíduos, substituindo-os por IAs cujos parâmetros são gerados a partir de ligeiras variações nos pesos de um “sobrevivente” da geração anterior. Essas variações, sim, são aleatórias, de forma que seja provável que haja tanto indivíduos de melhor quanto de pior desempenho. Assim, a cada geração, espera-se que haja indivíduos com melhores resultados do que na anterior, eventualmente obtendo-se um indivíduo com desempenho satisfatório. Nessa etapa não vamos eliminar os piores indivíduos, vamos deixar pra fazer isso depois que fizermos a nossa função de fitness (aquela que avalia o desempenho dos indivíduos). Por enquanto, vamos apenas fazer os mecanismos de crossing over e mutação. Na reprodução, a função de crossing over escolherá dois indivíduos da população existente e a partir deles criará um indivíduo novo. Depois disso a função de mutação irá possivelmente alterar esse indivíduo. Na função de mutação nós vamos possivelmente mutar os pesos do nosso indivíduo com uma probabilidade definida lá em cima nas nossas variáveis globais. CALMA CALMA CALMA! Como que eu mexo com uma probabilidade num código? Eu não sei como fazer algo acontecer com uma probabilidade dada! Calma leitor, nós pensamos nisso, vamos dar uma explicadinha de como lidar com as probabilidades. Para fazer com que um evento ocorra com probabilidade p basta gerar um número aleatório entre 0 e 1 (nossa variável aleatória com distribuição uniforme) e verificar se ele está dentro de um intervalo do tamanho da probabilidade que queremos. Exemplificando: Nesse caso um número sorteado entre 0 e 1 tem 20% de chance de cair no intervalo rosa. Assim, nosso código fica assim: Nesse caso, a mutação consiste em multiplicar o peso por um valor aleatório dentro de um intervalo, mas não necessariamente teria de ser isso. Como dito anteriormente, a função de crossing over é a que combina dois indivíduos. É nesse momento que vamos escolher dois indivíduos da população e a partir deles gerar um novo. Existem várias formas de fazer isso, você poderia, por exemplo, escolher metade dos pesos do pai e metade dos da mãe. No caso do Dino Run, o que fazemos é, primeiramente, definir a nossa “cria” como a cópia de um dos dois indivíduos selecionados e depois trocar cada um dos pesos pelo do outro indivíduo com uma chance dada. Bom, chegamos agora num ponto bem importante dos algoritmos genéticos, que é a função de fitness. Como dito anteriormente no texto, para simular o mecanismo biológico de seleção natural precisamos artificialmente criar uma pressão do meio. É aí que entra a função de fitness: a partir do score dado por ela para um dado indivíduo, é possível comparar a performance dele em relação aos demais. A função de fitness é extremamente importante para modelar um problema, ela deve ser escolhida sabiamente. Isso porque toda a força computacional será posta para maximizá-la; se ela não contempla de forma satisfatória o nosso problema, chegaremos numa solução que não nos será útil. Mesmo que a função de fitness penalize os indivíduos conforme o esperado para o nosso problema, ainda precisamos corrigir algumas nuances e penalizar mais alguns aspectos do que outros a depender do caso. Por exemplo, se num jogo de snake a nossa função de fitness apenas premiasse o maior número de casas percorridas pela cobrinha indiscriminadamente, poderíamos induzir o algoritmo a fazer com que ela andasse em círculos (o que não é nada bom). Uma forma de prevenir isso seria penalizar os passos dados em direção contrária a da comida. Num exemplo mais real, se quiséssemos maximizar o lucro de uma empresa com base no estoque a ser mantido por ela e, para isso, utilizássemos uma simulação com algoritmos genéticos, haveríamos de considerar várias nuances em nossa função de fitness. Nada nos ajudaria a empresa maximizar o lucro no curto prazo se com isso ela prejudicasse a satisfação dos clientes (o que a faria performar mal no futuro), por isso teríamos de pesar isso no nosso fitness. Voltando ao Dino Run, a nossa função de fitness mais simples a ser escolhida (que é o que faremos nesse caso) é o score do jogo. É claro, talvez pesar outros parâmetros poderia melhorar o nosso resultado final e a nossa taxa de convergência. Entretanto, nesse caso, preferimos utilizar apenas a pontuação do jogo para simplificar o código. Então, em termos de código, o que a função deve executar é: fazer com que o indivíduo jogue o jogo e depois pegar o score dele. Algumas ressalvas são: temos que dar reset no jogo antes de pôr nosso pequeno indivíduo à prova, para que ele comece um jogo novo (e não fique na tela de game over do indivíduo anterior). O legal é que agora estamos começando a encaixar as peças, nessa função tivemos que puxar algumas de nossa funções implementadas anteriormente. Agora que definimos a nossa função de fitness, podemos eliminar os nossos piores indivíduos e reproduzir os melhores. Ou seja, o que vamos fazer é unir o fitness com a nossa reprodução e também selecionar os melhores. Para isso, vamos definir uma função adicional que ordenará a população com base no fitness de cada individuo. Obs: na função acima, utilizamos o método random.choices() (e não np.random.choice()) porque o método de numpy não consegue trabalhar com listas de listas (ou listas de matrizes). Pronto, agora estamos com a faca e o queijo na mão. Só temos que juntar os pedaços que criamos até agora. Esse código de agora é essencialmente a transformação daquele pseudocódigo do início do texto em código python para o Dino Run. Observe que no final do código, na parte que mostra o melhor indivíduo, tivemos que diminuir a velocidade do jogo (o fps ou frames per second). Inicialmente criamos o jogo com um fps alto, para que o algoritmo rodasse mais rápido. No entanto, para que seja possível ver o dinossauro jogando, é necessário diminuir o fps. Em suma, o algoritmo genético é uma ferramenta simples, mas muito poderosa. Ele pode ser utilizado em combinação com outras técnicas e é uma forma de otimização menos custosa do que muitas famosas, como o grid search, por exemplo. Por hoje é só, caro leitor! Espero que tenha gostado do texto! Se quiser saber mais de IA, acompanhe nossos posts aqui e no Facebook, onde divulgamos os nossos eventos, como o workshop que fizemos sobre Algoritmos Genéticos."
https://medium.com/turing-talks/turing-talks-9-visualiza%C3%A7%C3%A3o-de-dados-93df670d479?source=collection_home---------102----------------------------,Data Science | Visualização de Dados,Compreenda melhor o seu dataset.,Fernando Matsumoto,570,15,2019-06-23,"Escrito por Fernando Matsumoto, Gabriela Fanucchi e Guilherme Fernandes. Voltamos essa semana com mais um post do Turing Talks! Nas últimas semanas, temos falado bastante de programação e ciência de dados. Dando continuidade à essa sequência de posts, hoje, falaremos sobre visualização de dados. Mas afinal, o que são dados e para que eles servem? Dados são conjuntos de informações, que podem ser quantitativas (numéricas) ou categóricas (por exemplo, o país em que uma pessoa mora ou a espécie de uma planta). Eles são muito utilizados em aplicações de Inteligência Artificial, para treinar algoritmos. Para isso, o cientista de dados precisa tratá-los da melhor maneira possível (de acordo com a finalidade e o tipo do dado) e visualizar o que está acontecendo para entender a informação que o dado passa. A-ha! Chegamos ao assunto do post. Após filtrar os dados, o cientista irá “plotá-los” da forma que lhe for conveniente para entendê-los. Pois bem, nesse post abordaremos as principais formas de plotar os dados, além de algumas bibliotecas em Python que facilitam esse trabalho! Dentre as bibliotecas que utilizaremos, uma já conhecemos: a matplotlib, que foi brevemente elucidada no Turing Talks #6 e será mostrada, também brevemente, nesse post. E a outra, que se chama seaborn, foi desenvolvida a partir da matplotlib e apresenta um nível de interface maior assim como mais opções de gráficos estatísticos. Para importar essas bibliotecas, usamos: Os histogramas servem para mostrar a frequência de dados, tanto numéricos quanto categóricos. Podemos dizer que eles mostram uma contagem dos valores de uma variável. Para entender melhor o que isso significa, vamos analisar um histograma da idade dos passageiros do Titanic. Nesse gráfico, o eixo x representa a idade e o eixo y, o número de pessoas com aquela idade. Dessa forma, o gráfico nos mostra que o navio transportava desde bebês até idosos, e que a maioria das pessoas tinha entre 20 e 40 anos. Analisando o gráfico, podemos perceber que o histograma se assemelha a um gráfico de barras. No entanto, em geral, não é possível criar um gráfico de barras para dados contínuos (imagine criar uma barra para cada valor real possível entre 0 e 80). Por isso, ao criar um histograma, o matplotlib divide os dados em intervalos de 10 anos (entre 0 e 10 anos, 10 e 20 anos, …). O que o histograma mostra é a quantidade de pessoas com idade em cada um desses intervalos. Para gerar um histograma, usamos a função plt.hist(x). Se quisermos controlar o número de intervalos usados pelo histograma, devemos usar o parâmetro bins. Para gerar o gráfico acima, utilizamos o dataset Titanic: Além do histograma, temos também o gráfico da densidade de uma feature (kde plot — kernel density estimation plot). Esse gráfico é uma versão “suavizada” do histograma. Note que o eixo y mostra uma estimativa da frequência relativa de cada valor. Em termos probabilísticos, trata-se de uma estimativa da função densidade de probabilidade da feature. Também podemos mostrar a densidade e o histograma juntos usando o distplot: Um caso especial da contagem de dados é quando a variável é categórica, ou seja, ela assume um número limitado de valores possíveis. Isso ocorre, por exemplo, com o país de origem de uma pessoa. Nesse caso, dizemos que cada valor da variável (cada país) é uma da categoria. Para o caso unidimensional, geralmente se deseja analisar o número de observações em cada categoria. Isso é bem similar a um histograma, salvo que ao invés de contar as observações em um intervalo de valores (quantas pessoas entre 0 e 10 anos), contamos elas por categorias (quantos homens?). Por exemplo, podemos analisar a distribuição de passageiros por sexo: O gráfico mostra que aproximadamente dois terços dos passageiros eram homens. Em alguns datasets é interessante analisar as mudanças em uma variável com relação ao tempo, ou a similaridade com uma função contínua por exemplo. Nesses casos uma boa escolha é usar os gráficos de linha. Esse tipo de gráfico é muito útil para funções contínuas (que de maneira simplificada, são funções cujo gráfico pode ser desenhado sem levantar o lápis do papel). Por exemplo, quando se quer representar a variação de temperaturas de uma determinada cidade em função do tempo. Vamos utilizar como exemplo o dataset Wine, que tem diversas características de vários vinhos (qualidade, pH, álcool, acidez, …). Vamos primeiramente tentar usar a função do matplotlib para plotar o pH em função da concentração de ácido cítrico no vinho. Claramente a função que cria gráficos de linha não funcionou. Por quê? Há dois motivos: A função sns.lineplot() faz essas duas tarefas automaticamente: Esse gráfico mostra a relação entre a concentração de ácido cítrico e o pH. Como os pontos do gráfico são apenas uma média dos pontos do dataset original, podemos dizer que o pH mostrado no gráfico é apenas uma estimativa. Assim, há uma certa incerteza em torno desses valores. Essa incerteza é expressa pela área sombreada em azul (mais rigorosamente, esse sombreado representa o intervalo de confiança de 95% em torno da média). Também é possível adicionar mais informações nos gráficos de linha, sobrepondo linhas de cores e padrões diferentes. Abaixo, mostramos um plot do dataset Iris, do comprimento pela largura das pétalas. A cor e estilo das linhas são determinados pela espécie da flor. Nos gráficos acima, pudemos ter uma ideia da distribuição de dados. Um passo a mais seria começar a prever os dados, que é o que veremos mais a frente. No entanto, já podemos ter uma ideia de alguns modelos simples usando o seaborn. A função sns.lmplot() plota os pontos do dataset, assim como uma regressão linear desses pontos, ou seja, a reta que está mais “próxima” dos dados (o significado preciso disso ficará mais claro daqui a algumas semanas). Heat map, ou mapa de calor, é uma representação por cores da intensidade dos valores de uma matriz de entrada. Comumente em uma análise de dados essa matriz é a matriz de correlação das features. A correlação é uma medida estatística entre duas variáveis contida no intervalo -1 e 1, a qual indica se uma variável descreve bem a outra por uma relação linear. Para valores próximos de 1, dizemos que as variáveis são positivamente correlacionadas e para valores próximos de -1, dizemos que estão negativamente correlacionadas. Nesse último caso a relação é inversamente proporcional. Sabendo de features altamente correlacionadas, podemos criar modelos de machine learning mais robustos (apresentam taxa de acerto em previsões maiores), uma vez que entendemos melhor como o dataset se comporta. (Spoiler: isso é tópico para um próximo Turing Talks.) Para obter a matriz de correlação de um dataframe, podemos utilizar o método df.corr() e, para plotar essa matriz, usamos sns.heatmap(). Na matriz abaixo, do dataset Wine, vemos que as features density e fixed acidity tem uma correlação positiva alta (o quadrado correspondente a essas duas features é quase branco). Por outro lado, a correlação entre pH e fixed acidity é negativa alta (o quadrado correspondente é quase preto). Geralmente, para decidir quais features são mais úteis, o que importa é se a correlação é alta ou baixa, e não negativa ou positiva. Assim, se quisermos prever a qualidade do vinho, queremos features que tenham correlação alta com a qualidade. Essas features são, principalmente, álcohol e volatile acidity. As outras features têm correlação relativamente próxima de zero, em comparação com essas duas. Nos gráficos de dispersão, os pontos representam o valor das observações em duas variáveis, uma medida ao longo do eixo x e a outra ao longo do eixo y. Podemos usar um scatter plot para ver como duas features numéricas estão relacionadas. Como exemplo, vamos utilizar o dataset Iris (o mesmo que usamos no post de bibliotecas). Nesse dataset, temos o comprimento e largura das pétalas e sépalas de várias flores, assim como a espécie de cada flor. Abaixo temos um scatter plot do comprimento pela largura das pétalas de uma flor, criado com o código: Podemos perceber que, como esperado, quanto maior a largura da pétala (eixo x), maior o comprimento da pétala (eixo y). De fato, a relação parece bem linear, de forma que, para um problema de regressão, um modelo linear funcionaria bem. Podemos extrair mais informações desse gráfico se identificarmos também outras características da flor no gráfico. Para isso, usamos os seguintes parâmetros da função sns.scatterplot(): Os parâmetros hue, size e style controlam a cor, tamanho e estilo de cada ponto com base nas features dos dados. Por exemplo, para colorir cada ponto com base na espécie da flor, podemos usar: Esse novo gráfico nos mostra que as flores menores são da espécie Iris setosa, enquanto as maiores são Iris virginica. Percebemos também que, para um problema de classificação (em que queremos determinar a espécie de uma flor com base em suas características), esse problema parece ser relativamente simples. As flores Iris setosa são linearmente separáveis das outras duas (podemos separá-las usando uma reta) e a espécie parece seguir uma relação simples com o tamanho da flor. Uma outra variação desse mesmo plot é usando os parâmetros size e style para gerar um scatter plot do comprimento pela largura das pétalas, em que o tamanho dos pontos é proporcional à largura das sépalas e o estilo (círculo, x ou quadrado) depende da espécie. Às vezes, é útil ver tanto as relações entre duas features como as características de cada feature individualmente. Para isso, usamos o joint plot, que mostra a distribuição de duas features entre si e individualmente. A forma mais básica mostra um scatterplot e os histogramas: Mexendo no parâmetro kind, podemos obter outros gráficos. Por exemplo, usando kind='kde', os plots são de densidade (tanto em uma variável, quanto em duas): Ao invés de plotar os gráficos para apenas algumas das features, podemos analisar todos os dados de uma vez. Para nos auxiliar nessa tarefa, a biblioteca seaborn fornece a função pairplot(), que plota a distribuição de cada feature, assim como as relações entre pares de features (na forma de scatter plots): O pairplot pode utilizar tanto histogramas como densidades. O padrão é plotar densidades quando o parâmetro hue é passado e histogramas caso contrário. Para controlar o tipo de plot, podemos passar o parâmetro diag_kind='hist' ou diag_kind='kde'. Ao lidar com dados categóricos, temos dois tipos de gráficos: aqueles que se assemelham aos scatter plots (stripplot e swarmplot) e aqueles que estimam a função de densidade dos dados (boxplot e violinplot). Esses gráficos colocam uma feature categórica em um dos eixos e uma feature numérica no outro. Se quiséssemos ver a distribuição conjunta de uma feature categórica por uma feature numérica, a nossa primeira ideia pode ser simplesmente fazer um scatterplot. Por exemplo, podemos fazer um scatterplot do titanic entre a variável Survived, que indica se o passageiro sobreviveu, e a variável Fare, que indica o quanto ele pagou. Nos próximos exemplos, vamos usar apenas uma parte do dataset do titanic (o motivo para isso será esclarecido mais tarde). O resultado disso pode ser visto abaixo: É fácil perceber que esse gráfico não é muito útil. Há tantos pontos acumulados, um em cima do outro, que não sabemos a real distribuição dos dados. Uma modo de arrumar isso seria diminuir a sobreposição dos pontos, introduzindo um “ruído” no eixo horizontal, ou seja, fazendo com que os pontos se movam aleatóriamente na horizontal. Esse tipo de plot se chama stripplot. O gráfico não está completamente claro, pois há muitos pontos sobrepostos, mas ele parece indicar que as pessoas que sobreviveram (Survived=1) pagaram, em geral, mais caro para estar no Titanic. No stripplot, podemos ver que ainda há sobreposição dos pontos. O swarmplot (também chamado de “beeswarm”) elimina essa sobreposição. Isso fornece uma melhor visualização da distribuição dos valores, mas não performa bem com um número grande de observações. Com o swarmplot, fica ainda mais claro que as pessoas que sobreviveram, em geral, pagaram mais, visto que há muito mais pontos acumulados em baixo do lado esquerdo do que do lado direito. Apesar desse gráfico estar melhor que o anterior, podemos ver que os 200 pontos desse dataset já estão chegando perto do limite do swarmplot, como evidenciado pela sobreposição de alguns pontos no canto inferior esquerdo. Os problemas desses plots com datasets grandes ficam ainda mais claros se utilizarmos o dataset inteiro do Titanic: Um boxplot é um diagrama das features de um dataset. Essa representação gráfica guarda 5 informações principais sobre a distribuição dos dados, são elas: No boxplot temos 3 quartis, que dividem o dataset em 4 regiões: o primeiro quartil (Q1) representa o número tal que abaixo dele estejam 25% dos dados, e por conseguinte o segundo quartil (Q2) indica o valor de forma que abaixo dele estejam 50% dos dados o que é equivalente à mediana, e por fim, o terceiro quartil (Q3) está acima de 75% dos dados. Os outliers indicam possivelmente erros nos dados (dados discrepantes), uma vez que dista mais que os limites do intervalo esperado, e portanto, podem ser desconsiderados. Com isso, o boxplot fornece a ideia de posição, dispersão, assimetria, caudas e dados discrepantes. As posições dos quartis fornecem uma noção da assimetria da distribuição dos dados e o comprimento dos limites indicam comprimento das caudas. Para elucidar essas características, utilizaremos o dataset Quarteto de Anscombe, que foi desenvolvido pelo estatístico Francis Anscombe. É um conjunto de dados que compreende outros quatro datasets, cada um contendo 11 pares (x,y). O que é curioso sobre esses dados é que os quatro dados comprimidos apresentam as mesmas descrições estatísticas (soma, média, desvio padrão, etc…), mas são completamente diferentes quando visualizados graficamente. É possível acessar esse dataset pelo seaborn, que retorna os dados em formato de pandas dataframe. Para a feature x temos que o boxplot de I, II e III são iguais já que as observações são as mesmas. Além disso, podemos notar a simetria nos quartis e mesmo comprimento dos limites (caudas), isso nos informa que o número de observações e os valores da features são simétricos com relação a mediana. Já para IV é interessante notar que os quartis e os limites coincidem com a mediana, em linhas gerais os valores estão em sua maioria próximos a mediana, e a observação do dataset IV acima do limite superior é um outlier. Já para a variável y, podemos observar em II a assimetria dos quartis e dos limites. Como o terceiro quartil (e o limite superior) estão próximos da mediana, podemos concluir que os dados acima da mediana estão concentrados mais próximos da dela. Por outro lado, o primeiro quartil e o limite inferior estão relativamente longe da mediana, nos levando a concluir que os dados abaixo da mediana estão mais bem distribuídos. Esses gráficos juntam o boxplot com uma curva de densidade dos dados. O nome do gráfico (violinplot) vem do formato dos plots gerados, como podemos ver abaixo: Essa informação diz mais sobre o dataset do que uma análise somente com boxplots. Isso é perceptível na imagem abaixo, em que os dados originais (à esquerda) estão variando significativamente. No entanto, os boxplots continuam iguais, ou seja, as estatísticas (mediana, quartis) dos dados não estão variando. Já os violinplots conseguem capturar a variação na distribuição dos dados. Porém, como os violinplots precisam estimar a distribuição do dataset não é o ideal utilizá-los quando se dispõe de uma base de dados muito pequena. Nesse cenário é mais conveniente o uso de gráficos como o stripplot e swarmplot, que mostram os dados “crus”. Violinplots para o dataset Anscombe Com o seguinte comando geramos o gráfico: Agora contendo a distribuição da variável em um diagrama com o boxplot podemos confirmar as afirmações sobre a simetria de x verificadas apenas com o boxplot. Enquanto I, II e III possuem a mesma distribuição simétrica, IV apresenta uma maior concentração de observações no valor médio. Note que nessa representação é mais fácil identificar as caudas. Com relação a feature y do dataset II, também podemos confirmar a assimetria e concentração de observações acima da mediana, enquanto os dados abaixo da mediana estão mais dispersos. Dizemos então que há uma cauda inferior nesses dados, em vista do formato da distribuição. Chegamos ao final de mais um Turing Talks. Com essas ferramentas de visualização de dados agora é possível concluir muito mais sobre os dados. Isso abre um leque para novas possibilidades, como uma análise estatística mais aprofundada dos dados e uma série de modelos de machine learning. Caso a utilização das funções apresentadas não tenha ficado claro, recomendamos que vocês leiam a documentação do seaborn e do matplotlib, ou comentem abaixo no post. Nas próximas semanas continuaremos com os Turing Talks com mais assuntos para você ficar por dentro do mundo da IA! Se quiser saber mais siga também nossa página no Facebook e fique ligado no que postamos por lá. Por hoje é só!"
https://medium.com/turing-talks/turing-talks-10-introdu%C3%A7%C3%A3o-%C3%A0-predi%C3%A7%C3%A3o-a75cd61c268d?source=collection_home---------101----------------------------,Modelos de Predição | Introdução à Predição,Primeiros passos para entender os modelos preditivos.,Letícia Mendonça Carraro,991,14,2019-07-14,"O quão bem você aprende com seus erros e experiências passadas? Mal? Não se preocupe, depois desse artigo você entenderá melhor sobre conceitos de predição e como eles podem impactar o seu projeto de machine learning. Dessa forma, pelo menos seu algoritmo vai tomar as decisões certas! O conceito de Machine Learning é baseado na construção de algoritmos de maneira a possibilitar que o computador tome decisões de acordo com as situações e dados inseridos nele. Sendo assim, não se está implementando um algoritmo que segue instruções em uma rotina e sim treinando a máquina para aprender como executar a tarefa. Mas como é possível a máquina aprender? Existem três tipos de aprendizagem de máquina, o supervisionado, o não supervisionado e o por reforço. Caso queira saber mais sobre o assunto e machine learning, existe uma explicação em detalhes neste post! Bem, agora você já sabe sobre machine learning, e os tipos de aprendizagem de máquina e quer criar um modelo! Mas como fazer? Veremos diferentes tipos de modelo em posts futuros, contudo antes de avançarmos é preciso entender alguns conceitos ligados a aspectos básicos de predição. Quando buscamos determinar um valor — por exemplo, determinar o preço de um automóvel baseado no ano em que foi fabricado, se é usado ou não, sua quilometragem, etc. — ou uma categoria, precisamos de um modelo de predição, ou seja, precisamos de uma função que se aproxime daquela descrita pelos dados que temos. Existem diversos modelos de predição (regressão linear, SVM, Naive Bayes, …), que serão abordados em detalhes em posts futuros. É necessário escolher o que melhor se encaixa em sua necessidade. Tal escolha depende dos tipos de dados que se possui e do problema a ser resolvido. Conforme nos aprofundarmos nos diferentes modelos de predição e nos familiarizarmos com suas características a escolha se tornará mais fácil. Depois de escolhido um modelo, é preciso verificar se os resultados obtidos foram bons o suficiente. Para avaliar isso é necessário conhecer um pouco sobre erros comuns que acontecem ao desenvolver os algoritmos, como identificá-los e corrigi-los. Ao construir uma função que se aproxima dos nossos dados, podemos proceder de forma muito complexa, tentando ajustá-la para cada ponto possível; afinal, queremos que ela seja extremamente precisa. Entretanto, isto gera um problema de overfitting: sua função é excelente na base de treino, contudo não tem um desempenho tão bom na base de testes. Isso acontece pois, de certa forma, a sua função “aprendeu o ruído”, ou seja, o modelo se ajusta a erros e fatores aleatórios (exceções ou outliers), que não necessariamente condizem com a realidade dos dados apresentados. Para exemplificar esse problema, vamos analisar um dataset com duas classificações (azul e vermelho). Na imagem abaixo, o modelo ideal é representado pela linha preta, que separa os pontos vermelhos dos azuis. Observe que esse modelo erra algumas vezes. No entanto, esses erros ocorrem por causa de ruído. A linha verde, que não comete nenhum erro, é muito mais complexa: ela se ajustou ao ruído. Se obtivéssemos pontos novos (que não estavam no dataset), o modelo verde provavelmente cometeria erros nesses pontos. Em contrapartida, se o seu modelo for muito simples, você pode acabar com um caso de underfitting, ou seja, sua função não captura corretamente a complexidade dos dados apresentados. Uma linha reta, por exemplo, nem sempre consegue separar um conjunto de pontos em duas categorias. Para entender melhor os erros de predição é necessário entender que fatores influenciam tais erros, bem como a relação entre eles. O primeiro fator, chamado de bias (viés), é a diferença entre a predição média do nosso modelo e o valor correto esperado. Sendo assim, um modelo com bias aprende relações erradas e gera previsões longe do esperado. O modelo não aprende corretamente com o dataset, assumindo muitas informações sobre os dados que não são necessariamente corretas. Dessa forma, modelos com alto bias possuem um problema de underfitting. O segundo fator, variance, é a variabilidade do modelo para um determinado dado, ou seja, a capacidade do modelo de se adaptar à base de treino e ao ruído. Dessa forma, modelos com alta variance focam excessivamente se ajustar aos dados e, inclusive, ao ruído. Assim, esses modelos têm um problema de overfitting, ou seja, se adaptam tão bem ao dataset que não conseguem generalizar para além dele. Por último, temos o erro irredutível, que refere-se a pontos fora da curva, exceções. Não é possível observar cada pequeno fator que leva um evento a acontecer, possuímos limitações. Ao elaborar um modelo escolhemos os fatores que são mais relevantes ao nosso problema e deixamos de lado alguns outros. Em outros casos, podemos não ter acesso a todas as informações. Dessa forma, pode haver alguma feature faltando em nosso dataset que nos impede de gerar predições corretas. Um caso similiar ocorre em um ambiente estocástico, ou seja, um ambiente no qual há algum grau de aleatoriedade. Nesse caso, é realmente impossível prever com exatidão cada saída esperada. Chamamos esses tipos de erro de irredutíveis, uma vez que não podemos reduzi-los apenas mudando o modelo. Para entender melhor a relação entre bias e variance é necessário notar que algoritmos com alta variance tendem a ser mais complexos visto que conseguem se adaptar muito bem a qualquer conjunto de dados. Em contrapartida, os algoritmos com alto bias são muito limitados por tudo aquilo que assumem sobre os dados, de forma que tem menor complexidade. Ou seja, ambos estão ligados ao nível de complexidade do modelo e são dependentes entre si. Em geral, modelo mais simples têm alto bias e baixa variance, enquanto modelos mais complexos têm baixo bias e alta variance. Imagine que você tem um problema para resolver e não sabe muito bem como prosseguir. Como uma primeira tentativa, você decide utilizar um modelo de regressão linear. Ao observar os resultados obtidos, você percebe que estes estão consistentes, porém consistentemente errados. Ao se questionar o motivo do comportamento do seu modelo você nota que a regressão linear assume que os dados estão numa reta, e isto que não é verdadeiro no seu projeto. Nesse caso, o modelo que você escolheu possui alto bias: ele assumiu muito sobre os dados e acabou com um problema de underfitting ao tentar simplificar de mais. Dada a falha na sua tentativa anterior, você decidiu utilizar um polinômio de grau 10000 para construir seu modelo. Contudo você percebe que ainda assim seu modelo não está com um desempenho satisfatório: os resultados estão dispersos e, apesar de as vezes seu modelo acertar, ele não está servindo muito bem para o seu propósito. Isto aconteceu pois ao construir seu modelo extremamente complexo, você acabou com uma variance altíssima e de brinde ganhou um problema de overfitting. Observe que para diminuir o bias, precisamos aumentar a variance, e vice-versa. É assim que funciona a ligação entre Bias e Variance, eles estão intimamente conectados, e o modelo ideal é aquele que melhor harmoniza essa relação e reduz o erro. Esse problema é denominado de bias-variance tradeoff. Sendo assim, afim de se obter o melhor modelo é necessário que se encontre a melhor relação entre bias e variance de modo a minimizar o erro. Para tal, segue-se a seguinte relação matemática: MSE = Bias² + Variance + Erro Irredutível, onde MSE é o erro quadrático médio (veremos melhor o que isso é o MSE mais abaixo). Separar a sua base de dados em base de treino e base de teste é um ponto fundamental para validar seu modelo. A base de treino é aquela que, como o nome sugere, será utilizada para treinar seu modelo. Já a base de teste refere-se à amostra de dados que será utilizada para avaliar o desempenho do seu modelo no mundo real. Essa método de dividir o dataset é denominado holdout. E por que não podemos treinar e testar na mesma base? Pense no que acontece em um modelo com overfitting. Esse modelo se ajustou ao ruído. Se ele for testado na mesma base em que foi treinado, ele vai funcionar corretamente, pois vai produzir o mesmo ruído que já existe na base. No entanto, se o testarmos em uma base diferente, vamos perceber que esse modelo se ajustou ao ruído e que ele não está performando bem. Assim, usamos holdout para medir a capacidade de generalização de um modelo (se ele funciona bem em dados que ainda não viu). Também existe uma terceira base de dados. Para entender o seu propósito, vamos analisar um pouco melhor o que acontece durante o treinamento de um modelo. Suponha que o nosso objetivo seja prever a altura de uma pessoa com base em sua idade e que o modelo escolhido seja uma função polinomial de grau n. Esse modelo é parametrizado pelo grau n, assim como pelos coeficientes a₀, a₁, a₂, …, aₙ, com: f(x) = a₀ + a₁⋅x + a₂⋅x² + ⋯ + aₙ⋅xⁿ. Nesse caso, o grau do polinômio é decidido antes de treinarmos o modelo. Já os coeficientes são determinados pelo modelo durante o treinamento. Dizemos então que n é um hiperparâmetro, enquanto a₀, a₁, a₂, …, aₙ são parâmetros do modelo. Surge então uma dúvida: como devemos escolher o valor de n? Será que podemos treinar o modelo para vários valores de n, testá-lo na base de teste e escolher o valor de n que resulta no menor erro? A resposta é não, porque isso seria uma forma de overfitting: poderíamos estar escolhendo o valor de n que melhor adapta o modelo ao ruído da base de teste. A solução é criar uma terceira base: a base de validação. Essa amostra dos dados tem como objetivo avaliar o desempenho do modelo durante a fase de otimização dos hiperparâmetros. A base de teste fica então reservada para o teste final, quando o modelo já estiver otimizado. Validação Cruzada Uma outra maneira de verificar a performance e capacidade de generalização do seu modelo é o método de validação cruzada. A validação cruzada é semelhante ao holdout no sentido em que também treina e testa o modelo em partes diferentes do dataset. Porém, ao contrário do holdout, que utiliza apenas uma divisão entre base de teste e base de treino, a validação cruzada consiste na utilização de várias divisões do dataset. A validação cruzada nos fornece uma indicação melhor do quão bem o modelo se sairá com novos dados, já que, por meio das várias divisões, esta acaba testando o modelo no dataset inteiro, em contraste com o holdout que, por possuir apenas uma divisão, acaba dependendo de como os dados foram divididos entre as bases de treino e teste. Além disso, validação cruzada é o método mais indicado quando possuímos poucos dados, já que a divisão em apenas duas bases pode acabar não fornecendo bases de treino e de teste boas o suficiente. K-fold é provavelmente o mais conhecido e utilizado método de validação cruzada. Consiste em dividir o dataset em k grupos, selecionando um para ser o grupo de validação e os outros k-1 grupos para serem grupos de treino para o modelo. O modelo é treinado e avaliado. Depois dessa primeira iteração, um dos grupos que anteriormente era de treino torna-se o grupo de validação e o antigo grupo de validação passa a ser um grupo de teste. Esse processo se repete até que todos os k grupos tenham sido utilizados como grupo de validação. No final, a performance do modelo é calculada como a média de sua performance em cada iteração. Por exemplo, em uma validação cruzada 5-fold, o dataset é dividido em 5 grupos e o modelo é treinado 5 vezes, para que cada grupo tenha a chance de ser um grupo de validação. Este processo pode ser observado na imagem abaixo. Agora que temos os conceitos mais claros, é hora de avaliar seu modelo. Existem diversas métricas para determinar a qualidade de um modelo. Por exemplo, em um problema de classificação, um modelo bom é aquele que tem alta acurácia, ou seja, aquele que acerta mais: Por outro lado, para avaliar um problema de regressão, uma métrica muito comum é a do erro quadrático médio. Sendo assim, um modelo bom é aquele que possui menor erro quadrático médio, calculado por: em que n é o número de observações, yi é o valor real e ŷi é a predição do modelo. Algumas consequências da diferença (ŷ-y) ser elevada ao quadrado são: É importante notar que dependendo da sua aplicação, nem todos os erros são iguais. Em modelos binários — em que só existe certo ou errado, 0 ou 1 — é preciso introduzir o conceito de falsos positivos e falsos negativos. Imagine que devemos determinar se um paciente possui ou não determinada doença. Nesse caso, percebe-se que existem erros diferentes. É melhor falar a um paciente saudável que ele está doente ou falar a um paciente doente que ele está perfeitamente saudável? Mas afinal o que são falsos positivos e falsos negativos? Bem, como o nome sugere, falsos positivos são casos em que o resultado correto é negativo entretanto o resultado obtido é positivo. Analogamente, falsos negativos são aqueles casos em que o resultado correto é positivo, contudo o programa indica um resultado negativo. A tabela a seguir é chamada matriz de confusão; ela mostra os quatro tipos de classificação do modelo. Dependendo da sua aplicação, pode ser, por exemplo, mais vantajoso diminuir ao máximo o número de falsos negativos. Imagine o exemplo dado anteriormente de determinar se um paciente possui uma doença ou não (em que o resultado do exame é positivo se o paciente está doente). Neste caso é melhor dizer a um paciente saudável que ele tem a doença (falso positivo) do que informar a um paciente doente que ele está saudável (falso negativo). Com base na matriz de confusão, podemos definir várias outras métricas, algumas das quais já mencionamos em um de nossos posts. Como já vimos, o conceito de acurácia está ligado com a frequência com a qual o seu modelo está certo. Em termos de falsos positivos (FP), falsos negativos (FN) e dos outros termos, podemos escrever: Já a precisão diz respeito à taxa de acerto do modelo nas observações classificadas como positivas. Para facilitar a visualização, voltando ao exemplo do paciente, precisão seria a resposta da pergunta: “dos pacientes com exame positivo realmente têm a doença?”. Para calculá-la, usamos a fórmula: Por fim, recall (ou true positive rate) tem a ver com a capacidade do modelo de identificar observações positivas. Ainda no exemplo do paciente, recall seria a resposta da pergunta: “dos pacientes que têm a doença, quantos foram diagnosticados?”. É calculado por: Uma outra métrica para avaliar seu modelo é a F1. Tal métrica leva em conta tanto o recall (r) quanto a precisão (p) do seu modelo, fazendo uma média harmônica entre os dois. O melhor desempenho é aquele em que a média é 1 e o pior, em que o resultado é 0. Uma propriedade da média harmônica é que se qualquer um dos dois valores (recall ou precisão) for baixo, a média será baixa. Dessa forma, um modelo bom precisa ter recall e precisão bons. Outras métricas importantes para checar a performance do modelo são a curva ROC (Receiver Operating Characteristic) e a AUC (Area Under the ROC Curve). ROC é uma curva de probabilidade que mostra a performance de classificação. Os fatores que influenciam a curva ROC são TPR (True Positive Rate ou recall) e FPR (False Positive Rate): Vários modelos de classificação binária podem gerar um número como saída. Podemos então falar de um threshold, que indica até que ponto podemos considerar que a predição é negativa e a partir de que ponto ela passa a ser considerada positiva. A curva ROC é obtida ao plotar-se um gráfico TPR × FPR, variando-se o threshold. Já AUC representa a área abaixo da curva ROC e serve como uma medida de separabilidade. Se tomarmos o exemplo do paciente, quanto maior AUC, mais o nosso modelo vai classificar pacientes doentes como doentes e pacientes saudáveis como saudáveis. O modelo ideal possui AUC igual a 1, ou seja consegue identificar todos os resultados corretamente. Um modelo ruim possui AUC próxima de 0, isso na verdade significa que os resultados previstos são opostos, isto é, se um paciente está doente o modelo dirá que está saudável e se está saudável dirá que ele está doente. Quando AUC está em 0.5 o modelo não possui a capacidade de separar as classes. Para ajuadr no entendimento da AUC, vamos analisar alguns casos específicos graficamente. No caso ideal, o modelo é capaz de separar as classes perfeitamente, em momento nenhum se sobrepondo. No modelo real, é normal que exista alguma sobreposição entre as duas classes, indicando casos de FN e FP. O valor de AUC=0.7 indica que há uma chance de 70% do nosso modelo distinguir as classes corretamente. Com o AUC=0.5 há total sobreposição das duas classes, ou seja o modelo não é capaz de diferenciar entre elas. Esses foram alguns conceitos básicos de predição, necessários para elaborar modelos capazes resolver qualquer os tipos mais diversos de problema! Nos próximos posts falaremos mais sobre diferentes modelos de predição e como utilizá-los, fique atento! Para saber mais sobre IA, siga também nossa página no Facebook."
https://medium.com/turing-talks/turing-talks-11-modelo-de-predi%C3%A7%C3%A3o-regress%C3%A3o-linear-7842709a593b?source=collection_home---------100----------------------------,Modelos de Predição | Regressão Linear,Um modelo simples de machine learning para encontrar a reta que melhor explica a relação entre as features e um target contínuo.,Piero Esposito,542,9,2019-07-21,"Escrito por Fernando Matsumoto, Guilherme Fernandes e Piero Esposito. No último post, falamos sobre os conceitos básicos de predição. Dando continuidade a esse assunto, essa semana vamos introduzir um primeiro modelo de predição: a regressão linear. Para acompanhar a parte matemática deste artigo é necessário entender o básico de cálculo (gradiente e derivadas parciais). Para isso, se quiser, recomendamos as aulas do prof. Possani na UNIVESP. Além disso, para a reprodução em código é essencial python e bibliotecas de data science, como pandas, numpy e matplotlib. Para facilitar a leitura, esse artigo vai seguir a seguinte estrutura: (i) vamos enunciar o problema da regressão linear com a nossa motivação; (ii) depois, vamos pra intuição matemática do que ocorre na regressão linear; (iii) feito isso, vamos implementar um regressor em código com dados simples, para você poder fazer as contas na mão se quiser e ver que faz bastante sentido; (iv) por último, vamos concluir esse Turing Talks. Dito isso, vamos lá: Quando falamos em predizer valores com auxílio de um dataset, estamos dizendo que queremos descrever o target (y) por meio das features (X). Para cumprir essa tarefa vamos usar o conceito matemático de regressão linear, que, em linhas gerais, aproxima o target das features por uma reta (uma função linear). Desse modo, podemos fazer afirmações sobre valores não disponíveis no dataset. A exemplo, temos os problemas de estimar o salário de um funcionário com base no “tempo de casa” na empresa ou o valor do IPVA de um carro com base no seu preço de venda. Nesses casos, basicamente plotamos num gráfico pontos correspondentes a várias observações do contexto que estamos estudando, e eles seguem a forma (x, y) sendo x a observação que temos e y a que queremos tornar possível a previsão. No final, fica mais ou menos assim: Quando dizemos prever o valor do y com base no x, que acaba sendo meio que o mote do Machine Learning, basicamente, queremos encontrar uma reta que seja a mais adequada e passa com mais “smoothness” pelos pontos, ou seja, a que melhor descreve os dados. O resultado do modelo vai ser uma reta, com equação f(x) = y = ax + b que vai passar por esses pontos. A visualização dela vai ser mais ou menos assim: O nosso trabalho vai ser encontrar os coeficientes angular e linear que tornam essa reta ideal — e isso vamos ver com bastante detalhe no tópico 2. Importante dizer que o y pode ser previsto também com base em vários x’s, o que ocorre quando temos mais de uma feature. Nesse caso, que fica mais difícil de visualizar, a aproximação não se dá por uma reta, mas por planos ou hiperplanos dependendo da quantidade de features. Para esses casos, o target da predição é calculado de forma que cada feature (x1, x2, x3…, xn) tenha um “peso” associado a si. Veremos isso com mais detalhes a seguir. Partindo para explicação matemática do modelo de regressão linear, nosso objetivo será encontrar uma função cujas variáveis independentes sejam as features e a dependente, o target. Dessa forma temos: onde ŷ é a predição do modelo, e Erro o quão longe está da previsão correta. Para cada combinação dos coeficientes (a1, a2, …, an, b), com os dados de treino, temos um Erro, isso é, uma função que determina o quão erradas as previsões do nosso modelo estão. A modelagem da regressão linear consiste em encontrar os valores dos coeficientes da função que minimizam o erro. No entanto, a função Erro tem um valor diferente para cada observação do dataset. Precisamos então converter essa função em um número que possamos minimizar. Em problemas de regressão, uma boa prática é usar a função do erro quadrático médio (como vimos no último post): onde m é o número de observações do dataset. Usamos essa equação não somente por apresentar bons resultados, mas porque, se apenas usássemos o erro médio, erros positivos poderiam cancelar os erros negativos. Ao considerar o erro ao quadrado, todos os termos se tornam positivos e não temos mais esse problema. Além disso, temos como efeitos colaterais que: Para encontrar a função f(x1, x2, … , xn) que minimiza o erro, usamos o método do gradiente, também conhecido como gradient descent. Para entender esse método, precisamos primeiro entender o que é o gradiente de uma função. De forma simplificada, ele é um vetor que aponta na direção em que uma função aumenta mais rapidamente. Por exemplo, considere a imagem abaixo, que mostra o gradiente de uma função de duas variáveis. O gradiente aponta para fora da região azul (que corresponde a valores pequenos de f) e para dentro da região vermelha (que corresponde a valores grandes de f). Sabendo disso, se encontrarmos o vetor gradiente para determinado ponto da função, é só ir na direção oposta dele para diminuir o y. Vale lembrar que isso só vale para variações muito pequenas. Para ilustrar, vamos pensar em uma função de duas variáveis f(x1, x2). Se olharmos o gráfico dessa função de cima, veremos algo que se parece com a figura abaixo. Cada linha azul é uma “fatia horizontal” do gráfico, ou seja, uma curva na qual o valor de f é constante. Conforme chegamos nas curvas mais internas, o valor de f diminui. Começando em algum ponto qualquer, podemos calcular o gradiente e variar os parâmetros x1 e x2 no sentido oposto. O método do gradiente consiste em fazer isso várias vezes até chegarmos no mínimo da função, como ilustrado abaixo. No nosso caso, o custo é o erro quadrático médio L: Dado que os valores do target são constantes (são parte do nosso dataset) e ŷ é uma função dos pesos, a função de custo pode ser escrita como L(a1, a2, …, an, b). Antes de partirmos para o algoritmo, vamos explicar a notação que usaremos: X = vetor de features com dimensões 1×n; sendo n = numero de features. X pode também ser uma matriz m×n em que cada uma das m linhas é uma observação diferente. Nesse caso, x1, x2, …, xm são as linhas de X. w = (a1, a2, …, an) = vetor de coeficientes da função, com dimensões n×1. b = escalar, conhecido como viés. É o termo constante na função f. ŷ = Xw + b, previsão do target do dado X ou, se X for uma matriz, vetor com os previsões para os dados integrantes da matriz. L = Σ(ŷ - y)², a função de custo que queremos minimizar. α = taxa de aprendizado, ou seja, o quanto, a cada iteração do modelo, vamos mexer nossos W e b na direção do gradiente. Um pseudocódigo desse modelo pode ser: Se você tiver bastante interesse na matemática por trás do modelo, vamos explicar agora como calculamos os gradientes de Erro em relação a w e b. Vale dizer que essas contas valem pra uma só variável de saída (uma feature), caso contrario complica bastante e sai do escopo deste artigo. Para cada iteração, temos: No algoritmo descrito acima, que chamamos de Batch Gradient Descent, para calcular o gradiente de f, precisamos calcular o erro da função em todas as observações do dataset. Isso significa que para atualizar uma única vez os pesos (realizar uma iteração de gradient descent), é necessário calcular m erros (onde m é número de observações do dataset), o que pode ser muito custoso. Um modo de consertar esse problema é usando o algoritmo de Stochastic Gradient Descent. Nesse algoritmo, em cada iteração, calculamos o gradiente do custo considerando apenas uma das observações. Ou seja, para cada iteração, realizamos: Nesse algoritmo, temos o problema oposto: atualizar os pesos também é computacionalmente caro. Um meio termo entre batch e stochastic gradient descente é Mini-Batch Gradient Descent. Nesse terceiro caso, usamos algumas observações a cada iteração, ou seja, ao invés de selecionar uma observação xi, selecionamos várias observações xi1, xi2, …, xik (onde k é chamado de batch size). Na figura abaixo, podemos ver uma comparação dos 3 métodos de gradient descent. O método estocástico (em roxo) se move de uma forma que parece “aleatória”. Isso ocorre porque usamos apenas 1 observação para estimar o gradiente, o que significa que a estimativa é muito ruim. No caso de mini-batch (em verde), como usamos mais observações, a estimativa melhora um pouco e o algoritmo se move de forma que parece “mais correta”. Por último, o batch (em azul) é o método que chega ao mínimo em menos iterações. No entanto, é necessário lembrar que uma iteração de batch gradient descent demora muito mais do que uma iteração de stochastic gradient descent. Observando que os métodos batch e stochastic são casos especiais de mini-batch (com batch size = m e 1, respectivamente), podemos concluir que o batch size do gradient descent afeta a velocidade de convergência do algoritmo. Para nossa sorte, não precisamos codar esse algoritmo na unha, graças a biblioteca “Scikit-learn” de Python. Esta disponibiliza vários modelos de machine learning incluindo a regressão linear. É importante se familiarizar com essa biblioteca, pois muitos modelos de predição são acessíveis por ela, já otimizados. No exemplo de código vamos fazer regressão linear em dois datasets: No Turing Talks de hoje pudemos entender mais sobre regressão linear e gradiente descendente, os quais certamente serão muito úteis na sua jornada em ciência de dados. Além disso, agora você sabe como é feito um modelo de machine learning na prática. Esse modelo ainda pode ser melhorado com técnicas de feature-engineering e funções de erro (ou custo) diferentes, mas isso fica pra uma outra hora! Nas próximas semanas seguiremos com os Turing Talks com mais modelos de predição. Caso queira saber mais sobre o mundo de IA, siga nossa página no Facebook e confira o que postamos por lá também. Por hoje é só, até mais!"
https://medium.com/turing-talks/turing-talks-12-classifica%C3%A7%C3%A3o-por-svm-f4598094a3f1?source=collection_home---------99----------------------------,Modelos de Predição | SVM,Aprenda a criar seu primeiro algoritmo de classificação com SVM.,Bernardo Coutinho,613,8,2019-07-28,"Escrito por Bernardo Coutinho. Bem vindos de volta ao Turing Talks! No post dessa semana vamos falar de SVM (Support Vector Machine), um algoritmo de aprendizado supervisionado muito utilizado quando se quer classificar dados em grupos diferentes — mas que também pode ser utilizado para regressão. Entretanto, para adentrarmos esse assunto, é preciso um pouco de conhecimento prévio de Python e de algumas bibliotecas de Data Science. Se você ainda não está muito confiante nessas áreas, dê uma olhada nos nossos Turing Talks anteriores sobre Python (parte 1 e parte 2) e sobre bibliotecas importantes. O SVM é um algoritmo que busca uma linha de separação entre duas classes distintas analisando os dois pontos, um de cada grupo, mais próximos da outra classe. Isto é, o SVM escolhe a reta — também chamada de hiperplano em maiores dimensões— entre dois grupos que se distancia mais de cada um (no caso abaixo, a reta vermelha). Após descoberta essa reta, o programa conseguirá predizer a qual classe pertence um novo dado ao checar de qual lado da reta ele está. Quando temos que classificar nossos dados em mais de duas classes, precisamos de algum modo de simplificar o nosso trabalho. A maneira mais simples e comum é dividir nosso problema multiclasse em várias classificações binárias, que podem ser one vs one ou one vs all (também chamada de one vs rest). A divisão em one vs one consiste em separar nossa situação em classificações binárias de cada par diferente. Ou seja, se tivermos as três classes A, B e C, faremos as comparações (A, B), (A, C) e (B, C), e a classe mais votada será a escolhida. Por exemplo, se o SVM escolheu A em (A, B), C em (A, C) e C em (B ,C), nosso resultado será (A, C, C) e a classe escolhida será a C. A divisão em one vs all consiste em comparar cada classe com todo o resto. Ou seja, se tivermos as mesmas três classes A, B e C, dividiremos nosso problema nas comparações (A, B+C), (B, A+C) e (C, A+B), e a classe que ganhar do resto será escolhida. Utilizando o mesmo dado do exemplo passado, o resultado das nossas comparações seriam (B+C, A+C, C), e novamente a classe escolhida será a C. No entanto, existem grupos que não podem ser separados somente por hiperplanos, como os da imagem ao lado. Nesses casos, utilizamos o SVM não linear para delimitar as duas classes, que traçará uma ou mais linhas retas ou curvas para separar as classes da melhor forma possível. Para separar esses tipos de exemplos, o algoritmo primeiro faz uma transformação não-linear do espaço para depois poder separar os grupos com um SVM linear. Dessa forma, apesar da separação ser um hiperplano no espaço das features (como chamamos o espaço depois da transformação), no espaço das entradas (como chamamos o espaço inicial) a separação é não-linear. Apesar de ser mais comumente utilizado para classificação, o SVM também pode ser utilizado para regressão, ou seja, para prever valores contínuos com base nos dados em vez de prever as classes às quais os dados pertencem. Nesse caso, nosso objetivo é achar o hiperplano que mais se aproxime dos dados. Novamente, o SVM considera apenas um subconjunto dos dados. Se na classificação ele considera apenas os pontos mais próximos ao hiperplano, na regressão, ele considera apenas os pontos mais distantes. Isso quer dizer que, dada uma tolerância épsilon (Ɛ), o SVM considera apenas os pontos que estão a uma distância maior que épsilon do hiperplano. O objetivo é, então, encontrar o hiperplano mais próximo desses pontos. Para casos não lineares, podemos também usar a mesma técnica de transformação do SVM não-linear para fazer uma regressão não-linear. Assim como todos os outros modelos de predição, o SVM tem suas vantagens e suas desvantagens. Esse tipo de modelo se destaca na classificação de dados espalhados de maneira não regular, já que a separação não precisa ser linear e nem a mesma para todos os dados. Esse algoritmo é muito interessante para iniciantes também porque não é necessário tanto conhecimento da base de dados para conseguir uma predição com boa acurácia. Além disso, o SVM funciona bem em espaços com muitas dimensões (muitas features) e é garantido a convergir para o melhor hiperplano possível, visto que seu algoritmo não se perde em mínimos locais como acontece com redes neurais (que veremos mais adiante). Contudo, o resultado do SVM é dificilmente interpretável (mas possível) e, conforme o tamanho do dataset vai aumentando, o tempo necessário para fazer os cálculos cresce muito rapidamente e a interpretabilidade cai mais rápido ainda. Ao contrário da regressão linear, o SVM é mais capaz de trabalhar com dados não separáveis linearmente, então é a melhor escolha para datasets mais complexos. Além disso, o SVM sempre conseguirá achar o melhor hiperplano possível para separar duas classes, mas não confunda “melhor possível” com perfeito, às vezes o melhor hiperplano não consegue separar tão bem os dados. Entretanto, por ser um modelo mais simples, a regressão linear consegue resultados bem mais rápidos conforme a quantidade de dados aumenta, quando comparada ao SVM não-linear. Antes de aprofundar essa parte de SVM, é importante esclarecer que entender a matemática desses algoritmos não é necessário para poder aplicá-los. No entanto, esse estudo pode ajudar a conhecer melhor suas limitações e seus usos ideais. Não se preocupe se você se perder nesses cálculos, se essa seção não te interessar recomendamos pular para a aplicação na próxima parte. Primeiramente, cada dado é representado por um vetor (xᵢ, yᵢ), em que xᵢ é um vetor das coordenadas (features) de cada dado e yᵢ é a classe a qual o dado pertence (1 ou −1). Assim como uma reta é representada por “y = ax + b”, um hiperplano é representado por “w·x + b = 0”, em que o w é um vetor normal ao hiperplano, que determina sua direção, b é responsável por deslocar linearmente o hiperplano no espaço, sem alterar sua direção, e o conjunto de pontos pertencentes ao hiperplano são todos aqueles pontos x que satisfazem à equação apresentada. Mas por que “w·x + b = 0” forma um hiperplano? Bem, “w·x” é o produto escalar do vetor w com o vetor x (distância da origem até x), que será igual a zero quando w e x forem ortogonais. Dessa forma, os pontos x que satisfazem “w·x = 0” formarão um hiperplano que passa pela origem. Ao adicionar o b à equação, criaremos novos hiperplanos de mesma direção porém deslocados pra fora da origem. E, como nosso alvo é achar o hiperplano que melhor separe as duas classes, isso significa que, matematicamente, precisamos encontrar o w e o b ideais. Agora, para determinar o hiperplano ideal de separação entre duas classes, primeiro é necessário encontrar os hiperplanos que estão no limite de cada grupo (as margens). Para tal, utilizaremos os support vectors — os pontos de cada classe mais próximos do hiperplano de separação—por onde as margens devem passar. As representações destes são “w·x + b = 1” e “w·x + b = −1” para suas respectivas classes, como mostrado na imagem abaixo. Assim, todos os dados da classe 1 satisfazem a inequação “w·x + b ≥ 1” e todos os dados da classe −1 satisfazem a inequação “w·x + b ≤ −1”. Juntando essas duas inequações temos que “yᵢ · (w·xᵢ + b) ≥ 1”. Como a distância de um hiperplano até a origem é dada por b/|w|, a distância entre nossos dois hiperplanos será “(1 − b)/|w| − (−1 − b)/|w|”, cujo resultado é “2/|w|”. Já que o hiperplano ideal é aquele cuja distância a cada grupo é a maior possível, nosso w ideal é aquele valor para o qual “2/|w|” seja máximo. Portanto, o w do nosso hiperplano ideal será o menor w que satisfaça a inequação “yᵢ · (w·xᵢ + b) ≥ 1” para todos os pontos i. A partir disso, utilizamos algum algoritmo de otimização como gradient descent (explicado no Turing Talks passado) ou Multiplicadores de Lagrange para encontrar o w e o b do melhor hiperplano. Antes de qualquer coisa, lembre-se de importar as bibliotecas que iremos utilizar para aplicar o SVM. As principais são pandas, numpy, matplotlib e scikit-learn. Para exemplificar essa forma de classificação, usaremos a base de dados “Iris”, que contém dados sobre 150 flores de 3 espécies diferentes. A princípio, é recomendável usar alguns comandos simples para entender melhor sua base de dados. Depois que entendemos o nosso dataset, já podemos testar a acurácia do nosso modelo, para saber se ele conseguirá predizer a realidade com uma boa precisão. Finalmente! Testamos o nosso modelo e vimos que ele conseguiu uma acurácia de 98%! Agora, podemos começar a predizer dados cuja classe não sabemos. Pronto. De acordo com nosso modelo, podemos afirmar que essa flor é da espécie 2! Primeiro, o algoritmo recebe os dados das flores em um formato similar ao do gráfico abaixo (só que em mais dimensões). Depois, para encontrar as delineações e separar as classes, ele transforma não linearmente a entrada para depois separá-la por um hiperplano. Assim, as delimitações no espaço das entradas, mostrado na imagem abaixo, são não lineares, como visto anteriormente. Por fim, ele consegue predizer a qual grupo pertencerá um novo dado com base na área na qual ele se encontra. Com as informações desse post você já consegue criar seu primeiro modelo de classificação! Se ainda tiver alguma dúvida, dê uma olhada no notebook de SVM disponibilizado no final dessa publicação, ou mande uma mensagem pro nosso Facebook, que com certeza estaremos dispostos a te ajudar. Mas se o SVM não atender suas necessidades, dê uma olhada no nosso Turing Talks de semana passada sobre Regressão Linear, e nos siga no Facebook e aqui no Medium porque nas próximas semanas vamos postar sobre vários outros modelos de predição!"
https://medium.com/turing-talks/turing-talks-13-modelo-de-predi%C3%A7%C3%A3o-knn-3be880c9b9d1?source=collection_home---------98----------------------------,Modelos de Predição | KNN,Descreva um ponto pela sua vizinhança.,Felipe Azank,766,7,2019-08-04,"Escrito por Felipe Azank. Você conhece a frase “Diga-me com quem andas e direi quem tu és”? Esse ditado, regularmente dito por pais com medo de determinadas influências, nos faz refletir o quanto as características das pessoas mais próximas a você podem te definir de alguma maneira. Nesse Turing Talk veremos em detalhe um algoritmo de classificação e predição que parte justamente dessa ideia, utilizando ferramentas matemáticas para comparar dados semelhantes entre si com o objetivo de inferir uma determinada característica sobre um ponto desconhecido. Para que o algoritmo consiga classificar o dataset com base nos dados fornecidos e prever a característica desejada para um item, o modelo deve seguir 3 passos essenciais: Para que todo o mecanismo de comparação entre itens e o estudo das semelhanças ocorra, é necessário transformar, de alguma maneira, as informações e características presentes no dataset em elementos matematicamente manipuláveis, portanto, o primeiro passo a se tomar nesse sentido é normalizar os dados quantitativos, colocando as features em uma base apropriada (valores transformados para uma escala entre 0 e 1). Isso pode ser facilmente realizado utilizando a classe StandardScaler da scikit-learn (que será explorado no exemplo ao final). Após normalizar os dados, é possível calcular as distâncias entre os itens. E para fazer isso é só usar aquela fórmula padrão da distância entre dois pontos que vemos no ensino médio, certo? Nem sempre. Dependendo da situação pode ser vantajoso utilizar outras métricas de distância. O próprio scikit-learn disponibiliza várias métricas, como: Distância Euclidiana, Cosine Similarity, Manhattan Distance, Minkowski e Distância Ponderada, sendo as duas primeiras, as mais utilizadas. Essa métrica, também conhecida como distância simples, trata-se da raiz quadrada da soma das diferenças de cada característica ao quadrado. Embora, a primeira vista soe um pouco complicada, basta notar que trata-se da distância simples entre dois pontos (aprendida no ensino médio) só que com mais “dimensões”, sendo, cada dimensão, uma característica da base de treino. Essa métrica vale ser destacada pela sua crescente utilização em problemas nos quais a direção dos vetores é mais importante que a magnitude, como é o caso dos word vectors, que não serão discutidos nesse post :( Para entender a como funciona essa métrica observe a imagem abaixo, na qual cada observação é representada como um vetor. No caso da esquerda, os dois vetores apontam para a direita e o cosseno do ângulo entre eles é positivo. No caso do meio, em que os vetores apontam em sentidos quase opostos, o cosseno é negativo. E no último caso, em que os vetores são perpendiculares, o cosseno é nulo. Dessa forma, percebemos que o cosseno de θ indica quão próximos estão os sentidos de A e B. Por isso, definimos a métrica cossine similarity como: A última métrica a ser destacada, consiste em uma métrica de distância muito intuitiva para aqueles que moram em regiões com quarteirões bem delimitados. A Manhattam Distance (também conhecida como geometria do Taxi) consiste na computação da distância entre pontos como a soma da diferença absoluta entre as coordenadas. Em outras palavras, o quanto é necessário percorrer em X e em Y para chegar de um ponto a outro. Matematicamente, a distânica pode ser computada da seguinte forma (já expandindo para mais de duas dimensões). Abaixo, n representa o número de dimensões dos pontos, x e y dois pontos distintos, e x_i e y_i , cada um dos valores das dimensões. Ok, já sabemos o que o algoritmo faz e como ele funciona no geral, mas quantos vizinhos K seriam necessários para o meu programa classificar e predizer com eficiência? Bom, há muitas formas de escolher o valor do K, entre as mais utilizadas seriam: fazer uma validação cruzada, que faria com que o próprio programa encontrasse o melhor valor a se utilizar, ou simplesmente usar a raiz quadrada aproximada do número total de de dados da base de teste que você está analisando (técnica usada em datasets pequenos). Contudo, não existe um método que define com confiança o melhor valor. Como já explicado em detalhe no Turing Talks #10, os problemas de Underfitting e Overfitting são de grande preocupação para qualquer pessoa que utiliza algum algoritmo de predição, nesse sentido, o KNN não é uma exceção. Uma ação que gera Overfitting no modelo seria a escolha de um K muito pequeno. O algoritmo, ao considerar uma pequena quantidades de vizinhos, acabará predizendo características de determinados data points com base no ruído, o que aumentará o erro das previsões. Já uma atitude que gera Underfitting seria a escolha de um K muito grande. Nesse caso, o algoritmo passará a considerar data points muito distantes, o que dificultará, ou até impossibilitará uma identificação de padrões mais condizente para o ponto. O algoritmo KNN é considerado um lazy algoritm, isso significa que esse modelo não utiliza a base de treino para desenvolver uma função de discriminação, ao invés, disso, ele apenas realiza as predições calculando todas as distâncias relativas dos itens e prevendo as características por meio da moda. Portando, podemos perceber que o KNN não possui uma “fase de treino” seguida ao pé da letra, o que traz aspectos positivos e negativos para a implementação do algoritmo. Por ser uma técnica não paramétrica, sem assumir nada do dataset, o KNN é bom para dados que não sabemos muitas informações sobre, além de ser bem versátil para dados não lineares, tornando-o muito bom para qualquer problema de classificação de características em datasets pequenos. Contudo, o fato do algoritmo não desenvolver uma função de discriminação faz com que este tenha que memorizar todo o dataset de treino e tenha que calcular todas as distâncias entre os data points para promover uma classificação/predição. Esses fatores são responsáveis por dificultar, ou até impossibilitar, o uso do KNN em datasets muito grandes, uma vez que encontramos limitações de tempo e de eficiência das máquinas, nos fazendo recorrer a outros algoritmos que de fato desenvolvem uma função de discriminação (eager algorithms). Agora que já vimos como o algoritmo funciona, como calcula a semelhança entre itens e como classifica data points novos, está na hora de vermos o resultado disso na prática em um exemplo bem simples, atentando-se para as bibliotecas que importaremos e as ações tomadas antes da implementação do algoritmo em si. Após tratarmos das características principais do algoritmo K Nearest Neighbors, podemos concluir que trata-se de um modelo não paramétrico que classifica com base na similaridade entre elementos, sendo assim, muito versátil para datasets pequenos e com relações não lineares. Ademais, por ser um lazy learning algoritm devemos destacar que esse modelo não é adequado para datasets muito grandes devido a limitações de tempo e de eficiência tecnológica. Entretanto, uma observação final sobre o KNN deve ser levantada: como o modelo se baseia no processo do cálculo de distâncias entre pontos, a performance do modelo pode ser comprometida com a chama Maldição da Dimensionalidade, fator esse que será explicado em mais detalhes em um próximo Turing Talks. Enfim, esperamos que tenha gostado dessa breve explicação sobre um dos algoritmos mais triviais da área de aprendizagem supervisionada, perfeito para aqueles que querem começar a mexer em Machine Learning. Para saber mais, acompanhe o Grupo Turing no Facebook e Linkedin e nossos post no Medium."
https://medium.com/turing-talks/turing-talks-14-modelo-de-predi%C3%A7%C3%A3o-regress%C3%A3o-log%C3%ADstica-7b70a9098e43?source=collection_home---------97----------------------------,Modelos de Predição | Regressão Logística,Quando os seus dados não são suficientes para treinar um bom modelo de classificação.,Fernando Matsumoto,653,10,2019-08-11,"Escrito por Fernando Matsumoto. Nos últimos posts, vimos alguns modelos de predição, como o SVM, que é usado principalmente para classificação. Hoje vamos ver mais um modelo de classificação: a regressão logística (sim, a regressão logística é um modelo de classificação). Antes de ver esse modelo, é necessário entender um pouco de de regressão linear (Turing Talks #11) e de probabilidade. Neste post, vamos introduzir um pouco do básico de probabilidade, mas se você quiser saber mais, teremos um post um pouco mais aprofundado na próxima semana. Para motivar esse novo modelo, vamos analisar brevemente um dos modelos de classificação que vimos nas últimas semanas: o SVM. De forma simplificada, em um problema de classificação binária com 2 features, o SVM tenta achar curvas que separem os pontos de acordo com sua classificação. Com isso, podemos predizer a classificação de um ponto arbitrário olhando em qual lado da reta ele está. No entanto, em geral, os modelos cometerão erros. Às vezes, pode ser importante ter uma ideia da confiança do modelo na predição feita. Nesses casos, poderíamos usar a distância do ponto à reta como uma medida de confiança. Uma medida ainda melhor seria se a saída do modelo fosse a probabilidade da observação ter classe 1 (supondo um problema de classificação binária, com classes 0 e 1). É exatamente esse tipo de modelo que vamos construir no resto desse post. A partir daqui, vamos tentar entender como a regressão logística funciona matematicamente. Recomendamos fortemente que você tente entender o modelo, mas se você não estiver interessado na parte matemática, é só pular para a parte 6. Para acompanhar o resto da parte matemática desse post, é necessário saber o básico de probabilidade. Se você já se sentir confortável com o assunto, sinta-se livre para pular essa introdução. Usamos o conceito de probabilidade cotidianamente; por exemplo, quando ouvimos na previsão do tempo que a probabilidade de chuva amanhã é de 20%. Essa frase reflete a nossa incerteza sobre um evento: é improvável que chova amanhã, mas não temos certeza do que acontecerá de fato. No exemplo anterior, dizemos que “chover amanhã” é um evento, de forma que podemos escrever P(chover amanhã) = 20%, onde P(A) indica a probabilidade de um evento A. Essa probabilidade pode variar entre 0% e 100%, onde 0% indica certeza de que A não ocorrerá e 100% indica certeza de que A ocorrerá. É muito comum utilizarmos, ao invés de porcentagens entre 0% e 100%, números entre 0 e 1. Nessa notação, a probabilidade de chuva seria 0,20. Ainda no exemplo da chuva, qual seria a probabilidade de não chover amanhã? Podemos escrever uma vez que certamente ou choverá ou não choverá. Dessa forma, a probabilidade de não chover é igual a 1 − P(chover) = 0,80. Por último, como esse post trata de classificação, vamos escrever os eventos de maneira um pouco diferente. Denotando a classe de uma observação por y, o evento “a observação tem classe 1” pode ser escrito como “y=1”. Analogamente, se a probabilidade da classe ser 0 for de 32%, podemos escrever P(y=0) = 0,32. O nosso objetivo, como vimos no final da seção 1, é encontrar um modelo de regressão que calcule a probabilidade de um dado ponto ter classificação 0 ou 1 (que chamaremos também de classificações negativa e positiva, respectivamente). Nesse momento, podemos pensar na regressão linear que vimos duas semanas atrás. Será que podemos usar ela para estimar a probabilidade de classificação positiva? Não, pelo menos não diretamente. Observe que a regressão linear gera uma saída no intervalo (−∞, +∞), mas probabilidades estão no intervalo [0, 1]. Para resolver isso, precisamos de uma função que ligue esses dois intervalos: uma função de ligação. A função que utilizaremos é a função logit(x), que recebe um número entre 0 e 1, e retorna um número real: Para interpretar essa função, primeiramente, definimos a chance (odds) de um evento A como: onde A’ é a negação de A. Por exemplo, considere que chove uma vez a cada dez dias, de forma que a probabilidade de chover é 1/10 = 0,1. Então, odds = 0,1/0,9 = 1/9, ou seja, a chance de chover é de 1 para 9 (para cada dia de chuva, há 9 dias sem chuva). Tomando a probabilidade de um ponto x ter classe positiva como p = P(y = 1), temos então que: de forma que a função logit está intimamente relacionada com a chance de um evento ocorrer. Veremos mais a frente que essa relação nos ajudará a interpretar os resultados obtidos com a regressão linear. Agora que temos um número real, podemos estimá-lo usando um regressor f qualquer. Lembrando que para classificar um ponto queremos estimar o probabilidade de y = 1, temos: onde σ é a inversa de logit, também chamada de função sigmoide ou logística. É por isso que o modelo é chamado de regressão logística: ele calcula a probabilidade P(y=1) como σ(f(x)), onde σ é a função logística e f(x) é uma função de regressão. Mas qual regressor devemos usar como f? No caso da regressão logística, utiliza-se um regressor linear: Após predizer a probabilidade de um dado ponto ter classe positiva, o algoritmo precisa classificar esse ponto. Para fazer isso, simplesmente escolhemos a classe mais provável: se p > 0,5, a classe predita é positiva; caso contrário, é negativa. Resumidamente, a regressão logística prediz P(y=1) usando um modelo linear para logit(P(y=1)): Obs: chamamos os modelos que adaptam a regressão linear de modelos lineares generalizados. Em particular, dizemos que a regressão logística é um modelo linear generalizado com função de ligação logit(x). Até agora falamos somente de classificação com duas classes. No entanto, é possível estender a regressão linear para mais de duas classes. Há dois modos principais de fazer isso. O primeiro já vimos quando falamos de SVMs: trata-se do modelo “one-vs-rest”, em que, para cada classe, criamos um classificador que nos dá a probabilidade de um ponto pertencer ou não àquela classe. A classificação escolhida seria então aquela com a maior probabilidade. O outro modo, que chamamos de multinomial, consiste em adaptar a função de ligação para que o modelo consiga predizer diretamente a probabilidade de cada classe. Para conseguir interpretar os coeficientes wi, vamos primeiro entender o seu significado em um caso específico. Suponha que o nosso objetivo seja modelar a probabilidade de alguém desenvolver câncer de fígado com base em seu histórico familiar. Denotando por p a probabilidade de câncer e por x o número de familiares com câncer, temos: Como eʷ¹ é a razão entre a chance de câncer quando x=1 e quando x=0, podemos dizer que para cada familiar a mais com câncer, a sua chance de desenvolver câncer é multiplicada por eʷ¹. Estendendo isso para várias features, podemos dizer que: A primeira vista, poderíamos pensar em treinar a regressão logística do mesmo modo que treinamos a regressão linear: minimizando o erro quadrático médio. No entanto há alguns problemas com o erro quadrático médio (MSE) para regressão logística, como: Uma função de custo melhor no caso da regressão logística é a entropia cruzada binária. Esse nome pode parecer complicado, mas a interpretação é relativamente simples. Buscamos um modelo que dê probabilidades altas a observações positivas e probabilidades baixas a observações negativas. Pensando em uma observação positiva, a nossa função de custo deve ser elevada se p = P(y=1) for baixo. Essa condição é modelada pela entropia cruzada H: De forma análoga, podemos definir Hi = −ln(1 − p) para observações negativas. Juntando essas duas definições temos: Observe que, para observações positivas, o termo com (1−y) se anula e, para observações negativas, o termo com y se anula. A entropia cruzada é¹: Dessa forma, a entropia cruzada serve como uma medida de incerteza. A entropia cruzada no dataset inteiro é então a soma da entropia cruzada em cada observação: Na fórmula acima, yi é a classe da i-ésima observação e p é a probabilidade da observação ter classe positiva, ou seja, a saída da regressão logística. Como p = σ(f(x)), onde f é a regressão linear, o custo J depende dos parâmetros wi. Para visualizar melhor a entropia cruzada, vamos analisar o seu comportamento em dois datasets, cada um com apenas uma feature. No gráfico da direita, todos os pontos com x > 0 são positivos, enquanto os pontos com x < 0 são negativos. Dessa forma, p é quase sempre 0 ou 1 (alta confiança) e vai de 0 para 1 muito rapidamente, o que corresponde a uma baixa entropia. Já no gráfico da esquerda, os dados estão muito mais espalhados e misturados (por exemplo, há uma observação positiva no meio de observações negativas). Isso significa que o modelo não consegue aprender tão bem e p passa mais lentamente de 0 para 1, correspondendo a uma maior entropia. Agora que temos uma boa função de custo, o treinamento consiste em achar os parâmetros wi que minimizam o custo², ou seja, aqueles que maximizam a certeza (entropia) do modelo. Isso pode ser feito utilizando gradiente descendente ou outras técnicas, como IRLS. Visto que tanto a regressão logística como o SVM linear são modelos que separam as classes com retas, você pode estar se perguntando qual a diferença entre eles. Abaixo, listamos algumas dessas diferenças conceituais: Na prática, ambos os modelos apresentam resultados similares. Nesse caso, a regressão logística pode ser melhor, uma vez que fornece as probabilidades das classes. No entanto, em um problema que não é linearmente separável, ou seja, no qual não há uma reta ou plano que separe as classes, um SVM não linear apresentará resultados muito melhores. Agora que já entendemos como funciona a regressão linear, é hora de aplicá-la em algum dataset. Utilizando a biblioteca scikit-learn, o código fica bem parecido com o dos últimos modelos. Agora você está a par de mais um modelo, mais especificamente um modelo probabilístico de predição. Em nosso próximo post, vamos entender um pouco melhor alguns conceitos de probabilidade para depois aplicá-los em mais modelos de machine learning. Curtiu? Então não deixe de criar sua conta no Medium e seguir a gente para continuar aprendendo mais modelos e outros assuntos de IA. Dê uma olhada em nossos outros posts e acompanhe o Grupo Turing no Facebook e no LinkedIn, temos várias oportunidades em IA por lá também. Por hoje é só, até uma próxima! ¹ Na prática, p nunca é exatamente 0 ou 1, pois, para isso, a saída da regressão linear tem que ser −∞ ou +∞. Dessa forma, quando calculamos log(p) em p=0 ou log(1−p) em p=1, o resultado não é infinito, mas apenas um número muito grande. ² A entropia cruzada é igual a −ln(L), onde L é a verossimilhança. Isso significa que minimizar o custo é equivalente a encontrar um estimador de máxima verossimilhança."
https://medium.com/turing-talks/turing-talks-15-fundamentos-de-probabilidade-para-machine-learning-73dd3202e4c5?source=collection_home---------96----------------------------,Fundamentos de Probabilidade para Machine Learning,Conceitos básicos para o entendimento dos modelos de IA,Guilherme Fernandes,899,12,2019-08-18,"Escrito por Guilherme Fernandes, Fernando Matsumoto e Camila Lobianco Bem vindos a mais um Turing Talks, essa semana vamos interromper a sequência de modelos de predição para aprender a matemática que fundamenta os modelos de machine learning que veremos nos próximos meses. Sem mais delongas, vamos lá! Já notaram que a maioria das situações em que nos dispomos na vida possuem características aleatórias? Seja jogando cartas, esperando um ônibus no terminal ou o resultado de jogos de futebol, não é possível contar todos esses eventos. Pra nossa felicidade existe uma ciência que estuda isso, a probabilidade; não poderíamos desejar nada melhor, afinal isso nos permite raciocinar efetivamente diante dessas situações em que ter certeza é impossível. A compreensão dessa área envolve muitos conceitos, então se quiser ir diretamente a algum em específico, sinta-se a vontade. O primeiro ponto a ser compreendido em teoria de probabilidades são os eventos. Eles nada mais são do que o conjunto dos resultados que desejamos obter. ExemplosSuponha que lancemos dois dados, dessa situação podemos definir vários eventos, tais como: Aos eventos estão relacionados o que chamamos de espaço amostral, que é o conjunto de todos os resultados possíveis. É um valor numérico associado a um evento, informalmente denotamos por P(evento). Por definição é: P(evento) = Número de resultados desejados / Número de resultados possíveis Ou seja, é o número de elementos do evento dividido pelo número de elementos do espaço amostral. ExemplosPara os exemplos já apresentados, os resultados são: Em que, 36 é o número de resultados possíveis no lançamento dos dados. Algumas propriedades básicas A probabilidade, por ser conceitualmente abstrata assume alguns tipos de interpretação. Aqui citaremos brevemente duas: Imagine que lancemos um dado n vezes e realizemos uma contagem do número de ocorrências de determinado valor. Se fizermos n=1 vez, o número observado não é suficiente para definir com grau de certeza o valor de probabilidade. Mas a medida que n aumenta tendemos ao valor exato da probabilidade. Logo, os que se dizem frequencistas enxergam probabilidade por meio da contagem dos acontecimentos em um grande número de realizações da experiência aleatória, sendo portanto, uma frequência relativa das observações. Note que essa forma não admite valores exatos. Quando não se dispõe da possibilidade de repetir o mesmo experimento nas mesmas condições, ou em grandes quantidades se torna insignificante observar um fenômeno um grande número de vezes. Assim, temos uma alternativa pra compreensão de probabilidade. Essa se dá como um grau de confiança que atribuímos a uma observação. Nesse caso a probabilidade está associada a hipóteses; sendo assim, os valores confirmam ou refutam as hipóteses. Dois eventos são independentes quando a ocorrência de um evento não influencia na ocorrência do outro?Matematicamente a igualdade P(A ∩ B) = P(A).P(B) indica independência de A e B. ExemploEm uma prova de múltipla escolhe existem duas perguntas, a primeira contém 4 alternativas e a segunda contém 3 alternativas. Qual a probabilidade de adivinhar aleatoriamente a resposta certa nas duas perguntas? O ponto chave da questão é entender que acertar na primeira pergunta não influencia no acerto da segunda. Logo, são independentes e P(acertar 1ª ∩ acertar 2ª)= P(acertar 1ª).P(acertar 2ª) = (1/4).(1/3) = 1/12 Uma variável aleatória X é uma função que associa cada valor do espaço amostral a um número. Em outras palavras é uma regra para descrever os eventos desejados. Ainda mais simplificadamente, é uma variável que assume diferentes valores a cada vez que é executado o experimento associado a mesma. ExemploVamos novamente lançar dois dados, assim temos um espaço amostral S: S = {(1,1),(1,2),(1,3),…,(6,6)} Podemos definir X, por exemplo, como X(a,b) = máx(a,b), ou seja, para cada (a,b) no espaço amostral S a variável aleatória X está associada ao maior valor entre os dois números. P(X = 1) = P({(1,1)}) = 1/36P(X = 2) = P({(2,1), (2,2), (1,2)}) = 3/36P(X = 3) = P({(3,1), (3,2), (3,3), (2,3), (1,3)}) = 5/36P(X = 4) = P({(4,1), (4,2), (4,3), (4,4), (3,4), (2,4), (1,4)}) = 7/36 Da mesma forma, P(X = 5) = 9/36 e P(X = 6) = 11/36. Assim, determinamos a probabilidade para todos os resultados possíveis de X, e portanto, agora sabemos como a variável aleatória está distribuída. Perceba que podem existir infinitas distribuições de variáveis aleatórias, mas existem algumas mais importantes, as quais veremos a seguir. É muito mais comum, ao longo de um curso de probabilidade, pensarmos em como as variáveis estão distribuídas ao invés de pensar na sua regra; isso acontece porque na prática temos acesso aos dados, e analisar a distribuição é tarefa fácil analisando as frequências relativas, ou seja, contando aparecimento de valores. Além disso, existem algumas distribuições com características singulares, e que aparecem frequentemente em problemas. Tipos de VAs Classificamos as variáveis aleatórias em discretas e contínuas, esses termos podem não ser muitos familiares aos leitores, portanto explicaremos a seguir. Vamos supor que você queira contar o número de vacas em uma fazenda. Eu posso ter três, quatro, ou sete, mas não posso ter quatro e meio. Isso porque, para contar uma vaca, nós estamos supondo que existe um intervalo de uma unidade entre uma vaca e outra. Se eu fosse fazer um conjunto que servisse para diferenciar minhas vacas, entre uma e outra eu sempre teria um intervalo aberto de uma unidade de medida, com uma vaca em cada extremo. Certo, isso pode parecer um palavreado matemático demais para uma coisa tão trivial, mas é que, no fundo, esse processo de contar vacas só é desse jeito porque ele segue um modelo matemático de dados discretos, que são definidos exatamente dessa forma. Por outro lado, se eu quiser contar a quantidade de tempo que eu passo em uma viagem de avião, existem infinitos valores de horas exatas que eu posso chegar. Isso só é possível porque o tempo é uma unidade de dado contínuo. Ou seja, entre um milésimo de segundo e outro existem outros “milionésimos” de segundo. E entre os milésimos de milésimos de segundo existem “bilionésimos” de segundo. Essa lógica valerá para qualquer intervalo possível que você quiser pegar, pois entre cada intervalo de tempo existe um intervalo de tempo ainda menor que dá a esse fator uma sensação de continuidade infinita. Para sintetizar essa ideia, podemos dizer que o conjunto dos inteiros é um que possui uma estrutura de dados discretos e o dos reais, contínuos. Existem infinitas variáveis aleatórias, veremos a seguir as mais importantes. BernoulliImagine que você faça um experimento com dois resultados possíveis: sucesso ou falha. Sucesso acontece com probabilidade p, enquanto falha acontece com probabilidade 1-p. Uma variável aleatória que obtém valor 1 em caso de sucesso e 0 em caso de falha é chamada de Bernoulli. No que segue, usaremos frequentemente a escrita X ~ Bernoulli(p), que dispensa dizer a frase “a variável aleatória X tem distribuição de Bernoulli de parâmetro p”. Esta variável modela o resultado de lançamento de moedas ou qualquer evento binário aleatório. Essa distribuição modela uma série de acontecimentos de Bernoulli. Imagine que você lance uma moeda 10 vezes e deseja saber quantas vezes o resultado é cara. Aqui, lidamos com 10 eventos distribuídos conforme uma variável de Bernoulli. Até aqui só temos modelado eventos binários, ou seja, os quais admitem chance de acontecer ou de não acontecer. Mas se quiséssemos modelar a distribuição de resultados em uma série de lançamento de um dado por exemplo.Nesse caso estamos tratando de 6 resultados possíveis, cada um com uma probabilidade associada. Quando essas características estão presentes, podemos modelar o problema por meio de uma distribuição multinomial. Quando lidamos com variáveis aleatórias contínuas, sua distribuição pode ser descrita por uma função, a qual chamamos de função densidade de probabilidade. Distribuição Uniforme Como o próprio nome indica, é uma distribuição com probabilidade constante. Ela modela os eventos que tem a característica de ser igualmente prováveis. Distribuição Normal, ou Gaussiana Suponha que você esteja esperando um ônibus em um terminal e você queira saber quanto tempo será necessário esperar para o ônibus chegar. Sabemos que em São Paulo, por exemplo, os ônibus saem de acordo com um intervalo de tempo fixo, mas o tempo de chegada até você é variável, pois podem ocorrer eventos que influenciem esse fator ao longo do caminho. O tempo de chegada do ônibus é uma VA contínua, isso porque os valores possíveis são resultados numéricos de um fenômeno aleatório. Este exemplo pode ser modelado por uma distribuição Normal, ou Gaussiana. A função densidade de probabilidade de uma Normal tem a característica de ser simétrica em relação a média, e indica que os dados próximos a média ocorrem com mais frequência que os mais longes. Ou seja, existe um tempo médio de chegada do ônibus, que é o valor mais provável, a medida que se distância do valor esperado a probabilidade de acontecimento é diminui. A imagem abaixo é a função densidade de probabilidade de uma Normal, é fácil percebê-la pelo seu formato de sino. Esta assume dois parâmetros, μ e σ², que representam respectivamente a média e a variância. Veja o que acontece quando seus parâmetros são alterados. Note que ao mudarmos o valor da média, a função é deslocada para a esquerda, isso se deve ao fato de ser centrada na média. Quanto à variância é interessante notar que para baixos valores “espremem” a curva, isso porque os valores de x estão concentrados próximos a média. Comumente podemos encontrar problemas que envolvem mais de uma variável aleatória, contínuas ou discretas. Quando elas estão relacionadas, ou seja, as variáveis não são independentes podemos modelá-las por uma distribuição conjunta. Quando as variáveis são dependentes, podemos definir alguns outros termos, tais como, Esse teorema diz que a soma de variáveis aleatórias converge para uma distribuição normal — O quê?? Isso foi muito rápido eu sei, ainda não tratamos de soma de variáveis, mas é importante elucidar esse tópico, pois é um problema que se encontra frequentemente. Note que pode ser a soma de diversas variáveis aleatórias, independente de sua distribuição. Suponha que você saia de casa para ir a faculdade. Existe uma VA que define a existência de trânsito ou não, mas também existe uma que mede o tempo de vida do pneu do seu carro, assim como uma para a existência de vaga no estacionamento. Essas e muitas outras VAs definem o seu tempo de chegada a faculdade, que pode ser modelada como uma Normal. Agora imagine que escolhamos uma pessoa aleatoriamente no mundo. Todos tem a mesma chance de ser selecionados. Consideremos A o evento de a pessoa ser um estudante da USP e B o de uma pessoa que mora em São Paulo. Qual a probabilidade desses eventos? Isso já sabemos calcular. P(A) = Número de Estudantes da USP / Número de Pessoas no Mundo P(B) = Número de Moradores de São Paulo / Número de Pessoas no Mundo Mas e se desejássemos saber a probabilidade de escolher uma pessoa que é aluno da USP, sabendo que ele é um morador de São Paulo? Ou em outra forma: qual a probabilidade de escolher um aluno USP, dado que a pessoa é residente de São Paulo? É necessário se atentar as duas palavras em destaque: sabendo e dado, ambas nos dizem que já possuímos informação sobre o problema a ser resolvido, ou seja, que já sabemos que algo aconteceu. A pergunta que estamos fazendo é chamada de probabilidade condicional, isto é, a probabilidade de um evento acontecer dado que outro evento aconteceu. Usamos uma notação especial para representá-la: P(A|B) — que denota a probabilidade de A acontecer, dado que B aconteceu. Esse termo é calculado por: Como sabemos que a pessoa escolhida aleatoriamente é de São Paulo, estão contidas no evento B, todas as pessoas que estão fora dessa região são irrelevantes para a resolução do problema. E intuitivamente a resposta deve incluir a parte dos moradores de São Paulo que são alunos da USP, e é exatamente o que a intersecção entre A e B fornece. Dessa forma, a razão desses termos fornece a probabilidade condicional. Esse teorema nos fornece uma relação entre as probabilidades condicionais. Utilizando a interpretação bayesiana, podemos considerar A uma hipótese e P(A) a nossa certeza acerca dessa hipótese. Se obtivermos uma nova evidência B, a nossa certeza acerca de A mudará, tornando-se assim P(A|B), conforme dado pelo teorema de Bayes. A teoria de probabilidade fundamenta diversos algoritmos de machine learning. Se enxergarmos as features de um dataset como variáveis aleatórias, podemos pensar nos algoritmos como modelos probabilísticos. Por exemplo, suponha que o nosso objetivo seja predizer a espécie Y de uma flor com base em suas características X (como no dataset Iris). Nesse caso, podemos simplesmente predizer P(Y/X), ou seja, a probabilidade condicional de cada espécie dadas as características da flor, e escolher a espécie mais provável. Tal modelo é chamado de discriminativo, dado que ele nos permite discriminar entre as espécies. Um exemplo é o modelo de regressão logística que vimos na semana passada. Outra possibilidade é predizer P(X/Y) e, usando o teorema de Bayes, converter esse valor para P(Y/X). Esse segundo modelo é chamado de generativo, pois modela o modo como os dados X são gerados. Também podemos utilizar probabilidade para estimar os parâmetros de um modelo. Ao invés de definir uma função de custo e minimizá-la, podemos nos perguntar: quais parâmetros poderiam ter gerado esses dados? Ou seja, queremos encontrar os parâmetros que maximizam a probabilidade condicional dos dados: L = P(dados/parâmetros). Esse termo L é usualmente chamado de verossimilhança. Essa nova visão nos ajuda a justificar as funções de custo que escolhemos em alguns dos algoritmos de IA. Por exemplo, a regressão linear com erro quadrático médio encontra os parâmetros que maximizam a verossimilhança. O mesmo ocorre com a regressão logística com entropia cruzada. Por último, temos uma aplicação um pouco diferente, na qual não temos um dataset. Nesse caso, temos uma situação análoga ao que ocorre em nossas vidas. Por exemplo, quando aprendemos a andar, não temos um dataset com exemplos de como devemos fazê-lo. Ao contrário, aprendemos por tentativa e erro como andar. Será que um robô consegue fazer o mesmo? Veremos daqui a algumas semanas que a resposta é sim. De forma simplificada, o robô precisa descobrir quais ações o levarão até seu destino em menor tempo e sem cair. A ideia mais simples seria simular todas as sequências de ações possíveis e selecionar a melhor. No entanto, isso pode ser computacionalmente caro demais. Para resolver esse problema, o robô pode simplesmente se perguntar: qual ação provavelmente será melhor? O que eu espero que essa ação cause no futuro? Essas duas perguntas já destacam a presença da teoria de probabilidade nessa área de estudo, denominada aprendizado por reforço. Apresentamos muitas informações no texto de hoje as quais são essenciais para compreender melhor o que fundamenta diversos modelos de Machine Learning. Nas próximas semanas continuam as Turing Talks com mais modelos de predição! Se estiver interessado em mais conteúdo de IA, siga nossa página do Medium e do Facebook, e acompanhe nossas postagens! Por hoje é só. Até uma próxima!"
https://medium.com/turing-talks/turing-talks-16-modelo-de-predi%C3%A7%C3%A3o-naive-bayes-6a3e744e7986?source=collection_home---------95----------------------------,Modelos de Predição | Naive Bayes,Um algoritmo de Machine Learning que se baseia em probabilidades.,Ana Laura Moraes Tamais,663,7,2019-08-25,"Durante o aprendizado de Machine Learning, diversos algoritmos são introduzidos e hoje veremos em mais detalhes o algoritmo de classificação de Naive Bayes. Para entender este artigo, é necessário que se tenha alguma noção de Probabilidade e Variáveis Aleatórias, além de alguns modelos de distribuição de probabilidade. Mas não se preocupe: se você não se lembra desses conceitos, é só dar uma olhada no nosso post da semana passada. Caso você se sinta confortável com seus conhecimentos de probabilidade, principalmente quanto ao teorema de Bayes, sinta-se a vontade para seguir neste post. O Algoritmo Naive Bayes funciona como classificador e baseia-se na probabilidade de cada evento ocorrer, desconsiderando a correlação entre features. Por ter uma parte matemática relativamente simples, possui um bom desempenho e precisa de poucas observações para ter uma boa acurácia. Uma aplicação bastante comum é para identificar se um determinado e-mail é um spam ou não. Digamos que estamos lidando com uma base de dados com apenas 1 feature composta por 0 ou 1 e a label pode ser 0 ou 1 também. A tabela de probabilidade é montada da seguinte maneira: Para classificar, num caso geral com n features e m classificações, o algoritmo pegará os valores (x1, x2, …, xn) das features e calculará P(Y=y1 | X=x1, x2, x3, …, xn), P(Y=y2 | X=x1, x2, x3, …, xn), …, P(Y=ym | X=x1, x2, x3, …, xn). Ele escolherá o yk, 0 < k < m + 1, cuja probabilidade for maior. Como esse processo funciona será explicado a seguir. Vemos aqui como o algoritmo funciona para classificar a label (ŷ) de um novo evento (X = x1, x2, …, xn). Partiremos do Teorema de Bayes expandido para o caso de n features Sabendo que esse algoritmo irá considerar as features independentes duas a duas, pode-se concluir que a probabilidade de xi dado (y, x1, x2, …, xn) é a mesma probabilidade de xi dado y, pois não faz diferença o valor das outras features para a probabilidade da feature em questão: Assim, podemos dizer que P(x1, …, xn | y) = P( x1 | y ) * P(x2 | y) * … * P(xn | y): Como P(x1, …, xn) é constante, dado nosso input, podemos considerar que E, assim, nosso classificador escolherá o y cuja probabilidade (em outras palavras, o valor de P(y | x1, …, xn)) é maior: A acurácia mede a eficácia do seu modelo, ou seja, o quão “bom” é o algoritmo ou modelo treinado para representar um outro conjunto de dados qualquer. Se você deseja saber mais detalhes sobre esse e outros métodos de avaliação de algoritmos, leia o Turing Talks #10 ;) Considerando um caso simples em que temos apenas 2 features com 2 valores possíveis {0, 1} cada e nossa label pode ser classificada como {0, 1}, precisamos calcular o seguinte para fazer a tabela de probabilidade: P(X1 = 0, X2 = 0 | Y = 0) = ?; P(X1 = 0, X2 = 1 | Y = 0) = ? P(X1 = 1, X2 = 0 | Y = 0) = ?; P(X1 = 1, X2 = 1 | Y = 0) = ? P(X1 = 0, X2 = 0 | Y = 1) = ?; P(X1 = 0, X2 = 1| Y = 1) = ? P(X1 = 1, X2 = 0 | Y = 1) = ?; P(X1 = 1, X2 = 1 | Y = 1) = ? Cada uma dessas probabilidades será colocada na tabela de probabilidades. Na prática, devido ao fato de lidarmos com as variáveis X de forma que são independentes duas a duas, temos, por exemplo, que P(X1 = 0, X2 = 0 | Y = 0) = P(X1 = 0 | Y = 0) * P(X2 = 0 | Y = 0) E é feito o mesmo procedimento de forma análoga às demais probabilidades enunciadas acima. Isso diminui o número de probabilidades que precisamos calcular, principalmente quando o número de features e valores possíveis aumenta. Para a implementação desse modelo, temos 3 tipos de algoritmos Naive Bayes mais usados, os quais daremos mais detalhes. Cada algoritmo funcionará para calcular o P(xi | y) de modo que as features de cada observação estejam de acordo com um tipo de distribuição de probabilidade especificado pelo algoritmo escolhido. Gaussian Naive Bayes Nesse primeiro algoritmo, P(xi | y) é dado por: A média (μy) é o valor médio de xi, considerando as observações de classe y. E o desvio padrão (σy) é o desvio padrão da feature xi, considerando as observações da classe y. Multinomial Naive Bayes Esse algoritmo usa os dados em uma distribuição multinomial, que é uma generalização da distribuição binomial. Essa distribuição é parametrizada por vetores θyi=(θy1,…,θyn), θyi é a probabilidade do evento i ocorrer, dado que a classe é y. Podemos dizer que cada vetor θy representa uma observação e n é o número de features. Assim, θyi = P(xi | y) e é estimado pela seguinte fórmula: Nyi é o número de vezes que a feature xi aparece no conjunto de treinamento, Ny é o número de observações com classe y, n é o número de features e alfa é uma constante que contabiliza os recursos que não estão presentes nas amostras de aprendizado e impede que haja probabilidade igual a 0. Se alfa = 1, ele é chamado de Laplace smoothing e, se alfa > 1, é chamado Lidstone smoothing. Se alfa = 0, não há correção. Para podermos visualizar mais facilmente a necessidade, em alguns casos, da correção, digamos que temos a seguinte tabela de probabilidade: Se tentarmos classificar o caso em que X = (income = alta, age = >30 e <60, loan = medio), podemos perceber que P(Y = 0 | X = (income = alta, age = >30 e <60, loan = medio) = 0 P(Y = 1 | X = (income = alta, age = >30 e <60, loan = medio) = 0 Assim, é necessário fazer um additive smoothing (que pode ser Laplace smoothing ou Lidstone smoothing), que é uma correção que consiste em adicionar dados à base de treinamento para que não haja probabilidades iguais a 0. Tal processo é parametrizado pelo alfa. Bernoulli Naive Bayes Esse algoritmo se baseia numa base de dados de acordo com a Distribuição Multivariada de Bernoulli, que é composta por diversas features, as quais são valores binários, ou seja, podem assumir um valor dentre dois valores possíveis. Se alguma feature não for composta por valores binários, BernoulliNB() irá transformá-la em uma feature composta por valores binários dependendo do parâmetro binarize. Nesse modelo, o P(xi | y) é dado por: P(i | y) é o parâmetro p da distribuição de Bernoulli. Assim, P(i | y) = p se y = True, e P(i | y) = 1-p se y = False. Repare que, ao contrário do Multinomial Naive Bayes, há uma penalidade no caso de não ocorrência da feature i. Como já dito, o algoritmo Naive Bayes funciona calculando a probabilidade de cada evento. Mostraremos como esse processo é realizado em termos práticos. Usaremos uma base de dados de apenas 10 observações para simplificar a explicação do modelo Digamos que a tabela de probabilidade dessa base de dados é a seguinte: Testaremos a tabela para um novo indivíduo que tem income baixa, age >18 e <30 e loan alto: P_parcial(Risco de crédito = 0) = (5 / 10) * (1/2) * (1/2) * 1 = 1/8 P_parcial(Risco de crédito = 1) = (5 / 10) * (4/8) * (4/8) * (1/8) = 1/32 P(Risco de crédito = 0) = (1/8) / ((1/8) + (1/32)) = 80% P(Risco de crédito = 1) = (1/32) / ((1/8) + (1/32)) = 20% Portanto, a label desse indivíduo será 0. Agora vamos aplicar esse modelo usando o scikit-learn, sem nos preocupar tanto com a sua implementação. Embora assuma independência entre as features e não proporcione estimativa de probabilidade muito boa, o classificador de Naive Bayes é um potente modelo de predição. Isso porque não necessita de muitos dados, e na prática, funciona mesmo em alguns casos onde não há independência entre as features. Por esses motivos, é frequentemente implementado em aplicações, desde analise de textos até mecanismos de recomendação. E, acabou mais um Turing Talks :C Mas não fiquem tristes, semana que vem voltaremos com mais tópicos nesse assunto amado por todos nós. Por isso, não deixe de seguir nossa página do Medium para não perder nada! Se não aguentar esperar pelo próximo domingo, acompanhe nossa página do Facebook e Instagram, e fique ligado no que postamos por lá também! Até mais! :D"
https://medium.com/turing-talks/turing-talks-17-modelos-de-predi%C3%A7%C3%A3o-decision-tree-610aa484cb05?source=collection_home---------94----------------------------,Modelos de Predição | Decision Tree,Quão bem você faz perguntas?,Guilherme Fernandes,804,8,2019-09-01,"Escrito por Guilherme Fernandes, Fernando Matsumoto e Bernardo Coutinho. Sim, parece rotineiro abstrair a natureza para elaborar algoritmos, já vimos em algoritmos genéticos, KNN e desta vez temos um modelo de predição baseado em árvores. Curioso, não acha? Se você já jogou ou conhece esse tipo de jogo, sabe o quão importante é saber fazer boas perguntas. Nesse jogo, jogam duas pessoas, cada uma escolhe uma face, e a cada rodada os jogadores tentam descobrir a face escolhida por meio de perguntas de sim ou não. Quem acertar primeiro, ganha. As árvores de decisão funcionam exatamente assim, a ideia é construir um procedimento de perguntas de sim ou não a partir de certas características/atributos (no caso as features), para obter o target, que pode ser categórico ou numérico. Podemos abstrair a estrutura de árvore por meio dos conceitos de ramos, nós, sub-árvores e folhas, como na imagem abaixo. Os círculos roxos são chamados de nós e as linhas pretas que os ligam, de ramos. Os nós que não tem descendentes (com borda laranja na imagem) são chamados de folhas e o primeiro nó (com borda verde) é chamado de raiz. Juntos, os nós e ramos formam uma árvore. Para transformar isso em uma árvore de decisão, imaginamos que a raiz representa o dataset inteiro e que cada ramo representa uma decisão. No exemplo dado na seção anterior, a raiz poderia representar todas as pessoas possíveis junto com a pergunta “Qual o gênero?”. A aresta saindo da raiz e indo para a esquerda poderia então representar a resposta “feminino” e o da direita, “masculino”, de modo que o nó da esquerda representaria o conjunto de mulheres e o da direita, o conjunto de homens. Os outros nós e ramos corresponderiam então a outras perguntas e as folhas dariam a classificação final (qual é a pessoa correta). O número total de perguntas feitas nesse processo é chamado de profundidade (ou altura) dessa árvore. No caso da figura acima, temos uma árvore de profundidade 3: precisamos de 3 ramos (3 perguntas) para ir da raiz até as folhas. Note que isso corresponde a altura geométrica dessa árvore. Ao longo desse processo, você pode ter percebido que uma árvore é formada de várias sub-árvores. Por exemplo, se olharmos apenas para o nó à direita da raiz e para os nós abaixo dele, temos uma sub-árvore, identificada pela circunferência tracejada. Do lado esquerdo, temos outro exemplo de sub-árvore. Vamos pensar novamente no jogo que foi descrito, em que desejamos descobrir uma face. Se quisermos encontrar a solução da maneira mais rápida, as melhores perguntas a se fazer são aquelas que melhor dividem o seu conjunto de possibilidades de resposta, de forma a otimizar a eliminação das faces possíveis. Isso corresponderia a tentar separar o seu conjunto exatamente no meio. Por exemplo, se metade dos indivíduos possuírem cabelo escuro, e a outra metade não, a melhor alternativa seria perguntar se a pessoa em questão tem cabelo escuro, para eliminar uma grande parte das respostas de uma só vez. Tendo feito essa divisão, procuramos a pergunta ótima para o conjunto gerado, isto é, repetimos o processo, até descobrirmos a face. Para um dataset, o processo é similar. As perguntas são feitas a respeito dos valores das features. E o processo é realizado para todo conjunto de dados gerado pelas divisões. Termina quando todos os caminhos fornecem classificação. Note que as árvores crescem em tamanho máximo nesse processo. Para melhorar a capacidade de generalização de dados não vistos aplicamos um processo de poda (que será visto melhor no final do post). O algoritmo pode ser representado pelo que segue: Perceba que o principal problema é saber quais perguntas fazer e quando fazê-las. Os algoritmos que veremos a seguir fornecem diferentes respostas para esse problema. Esse algoritmo é baseado na ideia de entropia. A entropia de um dataset é uma métrica da sua incerteza ou impureza, ou seja, representa a aleatoriedade em seus valores. O seu valor é zero quando não há aleatoriedade (todos os elementos do dataset tem a mesma classificação) e aumenta conforme o dataset fica mais impuro. Considere, por exemplo, o caso da figura abaixo, em que há duas classes de elementos: as estrelas e os pentágonos. O dataset (a) é “impuro”, pois tem vários elementos de formas distintas. Já o dataset (b) é bem mais puro, pois quase todos os seus elementos têm a mesma forma (só há um pentágono). O caso extremo seria um dataset que só tivesse estrelas (ou, alternativamente, que só tivesse pentágonos). Nesse caso, o dataset seria completamente puro e sua entropia seria zero. Em uma árvore de decisão, como vimos acima, um corte divide o dataset em dois subconjuntos. O algoritmo ID3 tenta fazer com que esses subconjuntos sejam tão puros quanto possível. Por exemplo, se tivermos duas classes, A e B, o corte ideal seria capaz de dividir o dataset em um subconjunto com elementos de classe A e outro com elementos de classe B. Um corte ruim dividiria o dataset em dois subconjuntos, cada um com vários elementos de classe A e B. Escolhendo apenas cortes do primeiro tipo, esperamos que as folhas da árvore representem subconjuntos completamente puros, permitindo que classifiquemos bem as observações. Essa ideia é capturada pelo conceito de ganho de informação, que mede o quanto de entropia é removido a partir de um corte, ou seja, o quanto a pureza do dataset aumenta. Matematicamente é a diferença entre o valor da impureza antes de separar os dados e a média ponderada da impureza dos subconjuntos. Para deixar isso um pouco mais claro, vamos analisar o exemplo abaixo, em que o objetivo é prever se o objeto é um pentágono ou uma estrela a partir de sua cor e transparência. Inicialmente, o dataset tem entropia alta. Para diminuir essa entropia, temos dois cortes possíveis (um para cada feature). Se dividirmos pela cor da figura (a), conseguimos dois subconjuntos com entropia baixa, pois cada um tem objetos de um único tipo. Logo, o ganho de informação é alto. Já separando por transparência (b), os subconjuntos ainda são “impuros”, ou seja, têm objetos de formas diferentes. Desse modo, o ganho de informação é baixo. Por esse motivo, o algoritmo escolheria o corte da esquerda. Esse algoritmo é similar ao ID3, porém utiliza probabilidade para medir a impureza de um dataset, ao invés da entropia. Imagine que você selecione duas observações aleatórias de um dataset. Qual é a probabilidade delas terem classes diferentes? Para que o nosso dataset seja puro, queremos que a probabilidade seja 0, ou seja, todas as observações têm a mesma classe. Por outro lado, conforme a impureza aumenta, esperamos que a probabilidade dos elementos terem a classes diferentes aumente. Para aproveitar essa ideia, o CART define a impureza de Gini como a probabilidade de duas observações aleatórias de um dataset terem classificações diferentes. Ele segue um algoritmo semelhante ao do ID3, mas utiliza a impureza de Gini ao invés da entropia para escolher os melhores cortes. Como o nome indica, esse algoritmo pode ser utilizado para predizer valores numéricos. Basicamente, nesse caso, a divisão consiste em encontrar grupos com resultados similares, e a média dos seus resultados é usada como predição para esse grupo. O algoritmo encontra a melhor divisão tentando minimizar a variância do grupo. Utilizaremos a maravilhosa biblioteca scikit learn de python para aplicar uma árvore de decisão. Se você ainda não a possui, veja o guia de instalação. Após instalar, basta inserir o comando abaixo. E pronto, agora podemos implementar o algoritmo ID3 por meio da classe DecisionTreeClassifier. Para isso, é necessário alterar o parâmetro criterion para entropy, que por default é gini. Vamos aplicar esse algoritmos no dataset Iris. Já o CART pode ser utilizado pela classe DecisionTreeRegressor. Vamos aplicar esse algoritmo no dataset Boston Housing. Esse é o grande problema em todos os algoritmos de aprendizado! No caso de árvores de decisão ele surge quando a árvore se torna hiper específica para o dataset, perdendo portanto o poder de generalização em outros dados. Ou seja, conforme vamos aumentando demais o número de nós na árvore, a acurácia cresce na base de treino, mas piora na base de teste. Para resolver esse problema recorremos as seguintes questões: Pruning consiste em podar uma árvore de decisão já treinada, em uma tentativa de diminuir o número de nós e, portanto, o overfitting. A forma mais simples de fazer isso é pelo método do erro reduzido. Esse método passa por cada nó da árvore e: Quando a árvore está sendo treinada, é possível que alguma folha tenha apenas uma observação associada, o que pode levar a overfitting (a classe daquela folha é determinada por apenas uma observação). No sklearn, podemos usar os hiperparâmetros min_samples_split e min_samples_leaf para controlar isso. O primeiro garante que um corte só seja feito em nós com no mínimo min_samples_split observações. No entanto, mesmo que esse número seja alto, é possível que alguma das folhas geradas por esse corte tenham um número muito baixo de observações.Para resolver isso, podemos usar min_samples_leaf, que determina o número mínimo de observações que cada folha deve ter (se um corte fosse criar uma folha com menos observações, ele não será feito). Além disso, podemos usar também max_depth para controlar a profundidade máxima da árvore. Esperamos que tenha gostado desse Turing Talks. Nas próximas semanas voltaremos com mais tópicos nesse assunto amado por todos nós. Por isso, não deixe de seguir nossa página do Medium para não perder nada. Também, acompanhe nossa página do Facebook e Instagram, e fique ligado no que postamos por lá! Abraços e até uma próxima!"
https://medium.com/turing-talks/turing-talks-18-modelos-de-predi%C3%A7%C3%A3o-random-forest-cfc91cd8e524?source=collection_home---------93----------------------------,Modelos de Predição | Random Forest,Quem disse que as árvores não entram em consenso?,Fernando Matsumoto,656,6,2019-09-15,"Escrito por Fernando Matsumoto e Guilherme Fernandes. Bem vindos a mais um post entusiastas de inteligência artificial. Até agora na série de modelos de predição do Turing Talks temos abordado algoritmos de aprendizado que atuam sozinhos, isto é, apenas um procedimento é aplicado sobre os dados para realizar predições. Mas será que é possível combinar modelos? Isso poderia trazer resultados melhores? Sim, é possível combinar modelos e essa técnica é chamada de ensemble learning (aprendizado conjunto). Em linhas gerais ela combina um número finito de modelos para obter uma melhor performance preditiva. Random Forest (Floresta Aleatória) é um método de aprendizado conjunto. A ideia, porém, é treinar várias árvores de decisão (descorrelacionadas), obtidas a partir de amostras do dataset, e fazer predições utilizando os resultados que mais aparecem em caso de um problema de classificação, ou a média dos valores obtidos em caso de regressão. À primeira vista, essa parece uma ideia bem simples e intuitiva: gerar vários modelos e combinar as suas predições. Mas pensemos no caso das árvores. Se criarmos várias árvores e treinarmos todas elas no mesmo dataset, suas predições serão idênticas. A alternativa mais imediata seria dividir o dataset em várias partes, uma para cada árvore. Dessa forma, cada árvore teria o seu próprio dataset e, portanto, geraria predições diferentes. O problema dessa abordagem é o que cada árvore teria muitos poucos dados de treino. A solução para esse problema vem do campo da estatística. Quando fazemos uma análise estatística, é muito comum termos acesso à apenas uma parte dos dados. Por exemplo, pesquisas de intenção de voto são feitas com uma pequena amostra da população, visto que seria quase impossível entrevistar todos os brasileiros. Nesses casos, é importante entender as propriedades estatísticas da amostra utilizada em relação à população. Por exemplo, será que as intenções da amostra entrevistada são boas aproximações das intenções da população como um todo? Assumindo que a amostra é representativa da população (ambas têm a aproximadamente a mesma distribuição), podemos “reconstruir” a população criando várias (infinitas) cópias da amostra. Como a população deve ser parecida com a amostra, esperamos que essa “reconstrução” se aproxime da população inteira, como representado abaixo: Em estatística, a partir dessa população reconstruída, podemos criar novas amostras e, dessa forma, estudar as propriedades das amostras em relação à população. Esse processo é chamado de bootstrapping, e as novas amostras são chamadas de bootstrap samples (amostras de bootstrap). Observe que o processo de criar novas amostras a partir da população reconstruída é equivalente a pegar elementos aleatórios da amostra inicial com reposição. Ou seja, escolhemos um elemento da amostra inicial, anotamos ele e colocamos ele de volta na amostra inicial antes de escolher o próximo elemento. Lembre-se de que o nosso problema era como criar datasets diferentes para cada árvore. Utilizando bootstrapping, podemos fazer exatamente isso: primeiro, criamos uma bootstrap sample para cada modelo; depois agregamos (combinamos) esses modelos. Por isso, essa técnica é chamada de bootstrap aggregating ou bagging. Computacionalmente, para treinar M árvores em um dataset de tamanho N, faríamos: Depois de treinar todas as árvores, a predição é feita tomando a média dos resultados obtidos por todas as árvores, no caso de uma regressão, ou utilizando o valor que aparece mais (votação), em caso de classificação. Obs: o procedimento de bagging descrito acima é válido para qualquer modelo, e não exclusivamente para árvores. Essencialmente, ao se aplicar bagging com modelos de árvores de decisão se obtém por sua vez um modelo de aprendizado conjunto. Porém, é necessário mais um fator para definir uma floresta aleatória. Além do fato das florestas aleatórias serem treinadas em diferentes conjuntos de dados, também, é feita uma seleção aleatória das features que vão estar contidas nesse conjunto de dados; isso é chamado de feature randomness. Ou seja, a árvore é “duplamente aleatória”, graças ao bagging e a aleatoriedade das features. Mas por que isso é necessário? O ponto principal é garantir a baixa correlação entre as árvores. Isso significa que queremos que qualquer duas árvores de decisão escolhidas ao acaso não consigam se descrever por meio de uma relação linear. Quando utilizamos bootstrapping já conseguimos diminuir esse fator, isso devido a alta sensibilidade das árvores de decisão, isto é, pequenas alterações no dataset causam mudanças significativas no modelo gerado. E ao fazer seleção de features aleatoriamente corroboramos para as árvores serem mais diversificadas. Mas você deve estar se perguntando, por que a baixa correlação é importante? Suponha que você acorde de manhã e queira decidir se vai ou não levar seu guarda chuva quando sair. Você decide olhar em alguns jornais e considera que a combinação dos resultados vai garantir uma maior acurácia da previsão do tempo do que somente de um deles. O resultado não seria desfavorecido se ao invés de fornecerem predições diferentes (possivelmente com modelos diferentes) ambos usassem uma cópia de uma terceira fonte? Quando temos modelos descorrelacionados é mais provável que eles não vão cometer os mesmos erros. A diversificação garante que os erros sejam “cancelados” ou “sobrepostos” por acertos, fornecendo um modelo mais robusto em termos de performance. Para acompanhar essa parte, será necessário relembrar os conceitos de variância e viés (bias-variance tradeoff) que vimos no Turing Talks #10. De forma simplificada, o viés é o erro devido à falta de complexidade do modelo (correspondente ao underfitting) e a variância é o erro devido ao excesso de complexidade do modelo (correspondente ao overfitting). Há também um erro intrínseco, ou seja, que vem dos dados. Como não conseguimos alterar o erro intrínseco, vamos tratar apenas dos outros dois erros. A principal propriedade dessas duas medidas é que conforme uma aumenta a outra diminui. Por quê? Porque o viés é mais alto para modelos menos complexos, e é justamente nesses casos em que a variância é menor. Controlar o erro de um modelo é, portanto, uma questão de balancear esses dois valores. O cancelamento de erros causado pelo bagging e pela aleatoriedade de features corresponde à redução da variância do modelo e, portanto, do overfitting. Dessa forma, começar com um modelo de alta variância não seria muito problemático. De fato, visto que o viés tende a diminuir conforme a variância aumenta, deve fazer sentido escolher um modelo de alta variância e viés baixo, e utilizar bagging para reduzir essa variância. O modelo resultante teria então viés e variância relativamente reduzidos. Por outro lado, se escolhermos um modelo com variância alta demais, o bagging pode ser incapaz de reduzir suficientemente a variância. Por isso, ainda que os modelos base devam ter variância intermediária-alta, pode ser benéfico limitá-la pelo menos parcialmente. No caso de random forests, isso corresponde a utilizar árvores com alturas intermediárias, ou utilizar outros hiperparâmetros para controlar as árvores. Random forest é um modelo versátil, assim como as árvores de decisão, porque o modelo serve tanto para classificação como regressão. Possui viés e variância reduzida o que implica em melhores resultados, consistentemente, além de garantir robustez ao modelo. Por fim, são intuitivos, rápidos para se treinar (quando comparados a outros modelos de mesmo porte como redes neurais, por exemplo) e não precisam de muita alteração para se obter um primeiro modelo razoável. A relação tempo de treinamento e performance desse algoritmo faz juz a sua popularidade. Já foi implementado em diversas linguagens de programação, então agora é com você! Chegamos ao final de mais um Turing Talks. Esperamos que tenham curtido a leitura assim como adoramos escrevê-lo. Lembrando que pra ficar antenado com assuntos de IA no geral basta nos seguir nas redes: Facebook, Instagram e LinkedIn . Assim como não pode deixar de seguir nossa página do Medium. Por hoje é só!"
https://medium.com/turing-talks/turing-talks-19-modelos-de-predi%C3%A7%C3%A3o-redes-neurais-1f165583a927?source=collection_home---------92----------------------------,Redes Neurais | Teoria,A metalinguagem de um post sobre a abstração de como raciocinamos (ou a porta de entrada para o modelo de aprendizado de máquina mais em alta nos dias de hoje).,Caio Deberaldini,867,9,2019-09-22,"Escrito por Caio Deberaldini, Camila Lobianco e Paulo Sestini. O ser humano possui uma elevada capacidade de apreensão de padrões. Isso é o que forma nossa capacidade de dedução. Se nós estivermos em um supermercado e vermos na sessão das frutas um objeto pequeno, vermelho e com pequenos pontinhos pretos, saberemos que é um morango porque, desde que existimos, vimos morangos o suficiente para saber que essas características são o padrão que define esse objeto. Isso não é muito diferente do que um corretor de imóveis faz. Depois de alguns anos vendo um conjunto de casas e descobrindo que uma casa com característica X e Y vale um valor Z, ele é capaz de deduzir o valor de um imóvel. Não obstante, esse é um processo demorado, o qual demanda anos para alguém que trabalha nesse ramo adquirir esta “habilidade” . Com a ascensão dos computadores, pela primeira vez foi possível a ideia de automatizar o reconhecimento de padrões. Isso em função de sua capacidade de fazer contas com uma alta velocidade. Então os cientistas começaram a pensar em uma forma de traduzir esses padrões em cálculos e permitir que o computador fizesse o trabalho bruto por eles. É nesse contexto que surgem as redes neurais. Mas enfim, o que são redes neurais? Em síntese, elas são algoritmos de reconhecimento. Ou seja, sua função principal é a de receber um conjunto de dados, achar padrões dentre eles e definir o que aquele padrão significa. Ou seja, vamos supor uma rede neural que consiga ler letra cursiva e devolva o que está escrito para o usuário. O cientista de dados, nesse caso, pegaria cada pixel do input e o transformaria em uma matriz de valores. A rede neural descobriria padrões dentro dessa matriz que diriam se ali está escrito um “A” ou um “B”. Da mesma forma no exemplo do corretor de imóveis: ela veria que se a casa tem 2 quartos ela tem um valor e se tem 1, tem outro valor diferente. As redes neurais possuem uma estrutura complexa e dinâmica, o que as permite modelar uma ampla gama de problemas e se ajustar a situações. Quando precisamos resolver algum problema, se olharmos ao nosso redor podemos ver que, possivelmente, a natureza já resolveu problema similar por conta própria, através da evolução. Desse modo, podemos tentar copiar o que já surgiu de forma natural. Assim, quando se trata de encontrar uma solução que seja capaz de reconhecer padrões, nós olhamos para a criação da natureza especializada nisto: o cérebro. A unidade básica de uma rede neural é análoga às do cérebro: os neurônios. Um neurônio é uma célula que conduz sinal elétrico em apenas um sentido. Ele é composto pelo seu corpo celular, onde está o núcleo, pelo axônio, que conduz o sinal, e os dendritos, que conectam o neurônio a outros neurônios. Para o neurônio conduzir um sinal, esse deve receber um sinal de entrada com um valor mínimo de intensidade. Recebido o sinal, há a propagação de corrente elétrica através do axônio, chegando no corpo celular e passando para outros neurônios. O neurônio de uma rede neural será criado de maneira análoga. Esse receberá sinais, que passarão por uma função de ativação e darão origem ao sinal de saída do neurônio. Porém, permitiremos que cada sinal tenha uma importância diferente para o neurônio, um peso, que dirá o quanto cada sinal contribui para o sinal final. Com o neurônio criado, criamos a rede conectando vários neurônios. Dessa forma, passamos para a rede um conjunto de sinais inicial e utilizamos as saídas de alguns neurônios como entradas para outros. Após essa introdução sobre como as redes neurais foram idealizadas — com base em modelos biológicos, assim como muitos algoritmos de Machine Learning — , passaremos para a parte mais interessante das redes neurais: a matemática por trás do modelo. O modelo de redes neurais funciona por meio de camadas que, ao se comunicarem, produzem equações cada vez mais complexas — proporcionalmente à profundidade da rede - com o intuito de predizer melhor. Tomando como paralelo o uso da Regressão Linear para o caso da predição de valores contínuos para um determinado label, vimos que conseguíamos, para determinados datasets, obter uma acurácia relativamente elevada, ajustando os exemplos por uma (simples) reta. Extrapolando o raciocínio, se tivermos funções de ordens maiores e mais complexas, podemos obter um ajuste ainda melhor, ou a delimitação de uma fronteira mais bem definida para a classificação dos nossos objetos em análise (queremos usar nossa rede neural para classificarmos, não é mesmo?). Bom, até aqui, nada muito difícil de assimilar, porém, conhecendo a cara de uma rede neural entenderemos melhor esse processo, assim como os termos que a definem — representada pela figura abaixo: Em uma rede neural, temos três categorias de layers (camadas) por onde os dados fluem: Assim como o nome sugere, a combinação linear dos valores da camada anterior, passada como entrada para cada um dos neurônios da camada seguinte, é uma função linear em que seus parâmetros serão os valores da camada anterior e a combinação deles será feita pela matriz de pesos entre as camadas. Como ilustrado na figura acima, cada neurônio da Hidden Layer — representado por aᵢ⁽ʲ⁾— recebe o retorno da função de ativação — representada pela função g — , aplicada à uma função linear, a qual, por sua vez, é a combinação linear dos neurônios da camada anterior. Assim, se nossa Hidden Layer tiver mais camadas (ao invés de apenas uma), aplicamos a mesma ideia, usando-se os valores das camadas anteriores, obtidos por esse processo, para calcular os valores das próximas camadas. Dada uma camada j+1, aplicamos esse processo com os valores dos neurônios da camada j (simples, não?). Entretanto, em uma função linear temos o termo chamado de coeficiente linear (termo “solto” na função, que não depende dos parâmetros de entrada). No nosso caso, estes coeficientes seriam todos os valores da primeira coluna da matriz de pesos — todo θ(i, j, 0), onde j refere-se a camada anterior a camada que estamos analisando, i refere-se ao neurônio que estamos calculando o valor, na camada em análise e, por fim, 0 refere-se ao neurônio de número 0 da camada anterior. Mas, na prática, nos só temos os parâmetros de entrada, representados pelo vetor {x(1), …, x(n)}, onde n é o nosso número de features. Assim, para que os nossos dados estejam de acordo com o nosso modelo, para cada camada subsequente que desejamos calcular o valor de seus neurônios, devemos introduzir uma unidade de viés (bias unit) na camada anterior, a fim de que a combinação linear esteja correta. Como pode ser visto na ilustração da arquitetura de uma rede neural (um pouquinho acima neste post), tanto na camada de entrada (Input), quanto nas camadas escondidas (Hidden), nós adicionamos uma unidade de viés. Assim, tomando essa arquitetura como exemplo (camada escondida única), para o cálculo do valor dos neurônios da camada escondida, devemos adicionar um valor x(0) = 1 na camada de entrada. Dessa forma, ao multiplicar o peso correspondente pela unidade de viés, ficaremos com o peso “isolado” (nosso coeficiente linear da função linear) na combinação linear. Generalizando: para toda camada j+1, adicionamos uma unidade de viés à camada j, ou seja, inserimos a(0, j) = 1. Para finalizar o aprendizado sobre o fluxo dos dados e como eles são manipulados na nossa rede neural, precisamos retomar uma ideia passada no início desta seção sobre a matemática por trás dessa maravilha de modelo: criar funções mais complexas, conforme vamos nos aprofundando na nossa rede, com o objetivo de obtermos um modelo mais preciso e que consiga classificar melhor o nosso target. Existem diversas funções de ativação (Sigmóide, ReLU, Softmax, Tangente Hiperbólica, entre outras). Falaremos e utilizaremos, nos posts sobre redes neurais, a função sigmóide. Função Sigmóide A motivação para utilizarmos esta função de ativação é conseguir manter o valor da combinação linear entre 0 e 1. Como queremos classificar um alvo (ou múltiplos alvos), devemos ter saídas binárias — tumor benigno × tumor maligno, gato × não_gato, etc. A seguir, são apresentadas a cara da função sigmóide e como ela se comporta: Perfeito, além de tornarmos as nossas equações mais complexas, tendo em vista a não-linearidade da função sigmóide e por aplicarmos ela em cada neurônio, conseguimos manter os nossos valores entre 0 e 1. Mas, afinal de contas, porque precisamos disso? Lembrando-se de posts passados sobre modelos de predição, em especial aqueles voltados à classificação, sabemos que, no final de todo processo, obteremos a probabilidade condicional de o objeto em análise pertencer a determinada classe, dado que entramos com os valores correspondentes a suas features. Assim, escolhendo um valor arbitrário de probabilidade como sendo a fronteira entre o seu objeto não pertencer ou pertencer àquela classe, conseguimos classificá-lo! Por exemplo, vamos supor que, após treinarmos nossa rede neural, tentaremos classificar se determinada lata de cerveja pertence a marca A ou não. Podemos colocar um threshold em 0.5, de tal forma que, caso, ao final da rede neural, nossa probabilidade for 0.78, podemos afirmar que, como essa probabilidade é maior do que a nossa fronteira de decisão, então aquela lata pertence a marca A, ou seja, discretizamos nosso resultado — 0, não pertence; 1, pertence. Sabemos que este post, ainda que destrinchadas as partes necessárias, é bastante carregado e pouco prático. Porém, o conteúdo aqui apresentado não só introduz você, caro leitor, ao incrível mundo das redes neurais, como também te prepara para, no próximo post, entender como funcionam os algoritmos de treinamento de uma rede neural e como implementá-los. Esperamos que tenham gostado e nos vemos no próximo post! Curtiu? Então não deixe de criar sua conta no Medium e seguir a gente para continuar aprendendo mais modelos e outros assuntos de IA. Dê uma olhada em nossos outros posts e acompanhe o Grupo Turing no Facebook e no LinkedIn, temos várias oportunidades em IA por lá também."
https://medium.com/turing-talks/turing-talks-20-regress%C3%A3o-de-ridge-e-lasso-a0fc467b5629?source=collection_home---------91----------------------------,Modelos de Predição | Regressão de Ridge e Lasso,Um olhar para as parcelas L1 e L2 (Ridge e Lasso),Andre Devay,600,7,2019-09-29,"Esse texto é para você, iniciante no mundo de IA e que começou a ter contato com os primeiros tipos de métodos de Machine Learning. Embora o intuito dos Turing Talks seja deixar o texto o mais simples possível para o leitor, talvez você ainda não tenha muita familiaridade com regressão e outros conceitos básicos de ciência de dados, então, seguem algumas leituras adicionais a esse texto: E também, vale relembrar o modelo de regressão linear. Sem mais delongas, o texto. Um dos principais problemas a serem enfrentados na construção de modelos de predição é o de balancear a relação entre bias e variance. Esses conceitos já foram bem detalhados no Turing Talks #10, mas o retomaremos brevemente aqui somente de uma forma intuitiva. De uma maneira bem simplificada, bias e variance são métricas de erro do modelo e um bom balanceamento de seus valores corroboram para se obter alta acurácia preditiva. Para facilitar a compreensão desses conceitos vamos pensar na regressão linear. Sabemos que se trata de ajustar os dados a uma reta. E que obtemos essa reta minimizando o erro quadrático médio (MSE). Bias, quando em alta, indica que o modelo se ajusta pouco aos dados de treino, causando o que é chamado de underfitting. O que significa que o MSE é alto, para a base de teste. Variance, em alta, diz que o modelo se ajusta demais aos dados, causando por sua vez, overfitting. Nesse caso o MSE é zero para os dados de teste, mas podemos dizer que não generaliza bem os dados. Mas você pode estar se perguntando, como avalio esses indicadores na prática? Ambos tipos de erros podem ser avaliados, analisando-se os erros em dados de treino e dados de teste. Veja: Lembrando que sempre comparamos os erros dos modelos a um erro base. Por exemplo, se desejamos classificar imagens de cachorros em um dataset, o erro base é que o ser humano teria ao analisar essas imagens. Isso é definido porque não desejamos obter um erro muito maior do que o que um ser humano teria ao realizar essa classificação. Entendemos que bons modelos de predição precisam apresentar uma boa relação entre os erros bias e variance. Mas na prática se temos uma pequena quantidade de dados é mais complicado realizar uma boa generalização dos dados, e muito fácil causar overfitting. Temos algumas alternativas a esse problema, a mais rápida seria aumentar a quantidade de observações em dados de teste, podemos também treinar o modelo auxiliado por validação cruzada, isto é, utilizamos esse método para otimizar hiperparâmetros. Mas, além disso, é possível usar o que é chamado de regularização. E veremos que essa é essa a característica que os modelos de Ridge e Lasso trazem consigo. De uma maneira bem direta, podemos entender regularização como a inserção de bias em um modelo. Ou em outras palavras, essa técnica desencoraja o ajuste excessivo dos dados, afim de diminuir a sua variância. Dentro da regressão linear, Ridge e Lasso são formas de regularizarmos a nossa função através de penalidades. De forma simples, dentro de uma equação estatística dos dados, nós alteramos os fatores de forma a priorizar ou não certas parcelas da equação e, assim, evitamos ‘overfitting’ e melhoramos a qualidade de predição. Como já sabemos uma regressão linear tenta ajustar uma função linear aos dados: O procedimento de ajuste envolve a função de custo como soma residual dos quadrados ou RSS. Os coeficientes w são escolhidos para minimizar essa função de custo com base nos dados de treinamento: No entanto, pode ocorrer overfitting, ou seja, o modelo pode “memorizar” o ruído dos dados de treinamento. Nesse caso, dizemos que o modelo tem um erro de generalização (erro na base de teste) elevado. Esse fenômeno está associado à variância do modelo, como vimos acima. Portanto, uma forma de diminuir o erro é aumentar o bias. Para isso, regularizamos os coeficientes w, ou seja, restringimos o seu tamanho. Isso é feito adicionando um termo na função de custo, de forma que minimizar a função de custo automaticamente minimize também os coeficientes. Além de diminuir a variância do modelo, essa regularização tem uma outra importante aplicação em machine learning. Quando há múltiplas features altamente correlacionadas (ou seja, features que se comportam da mesma maneira) a regularização Lasso seleciona apenas uma dessas features e zera os coeficientes das outras, de forma a minimizar a penalização L1. Desse modo, dizemos que esse modelo realiza feature selection automaticamente, gerando vários coeficientes com peso zero, ou seja, que são ignorados pelo modelo. Isso facilita a interpretação do modelo, o que é uma enorme vantagem. Nesse caso, a penalização consiste nos quadrados dos coeficientes, ao invés de seus módulos. Qual será o efeito dessa regularização nos coeficientes de duas features altamente correlacionadas? Poderíamos ter duas features com coeficientes parecidos, ou uma com coeficiente alto, e outra com coeficiente zero. Como a penalização L2 é desproporcionalmente maior para coeficientes maiores, a regularização Ridge faz com que features correlacionadas tenham coeficientes parecidos. No entanto, essa regularização não diminui a susceptibilidade do modelo a outliers, de forma que é recomendável limpar o dataset e remover features desnecessárias antes de realizar esse tipo de regressão. Em termos matemáticos, a penalidade L1 não é diferenciável, o que pode complicar a sua implementação. Já a L2 é diferenciável, o que significa que ela pode ser usada em abordagens baseadas em gradiente. Sim, se trata exatamente de combinar os termos de regularização de L1 e L2. Assim, obtemos o melhor dos dois mundos, porém temos que enfrentar o problema de determinar dois hiperparâmetros para obter soluções ótimas. Agora que já vimos os 3 tipos de regularização utilizados com regressão linear (Lasso, Ridge e Elastic Net), vamos analisar os seus efeitos a partir dos gráficos abaixo. Os gráficos mostram os resultados da regressão linear em 3 datasets com relações x-y variadas. No gráfico da esquerda, ainda que os pontos sejam aleatórios, observa-se uma tendência fraca nos dados (ruído). Essa tendência é capturada pela regressão linear e Ridge, mas não pela regressão Lasso ou Elastic Net. Para entender esse comportamento, observe que, nesse caso, os coeficientes da regressão linear são pequenos, de forma que a penalização Ridge, que conta com os coeficientes ao quadrado, é muito pequena. Já a penalização Lasso é alta o suficiente para levar os coeficientes a zero. No gráfico do meio, a tendência de crescimento é maior, tal que a regressão Lasso obtém um coeficiente não nulo, ainda que pequeno. Os coeficientes da regressão Ridge são maiores e mais próximos do correto. No gráfico da direta, a situação se inverte. As regressões Lasso e Elastic Net chegam muito próximo dos dados, mas a Ridge fica mais longe dos dados, porque a penalidade L2 é mais influenciada por coeficientes grandes. Baseado nessas observações, podemos observar que a regressão com regularização Lasso tende a ignorar relações fracas, enquanto a regressão com regularização Ridge considera também relações fracas, mas não lida tão bem com relações fortes. A regressão Elastic Net é um intermediário entre Ridge e Lasso. Fazemos isso com o auxílio da biblioteca de python Sci-kit Learn, basta adicionar as seguintes linhas de comando: Documentação dos modelos: Ridge, Lasso e ElasticNet. Obs: Em ElasticNet o Sci-kit Learn utiliza uma forma alternativa para os parâmetros α1 e α2, vide documentação. Mais um Turing Talks se foi, mas dele ficam muitos aprendizados sobre os modelos de Ridge, Lasso e ElasticNet, e como uma ideia sutil pode trazer um diferencial para as nossas predições. E aí? Curtiu o Turing Talks de hoje? Quer saber mais e mais sobre tudo de inteligência artificial? O jeito que eu acho mais legal é seguir o Grupo Turing nas redes: facebook, instagram, linkedin e medium. Porque assim você vai estar sempre ligado nas melhores oportunidades de aprender IA. Enfim, por hoje é só, esperamos que tenham aproveitado a leitura. Até uma próxima!"
https://medium.com/turing-talks/turing-talks-21-modelos-de-predi%C3%A7%C3%A3o-redes-neurais-parte-2-b0c2c33ee339?source=collection_home---------90----------------------------,Redes Neurais | Teoria #2,Um mergulho mais profundo no aprendizado das Redes Neurais.,Caio Deberaldini,686,6,2019-10-13,"Fala pessoal, ficaram ansiosos para lerem e aprenderem mais ainda sobre as nossas queridas Redes Neurais? Bom, seguindo o último post a respeito do assunto — se você não viu, não perca aqui — , daremos continuidade nos nossos estudos a respeito do modelo que está na crista da onda. Assim como os demais modelos de Aprendizado Supervisionado os quais exploramos, ao final de todo o processo o que queremos é otimizar a nossa função de hipótese, de tal forma que o valor esperado pelo modelo e o valor real sejam o mais próximo possível, ou seja, o erro entre eles seja o mínimo possível. Desse modo, lembrando que, em nosso contexto, usaremos Redes Neurais para classificarmos se determinado exemplo pertence a uma classe em específico — todavia, podemos usá-las para uma regressão também — , temos a nossa função de custo definida da mesma forma que na Regressão Logística, com o porém de que, para a nossa análise ser generalizada, podemos trabalhar com K classes de análise. A equação abaixo é a nossa função de custo, a qual buscaremos minimizar para obtermos os parâmetros (representados por Θ) “ótimos” do nosso modelo — ou seja, aqueles que fazem a nossa predição tornar-se o mais próximo possível do valor real esperado: Ok, então queremos calcular a função de erro da nossa Rede Neural. Mas, para isso, precisamos calcular o valor esperado para a i-ésima entrada do nosso conjunto de dados. Ou seja, precisamos passar a nossa entrada pelas camadas, fazendo-se a ativação dos neurônios e obtendo funções cada vez mais complexas (e não-lineares) ao longo da rede — como explicado no nosso 1º post de Redes Neurais. O algoritmo utilizado neste caso é conhecido como Feedforward Algorithm. Vejamos a seguinte arquitetura, também mostrado no 1º post: Dada uma camada L, cada neurônio da camada L+1 é calculado como sendo a composição da função de ativação g sobre a combinação linear dos neurônios da camada anterior, ponderados com os parâmetros θ correspondentes. Por exemplo, para a₁⁽²⁾, tomamos a 1ª linha da matriz de parâmetros θ⁽¹⁾ e multiplicamos pelos correspondentes neurônios da camada anterior, ou seja, o valor da 1ª coluna, da 1ª linha de θ multiplica o valor da bias unit da camada anterior, o da 2ª coluna multiplica o próximo neurônio e assim sucessivamente. De tal forma que, para a arquitetura acima, temos o seguinte algoritmo: Onde a multiplicação ali representada é a multiplicação matricial entre a matriz θ⁽ⁱ⁾ e o vetor coluna dos neurônios a⁽ⁱ⁾. Além disso, sempre temos que nos lembrar de adicionar a unidade de viés (bias unit) na 1ª camada e nas camadas que compõem a Hidden Layer — dúvidas, leiam novamente o nosso primeiro post de Redes Neurais e não esqueçam de deixar seus claps rsrs. O algoritmo de Backprop serve para que consigamos calcular a derivada da nossa função de custo em cada uma das nossas camadas (com exceção da camada de entrada). Para cada uma das camadas escondidas e para a camada de saída, calculamos uma hipótese para cada neurônio, ativando-o com uma função determinada — no caso, explicamos sobre a função sigmóide — , compondo essa função com a combinação linear dos neurônios anteriores, parametrizados por θ. A imagem de ativação dos neurônios acima mostra como, matematicamente, é isso. Bom, então para cada vez que ativamos nossos neurônios, teremos um erro no valor esperado associado a ele. Podemos ter uma intuição melhor em nossa última camada, pois sabemos que o resultado em cada neurônio desta será a probabilidade para que, dada uma entrada, ela pertença à classe associada àquele neurônio. Portanto, como temos para cada exemplo do nosso conjunto de treino um vetor de labels associando-o a sua classe, para a Output Layer não temos grandes problemas para calcular o erro. Basta pegarmos o valor previsto e fazermos a subtração com o vetor de labels do exemplo. A equação seria então, para o i-ésimo exemplo do nosso conjunto de treino: Porém, nosso objetivo é obtermos os parâmetros que pesamos nossos neurônios de uma camada para as camadas consecutivas. Como não necessitamos nos aprofundar tanto na matemática do algoritmo, deixo para os amantes da matemática (como eu) e curiosos este artigo, super interessante, que demonstra como a partir do erro relativo a função de custo devido a ativação do neurônio e do erro relativo a ativação do neurônio devido aos parâmetros considerados, chega-se à derivada da função de custo devido aos parâmetros considerados para uma camada escondida. A ideia do algoritmo é brincar com derivadas parciais para, a partir dos erros das camadas subsequentes, obtermos a derivada da função de custo com respeito aos parâmetros das camadas anteriores. Por isso o nome Backpropagation! Com essa intuição bastante abstrata, têm-se as equações do algoritmo Backprop: Onde (*) é a multiplicação matricial, (.*) a multiplicação elemento a elemento e g’(z) é a derivada da função de ativação — no nosso caso, função sigmóide — para a camada L. Vale ressaltar que começamos o cálculo pela camada de saída e terminamos na camada que sucede a camada de entrada. Para a nossa função de ativação, sua derivada, a qual pode ser obtida pela regra da cadeia (os mais familiarizados com Cálculo Diferencial e Integral a conhecem muito bem), é dada pela seguinte equação: Depois de calcularmos o erro da função de custo com respeito à ativação dos neurônios em cada camada (como descrito pelas fórmulas acima), calculamos a derivada da função de custo com respeito aos pesos — representada por D⁽ᴸ⁾ — , para cada camada, como sendo a seguinte expressão: Onde m é o número de exemplos do nosso conjunto de treinamento. Por fim, precisamos atualizar nossos parâmetros a cada iteração do Backprop. Assim como apresentado no modelo de Regressão Linear, aplicamos o algoritmo do Gradiente Descendente para obtermos novos parâmetros, ponderando o seu gradiente pela taxa de aprendizado α: Em síntese, o fluxo dos dados em uma Rede Neural, como visto, segue o pipeline: aplica-se o Feedforward para calcularmos o valor dos neurônios e o valor esperado de nossos exemplos de treinamento; retornamos na rede, por meio do Backward Propagation, e calculamos a derivada da nossa função de custo com respeito a cada matriz de parâmetros θ; atualiza-se os valores dos parâmetros, por exemplo, pelo Gradiente Descendente; o processo é iterado repetidas vezes, até que o resultado de saída da rede seja satisfatório, de acordo com alguma métrica. Eu sei, o caminho até aqui foi árduo e, muito provavelmente, seja necessária mais de uma leitura a respeito deste e do post anterior dessas belezinhas conhecidas como Redes Neurais. Porém, não se preocupem. Todo esse esforço será recompensado em nossa 3ª parte, onde, finalmente, colocaremos as mãos na massa e iremos implementar nossa primeira Rede Neural, juntos, treinando e testando-a para que ela classifique números escritos a mão (sim, exatamente isso que vocês ouviram! — ou leram — ). Esperamos que tenham gostado e até a próxima! Curtiu? Então não deixe de criar sua conta no Medium e seguir a gente para continuar aprendendo mais modelos e outros assuntos de IA. Dê uma olhada em nossos outros posts e acompanhe o Grupo Turing no Facebook, Instagram e no LinkedIn, temos várias oportunidades em IA por lá também."
https://medium.com/turing-talks/turing-talks-22-modelos-de-predi%C3%A7%C3%A3o-redes-neurais-parte-3-9c5d5d0c60e7?source=collection_home---------89----------------------------,Redes Neurais | Teoria #3,Um verdadeiro hands on em redes neurais.,Eduardo Eiras de Carvalho,1385,8,2019-10-27,"Escrito por Caio Deberaldini, Eduardo Eiras, Guilherme Fernandes e Fernando Matsumoto. A imagem acima é fruto de um tipo de aplicação de redes neurais, que é a restauração de imagens. Hoje vamos entender um pouco de representação de imagens por computadores, assim como iremos utilizar tudo que aprendemos nas últimas semanas sobre rede neurais para fazer predições sobre características de imagens, animados? Caso não tenham lido os últimos dois textos ou não entendam absolutamente nada de redes neurais, recomendamos que, primeiramente, leiam eles: Assim como todas as coisas no nosso computador, imagens são representadas por números e, neste caso especificamente, uma série de números em matrizes. Quando abrimos uma imagem, estamos nada mais do que abrindo uma ou mais matrizes que contêm um valor para cada pixel do nosso computador. Para entender um pouco mais sobre o que está acontecendo, vamos entender o conceito de espaço de cores. O espaço de cores mais utilizado para imagens em computador é o RGB — Red, Green, Blue — ou seja, ele é um conjunto de três matrizes em que cada uma possui um valor correspondente à vermelho, verde ou azul. Este valor vai de 0 a 255 e representa a intensidade da cor em questão. Assim, cada pixel é um vetor de tamanho 3, por exemplo, um valor RGB (0, 0, 255) representa um azul intenso, (0, 0, 0) é preto e (255, 255, 255) é a superposição das três cores, ou seja, branco. O RGB pode ser visto como um sistema cartesiano de cores: Como podemos então representar imagens em preto e branco? Isso pode ser feito com uma matriz em que cada pixel tem o valor da média dos três valores do espaço RGB. Um segundo espaço de cores amplamente utilizado é o HSV (Hue, Saturation, Value), geralmente representado por um cilindro: O fato do espaço de cores HSV representar também brilho e saturação é muito útil na análise de imagens, uma vez que nem sempre as imagens que analisaremos estarão em perfeitas condições de brilho. Agora temos que colocar isso em prática! Há algumas bibliotecas de python que nos permitem visualizar imagens. Neste artigo, abordaremos matplotlib e opencv. Antes de começarmos com os comandos, é bom saber que uma desvantagem deste biblioteca é que ela apenas suporta imagens com a extensão .png Inicialmente iremos importar a biblioteca com o nome plt por comodidade), e o módulo matplotlib.image que nos permite carregar e visualizar imagens. Para carregar uma imagem dando seu nome como img, usamos o comando img = mpimg.imread('nome_do_arquivo.png'). Por curiosidade, caso execute o comando print(img), irá receber uma matriz! Para visualizarmos corretamente a imagem, usaremos o comando plt.imshow(img). Esta biblitoteca, ao contrário do matplotlib, segue a ordem BGR (Blue Green Red) de cores, ao invés de RGB. Assim, talvez você deva checar como sua imagem está representada antes de usar qualquer uma destas bibliotecas. Uma outra diferença é que opencv não possui a limitação de abrir apenas arquivos .png. No python, a versão que usaremos é o cv2. A função que lê imagens é cv2.imread('nome_do_arquivo') e para visualizar devemos executar cv2.imshow('nome_da_janela', img). Uma peculiaridade desta biblioteca é que devemos executar um comando específico para fechar a imagem. O comando waitKey(0) deixa a janela aberta até o usuário pressionar alguma tecla, em seguida a janela é fechada com destroyAllWindows. Agora que sabemos como as imagens são representadas por um computador, veremos como treinar uma rede neural em um dataset de imagens. Instituto Nacional de Tecnologia e Padrões Modificado ou MNIST é uma base de dados de dígitos escritos a mão. Este é equivalente ao “Hello World!” de Deep Learning. Consiste em 70,000 imagens, as quais estão divididas em 60,000 para treinamento e 10,000 para testes. Em geral, as redes neurais carecem de um número alto de observações para oferecerem uma boa generalização. Simon Haykin em “Redes Neurais Princípios e prática” afirma que o tamanho do conjunto de treinamento (N) precisa ser da ordem da razão entre o número de parâmetros livres da rede (W) e a fração de erros (ε) de classificação permitida sobre os dados de teste. Matematicamente: Lembrando que os parâmetros livres correspondem aos pesos sinápticos e níveis de bias. Cada imagem é composta, por 28x28 pixels, totalizando 784 pixels que serão inputs para a nossa rede neural. Sem mais delongas, vamos à prática. Com as seguintes linhas de código podemos importar o dataset MNIST do módulo Tensorflow de Python. Precisamos alterar o formato das imagens, que vêm do dataset como matrizes, em vetores. Fazemos isso empilhando as linhas da matriz e convertendo essa pilha em um vetor, por exemplo. Essa etapa é necessária já que os números que representam cada pixel correspondem às entradas da rede neural. Além disso, normalizamos os números da matriz, dividindo pelo seu valor RGB máximo (255), para que seus valores estejam entre 0 e 1. Isso é importante porque garante otimização de performance dos algoritmos. A implementação dessas funções se dá de maneira bem simples, seguindo as fórmulas que vimos nos últimos posts: A ideia aqui é converter as nossas saídas (labels) que correspondem aos dígitos de 0 a 9, em vetores com zeros e uns. Por exemplo, o número 0 corresponde ao vetor [1 0 0 0 0 0 0 0 0 0], o 1 ao [0 1 0 0 0 0 0 0 0 0] e assim sucessivamente. Até poderíamos fazer a rede neural fornecer uma saída entre 0 e 9. Um dos problemas com essa abordagem é que cada label se refere a certos conjuntos de pixels. Dessa forma, não se deve pensar nas labels como números, mas sim como classificações (da mesma forma que fazemos com cores, por exemplo). Lembrando que a saída é um número entre 0 e 1, podemos pensar que cada uma das saídas da rede indica a probabilidade de uma das classificações. A saída do primeiro neurônio, por exemplo, indica a probabilidade do dígito ser “0”. Dessa forma, a classificação gerada pela rede seria a mais provável, ou seja, aquela correspondente à maior saída da rede. Antes de começar a treinar a rede neural, precisamos inicializar valores para esses parâmetros, certo? Se inicializarmos esses valores com zero, as ativações da primeira camada serão todas zero, fazendo com que todos os gradientes da primeira camada fiquem iguais (pela fórmula que vimos no último post). Isso pode dificultar o aprendizado da rede. Uma solução prática é inicializar com valores distribuídos segundo uma Normal(0, 0.1). Existe uma alternativa melhor para inicialização que você pode encontrar aqui. O primeiro passo para calcular o gradiente da função de custo é implementar o feedforward. Para fazer isso, traduzimos as equações que vimos no último post para código: Para lidar com a bias unit, adicionamos uma coluna com 1s no começo das ativações (a1 e a2). Cada elemento da coluna corresponde à bias unit em uma das imagens. Além disso, a multiplicação de matrizes é representada em código pelo símbolo @. Em seguida, calculamos o custo, conforme dado no último post: No código, a somatória dupla é realizada por meio de uma matriz. Cada termo da matriz corresponde a uma combinação de i e k. Basta então somar todos os elementos da matriz e dividir por m. Por último, o backpropagation corresponde à tradução direta das equações que vimos no último post para python. No final, a função nnRegCostFunction retorna o custo e os gradientes do custo em função de theta1 e theta2. Para obter as classificações geradas pela rede, primeiramente realizamos o feedforward da mesma maneira que fizemos acima. A partir das ativações da última camada, podemos obter as classificações com a função np.argmax, que nos diz qual dos 10 neurônios de saída tem a maior ativação (para cada imagem). Em seguida, basta contar os acertos e dividir pelo total de imagens para obter a acurácia. Para treinar a rede neural, precisamos utilizar todas as funções que definimos acima. Primeiramente, criamos os parâmetros theta1 e theta2, considerando uma camada escondida com 800 neurônios. Em seguida, realizamos o gradiente descendente. Essa etapa consiste em, repetidamente, calcular os gradientes do custo e fazer a seguinte operação (D é o gradiente e α é a taxa de aprendizado): O loop para após um número máximo de iterações (max_iter) ou quando o erro fica baixo o suficiente, conforme determinado por epsilon. Até agora construímos uma rede neural usando apenas numpy, mas não verificamos se ela funciona nem como ela performa. É isso que fazemos no notebook abaixo. Primeiro, obtemos os parâmetros theta1 e theta2 da rede treinada com alpha=.3. Em seguida, verificamos a acurácia da rede e vemos as suas predições para algumas imagens da base de teste. Você conseguiu construir com sucesso uma rede neural à mão que classifica as imagens do dataset MNIST! O código completo pode ser encontrado aqui. Você deve estar se perguntando se não existe uma biblioteca de python que já tenha uma rede neural implementada, assim como temos feito com os outros modelos vistos até agora. E a resposta é sim, e esse módulo se chama TensorFlow. No entanto, optamos por fazê-lo a mão, a fim de retirar a sensação de “caixa preta” que geralmente as bibliotecas trazem consigo. Até porque redes neurais é um dos modelos mais importantes na realização de predições, e sobretudo abre uma porta porta para um novo patamar em Machine Learning, devido a sua vasta gama de aplicações. Gostaram do texto? Esperamos muito que sim. Estamos sempre buscando aproximar as pessoas desse mundo de inteligência artificial, da maneira mais agradável possível. Se quiserem conhecer mais do que fazemos, não deixem de nos seguir nas redes sociais: Facebook, Instagram, Linkedin e, obviamente, no Medium. Por hoje é só. Até logo!"
https://medium.com/turing-talks/turing-talks-23-modelos-de-predi%C3%A7%C3%A3o-redes-neurais-convolucionais-d364654a34de?source=collection_home---------88----------------------------,Redes Neurais | Redes Neurais Convolucionais,As redes neurais especializadas em imagens.,Rodrigo Estevam,807,10,2019-11-03,"Escrito por Rodrigo Estevam e Rodrigo Fill Rangel. Olá amável ser! Que tal relaxar um pouco, pegar um café e aprender um novo algoritmo muito útil para aplicações de IA em reconhecimento de imagens? Então se acomode e aproveite! No Turing Talks #19 em diante nós falamos bastante sobre Redes Neurais, sua construção e seu poder de reconhecimento de padrões. Neste artigo, expandiremos essa noção apresentando uma importantíssima arquitetura de Redes Neurais: as Redes Neurais Convolucionais. Você já abriu o Facebook em uma foto aleatória de um amigo ou conhecido que apareceu no seu feed e viu que o próprio Facebook já sabia quais pessoas estavam na foto, e de quebra ainda perguntava se você quer marcá-las; bom este é um bom exemplo de Redes Neurais Convolucionais aplicadas ao cotidiano. O Instagram também já aplica muitas redes neurais convolucionais. Na realidade, quase todas as empresas que têm acesso à uma grande quantidade de imagens têm interesse em saber como obter mais informações delas. O Facebook e o Instagram são apenas alguns dos exemplos, ainda existem outros como o Google, que também usa CNNs — vamos carinhosamente apelidar as redes neurais convolucionais de CNNs — para sua ferramenta de pesquisa de imagens no site. Mas se elas são assim tão maravilhosas, por que essa ferramenta está em alta só agora? E de onde vem tanto entusiasmo quanto às CNNs? Bom, para responder essas perguntas precisamos voltar alguns anos no tempo; mais especificamente, para 2012, no ILSVRC (ImageNet Large-Scale Visual Recognition Challenge), um dos maiores eventos de visão computacional do mundo. Neste ano Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton apresentaram o que eles chamaram de uma “rede neural convolucional grande e profunda”, a AlexNet, que seria o primeiro algoritmo de Deep Learning aplicado à visão computacional a ter o melhor desempenho na competição, com a taxa de erro de 15.4%, enquanto, para efeito de comparação, o segundo melhor algoritmo da competição teve um desempenho de 26.2%. Neste momento as CNNs tomaram os holofotes da visão computacional. No ano seguinte a ILSVRC estava cheia de CNNs, claro. A AlexNet abriu uma nova área para exploração, e ela foi explorada até chegar em marcas de erro inferiores a 4%, sendo que a expertise humana geralmente gera um desempenho entre 5% e 10%. Ou seja, estamos em uma época em que o computador já consegue reconhecer melhor o que ele vê do que você, caro leitor. Realmente parece que estamos descrevendo um livro do Asimov. Anteriormente, falamos que as NNs eram inspiradas nas ligações presentes em nosso cérebro. As CNNs têm sua ideia de um lugar parecido: o córtex visual. Em 1962 Hubel e Wiesel fizeram um experimento que mostrou que alguns neurônios reagiam somente na presença de bordas em certas orientações (bordas na vertical, horizontal, diagonal, etc), e que os neurônios se organizavam em uma estrutura colunar e , juntos, criavam a percepção visual. Essa ideia de neurônios especializados cumprindo tarefas diferentes no mesmo sistema serviu de base para a criação das CNNs. Como explicado acima, essa arquitetura usa muita coisa do que já conhecemos de Redes Neurais, só que agora nossos neurônios tem uma especialização adicional para reconhecimento de padrões em imagens: as convoluções e o pooling. A estrutura típica de uma Rede Neural Convolucional é: Entrada → (Conv → ReLU) → (Conv → ReLU) → Pool → (Conv → ReLU) → ReLU) → Pool → Fully Conected. Essa estrutura é a mais didática e simples, mas não a mais eficiente. Existem muitas outras estruturas que são muito mais usadas que essa, mas todas utilizam as mesmas camadas (Conv, Pool e Fully Connected), então cabe aqui explicar com a mais simples. Passaremos por cada etapa estrutural para entendermos bem suas funções. As camadas de convolução presentes nas CNNs são, essencialmente, o que as tornam capazes de identificar padrões nos inputs, geralmente imagens, tornando-as a principal característica de uma rede neural convolucional. Mas como essas camadas conseguem fazer isso? Ou melhor, o que é uma convolução? Intuitivamente, podemos entender convolução como uma operação que mede o grau de similaridade ou sobreposição entre duas funções. Vamos nos atentar a funções discretas, já que o computador, atualmente, só consegue lidar com esse tipo. Então, consideremos uma matriz que representa uma imagem. Ao convolvermos com uma outra matriz de dimensão menor, podemos detectar certos padrões, como bordas; e também, desfocar imagens , aumentar a sua nitidez e muito mais, é bem similar a um filtro não? Este é o nome mais comum dado a essa matriz, embora alguns se refiram como kernel. E são características como essas que desejamos captar pela CNN. Imagine que você possua uma lanterna; que, por algum motivo, ela seja quadrada e que você aponte ela sobre a imagem de entrada, de tal distância que a lanterna cobre uma área 3 x 3 da imagem original. A lanterna é nosso filtro, a área sobre a qual ela aponta é chamada de campo receptivo. Nesse caso, a convolução consiste em mover o filtro ao longo de toda a imagem, lembrando que o filtro, geralmente, tem tamanho menor que o da imagem de entrada, de forma que faz sentido falar em movê-lo pela imagem. Isso ocorre como ilustrado no GIF abaixo: Para cada uma das regiões 3x3 iluminadas pela lanterna, podemos identificar quão parecida essa região é ao filtro. O resultado da convolução (à esquerda) é, então, uma outra matriz que representa o grau de similaridade entre o filtro e cada região da imagem original. Para identificar esse grau de similaridade, utilizamos os pesos do filtro, ou seja, os valores de seus elementos. A forma como estes pesos são distribuídos ditam quais features serão identificadas por aquele kernel. Considere um filtro com a seguinte disposição: A imagem da direita mostra a forma que o filtro (na esquerda) identifica. Para identificar a semelhança entre o filtro e o seu campo receptivo, basta realizar a multiplicação ponto-a-ponto entre as duas matrizes e, em seguida, somar os valores obtidos. Este último valor se torna o elemento de uma nova matriz, que será enviada às camadas posteriores. Vamos ilustrar os processos descritos acima: Com o filtro anterior, trabalhando com a imagem original à esquerda, podemos identificar a região dentro do quadrado amarelo acima. Note como esta região da imagem é muito próxima em formato do formato do nosso filtro. Fica claro, portanto, que ao multiplicar o nosso filtro por esta região vamos obter um valor alto, por exemplo: A primeira imagem corresponde ao nosso campo receptivo, ou seja, à qual região da imagem estamos apontando a lanterna. Ao lado temos as duas matrizes que serão multiplicadas. O resultado da soma será: Soma = (50·30) + (50·30) + (50·30) + (20·30) + (50·30) = 6600, que corresponde à um número alto. Este valor será passado às camadas posteriores, passando a informação que naquela região da imagem o filtro foi ativado, ou seja, nesta região a imagem apresenta uma forma muito próxima à do filtro. Só para complementar essa explicação, vamos mostrar uma outra região do ratinho, e ver o quanto resulta a soma neste caso, utilizando o mesmo filtro: Analisando a região das orelhas, pegando um pouco do olhinho, vemos que esta área não tem nenhuma similaridade com nosso filtro. O resultado, portanto, da multiplicação e soma das matrizes acima é: Soma = 0, pois em nenhum ponto há sobreposição de valores não nulos. Podemos definir a matriz filtro de duas formas: manualmente, e assim ela identifica padrões que queremos identificar, como foi feito no caso do ratinho; ou automaticamente, e isso é feito durante o treinamento da rede como veremos mais adiante. Agora que você leitor já está absolutamente familiarizado com a estrutura da camada de convolução, vamos apresentar outras camadas, também muito importantes, que geralmente são usadas intercaladas com as conv layers. Mas não se preocupe, essas são bem mais simples que a convolucional. Em uma rede neural os neurônios aplicam às entradas a função de ativação, cujo principal objetivo é adicionar não linearidades ao modelo de treino. É comum que até este ponto a rede neural realize apenas manipulações lineares aos dados, como soma e multiplicação por pesos. O problema é que na realidade a grande maioria dos problemas reais não são lineares, e as funções de ativação tentam garantir a adaptabilidade da rede à tais circunstâncias. Existem as mais variadas funções de ativação, as mais comuns são: logística, tangente hiperbólica e a ReLU. Algo importante da função de ativação é que elas, idealmente, não devem ‘matar’ o gradiente, uma vez que este gradiente é muito importante no processo de backpropagation para garantir um bom treinamento e desempenho do modelo. Como podemos ver no gráfico acima, a ReLU entra neste contexto como uma função que não satura em valores positivos, evitando que nestes casos o cálculo do gradiente seja inviabilizado. Ela ainda tem seus problemas, mas é mais vantajosa que as anteriores por ser simples e fácil de derivar, diminuindo o tempo de treino. A área de funções de ativação ainda é muito pesquisada, não existe uma função ótima para qualquer modelo, já existem outras variantes da própria ReLU, por exemplo Leaky ReLU, que tentam corrigir alguns defeitos, mas ainda é uma área em progressão. A camada de Pooling é parecida com a camada de convolução no sentido de passar um filtro na imagem, mas tem um efeito diferente. No pooling, o passo do filtro, ou stride, é igual à dimensão do filtro. Isso significa que não há sobreposição entre os campos receptivos, como veremos mais abaixo. O objetivo é pegar alguma informação que indique o comportamento geral de cada campo receptivo. Para isso, existem o max pooling (adquire o máximo da região), mean pooling (adquire a média da região) e o min pooling (adquirindo o mínimo da região), sendo o max pooling o mais famoso e que iremos abordar nesse texto. Resumindo então, o max pooling é uma camada que pega o valor máximo de cada região da imagem, e os junta para formar uma nova imagem, agora com dimensão menor. Para entender melhor, observe a imagem a seguir: Como vimos, as camadas de convolução identificam características em partes das imagens com valores altos em sua saída, então a camada de pooling iria abstrair tal resultado, já que a posição exata não é tão importante quanto a posição relativa para as features mais abrangentes (de mais alto nível). Então é normal utilizarmos a camada de pooling após as convoluções, servindo para redução de dimensionalidade da entrada, o que ajuda computacionalmente, e para prevenir overfitting, já que de certo modo o pooling generaliza a entrada. As outras camadas eram novas na CNN e muito diferentes da NN clássica, mas essa aqui é bem conhecida de você, que já leu os textos anteriores dessa série. Essa camada é basicamente uma camada de NN normal, sem nenhuma diferença. Sua presença é justificada pelo fato que essa estrutura é muito boa para aprender padrões de alto nível, os quais são gerados por todas as camadas anteriores. Assim como as redes ‘vanillas’ (redes neurais convencionais), as CNNs precisam passar por um treinamento para aprender seus parâmetros, o que normalmente é feito com backpropagation em ambas as arquiteturas. Agora, não só os pesos da fully connected são parâmetros mas também cada elemento dos filtros aplicados nas convoluções. A ideia de definir uma função de erro e minimizá-la se mantém e é aplicada exatamente igual a antes, o que acontece é um aumento de complexidade da operação, por conta dos parâmetros das convoluções. Fazer uma CNN “na mão” é um processo muito complexo (mais ainda do que as NN normais) e muito dificilmente os resultados obtidos são melhores que as bibliotecas já existentes. Então, utilizaremos o Tensorflow 2.0 para mostrar um pequeno exemplo de aplicação dessa arquitetura em imagens… só que não nesse texto, e sim em um próximo post. Para você que chegou até esse ponto, muito obrigado! E esperamos que tenha conseguido seguir a explicação e aprendido algo novo. Caso esse post não tenha saciado sua sede por inteligência artificial, ou tenha ficado alguma dúvida sobre CNNs, nos procure nas redes sociais: Facebook, LinkedIn e Instagram, certamente poderemos ajudar; e aproveite o conteúdo que divulgamos por lá também. Se hidrate, faça carinho nos cachorros e até um próximo texto!"
https://medium.com/turing-talks/turing-talks-24-modelos-de-predi%C3%A7%C3%A3o-ensemble-learning-aa02ce01afda?source=collection_home---------87----------------------------,Modelos de Predição | Ensemble Learning,“Apes together strong!”,Enzo Cardeal Neves,761,8,2019-11-10,"Escrito por Enzo Cardeal e Bernardo Coutinho. Bem-vindos a mais uma edição do Turing Talks! Nessa semana falaremos sobre os métodos de Ensemble Learning, que consiste na união de diversos modelos de predição mais simples para obter um modelo geral mais consistente e menos suscetível a ruídos. Sem mais delongas, vamos direto à explicação! O conceito de Ensemble Learning, também chamado de aprendizado por agrupamento, se baseia na ideia de combinar diversos modelos de predição mais simples (weak learner), treiná-los para uma mesma tarefa, e produzir a partir desses um modelo agrupado mais complexo (strong learner) que é a soma de suas partes. Esse agrupamento objetiva, ao juntar múltiplos modelos mais fracos, diminuir a suscetibilidade geral deles a bias e a variance, tornando-os mais robustos. Portanto, os métodos de Ensemble devem levar em conta a maneira com a qual eles agrupam os modelos, associando os algoritmos de forma a minimizar suas desvantagens individuais no modelo final. Geralmente é escolhido apenas um modelo base para ser treinado em sets diferentes e posteriormente é feita a combinação, formando assim um modelo homogêneo. Se o Ensemble final for formado por modelos diferentes dizemos que é um preditor heterogêneo. Por exemplo, se quisermos criar um modelo Ensemble baseado no agrupamento de diversas árvores de decisão, que são modelos simples de alta variância, precisamos agregá-las de modo a aumentar sua resistência a variações nos dados. Logo, faria sentido treinar as árvores separadamente de forma a adaptar cada uma a partes diferentes da base de dados, assim, o conjunto delas saberia lidar com todas as variações dos dados. Esse é um dos principais pilares das Random Forests, o modelo mais popular de Ensemble. Existem variados métodos de Ensemble Learning, mas neste texto focaremos principalmente em Bagging, Boosting e Stacking, os tipos mais utilizados em Data Science. Suas principais características são: · Bagging: geralmente é feito com preditores homogêneos, cada um de forma independente em relação ao outro, de forma paralela. O algoritmo final é então feito a partir de algum tipo de resultado médio do que foi obtido a partir dos modelos bases. · Boosting: geralmente é feito com preditores homogêneos, que são aplicados de forma sequencial (o posterior depende do antecessor) e depois combinados no modelo final. · Stacking: geralmente é feito com preditores heterogêneos, treinando-os em paralelo. É então aplicado um modelo no output dos weak learners (podendo incluir ou não as features utilizadas para treiná-los). Antes de entendermos Bagging precisamos primeiro entender o que é Bootstrapping. Essa técnica consiste em subdividir nosso dataset em grupos menores, tomando elementos de forma aleatória e com reposição. Isso nos permite entender melhor o comportamento da média e do desvio padrão dos dados que estão além do dataset. Com isso, é possível fazer as pesquisas de opinião por exemplo. Aplicamos então o mesmo modelo de predição em cada um dos subgrupos e tomamos a média dos resultados como a predição do nosso modelo final. Essa abordagem gera um preditor com menos overfitting (por outro lado, esse método pode aumentar um pouco a bias). Como Bagging já foi abordado no Turing Talks #18 não iremos nos estender muito nessa parte. Os métodos de Boosting compõem uma outra categoria de algoritmo de Ensemble Learning, focada principalmente em reduzir a bias dos modelos iniciais. Esse tipo de aprendizado se tornou muito popular no meio da Ciência de Dados nos últimos anos por obter ótimas performances em competições de Machine Learning, devido a sua grande adaptabilidade. Mas qual a teoria por trás desse algoritmo? Assim como Bagging, os métodos de Boosting se baseiam em treinar diversos modelos mais simples a fim de produzir um modelo final mais robusto. Entretanto, nos algoritmos de Boosting os modelos não são mais treinados de forma independente entre si, mas de forma sequencial, a partir de uma análise dos modelos treinados previamente. De modo a maximizar o desempenho do preditor final, o Boosting treina iterativamente novos modelos sempre com um enfoque nas observações que os modelos anteriores tiveram mais dificuldade, tornando a predição mais resistente a bias. Em sequência, atualizamos o modelo para priorizar as predições com maior acurácia nas observações da base de teste. A maneira como ocorre esse treinamento e essa atualização é onde diferem os diferentes algoritmos de Boosting. Como o principal objetivo dos métodos de Boosting é de reduzir a bias dos preditores mais simples, é ideal escolher como base para a agregação um modelo simples com alto bias e baixa variância. Decorrente disso, geralmente escolhemos árvores de decisão de baixa profundidade para compor o preditor final. Adaptive Boosting, ou simplesmente Adaboost, é um algoritmo consiste em combinar de forma sequencial vários modelos mais fracos, sendo assim o weak learner subsequente leva em consideração as predições do anterior, para formar um preditor mais conciso. O diferencial desse algorítimo é que as predições mais difíceis (aquelas em que o weak learner da iteração atual mal previu) recebem um peso maior no preditor seguinte, buscando assim uma maior otimização do algoritmo final. Noutros termos, cada modelo é inicializado com um peso padrão, que define seu poder de decisão no modelo final. A partir disso, conforme vamos treinando esses modelos simples, cada weak learner ganha um peso maior para as observações que ele prevê corretamente, e um peso menor para as observações em que ele possui um alto índice de erro. Dessa forma, os weak learners com maior precisão terão maior poder de decisão no modelo final, produzindo um Ensemble extremamente robusto. O Gradient Boosting é um outro tipo de algoritmo de Boosting, que difere do Adaboost quanto à maneira com a qual os modelos são treinados com relação aos anteriores. Ao invés de estabelecer pesos para os weak learners, o Gradient Boosting treina novos modelos diretamente no erro dos modelos anteriores. Ou seja, os novos modelos tentam prever o erro dos modelos anteriores em vez de prever independentemente o target. Dessa forma, obtemos a predição final somando a predição de todos os weak learners. O algoritmo do Gradient Boosting funciona assim: O primeiro modelo faz uma aproximação bem simples da predição, e obtemos os nossos erros residuais: Depois, treinamos mais modelos nesses erros residuais, para tentar predizer o erro do primeiro modelo. Dessa forma, quando somamos as predições de cada modelo para obter a predição final, obtemos uma versão mais corrigida da primeira predição: Repetimos esse processo por várias iterações, obtendo erros residuais cada vez menores. Esse modelo é criado a partir das predições de weak learners, que são usadas como features . Essas novas features permitem que nosso modelo final agregue onde nossos modelos iniciais melhor performaram e descredite onde performaram mal. Na realidade, isso pode ser feito de diversas maneiras então vamos explicar aqui uma forma que será de mais fácil entendimento :) 2. Um modelo base (decision tree por exemplo) é fitado (treinado) em 9 partes e a predição é feita na parte restante. Isso é feito para cada parte do set de treino. 3. O modelo é então fitado em todo o set de treino. 4. As predições são então feitas no set de teste. 5. Os passos 2 e 4 são repetidos utilizando outro modelo base (KNN por exemplo), gerando outro set de predições tanto no set de teste quanto no set de treino. 6. As predições do set de treino são usadas como features para construir o modelo final (modelo stacking, nesse caso utilizamos logistic regression). Note que para construir o modelo final você pode também utilizar as outras features do dataset. Agora que você já sabe a teoria por trás dos métodos de Ensemble Learning, podemos ir direto para a prática! Vamos mostrar como fazer predições com um modelo de Boosting, e um modelo de Bagging. Mas primeiro, precisamos preparar a nossa área de trabalho! Primeiramente, vamos importar a biblioteca pandas, para poder lidar melhor com nossos dados. Se você não está familiarizado com a biblioteca, recomendamos ler o nosso Turing Talks sobre Bibliotecas de Data Science. Agora, precisamos de uma base de dados. Para esse exercício, escolhemos o Boston Housing, um dataset bem simples com dados de casas na área de Boston, cujo objetivo é prever o preço de cada uma. Como essa base requer pouca limpeza de dados, vamos direto à aplicação dos modelos. Se você quiser aprofundar mais em Data Cleaning, recomendamos nosso Turing Talks sobre esse assunto. Para exemplificar Boosting, vamos aplicar um modelo de Gradient Boosting no dataset de Boston, usando a biblioteca do Sci-Kit Learn. Depois de importar o modelo, precisamos configurá-lo e escolher seus hiperparâmetros. No caso, definimos o número de estimadores, a taxa de aprendizado, e a profundidade de cada árvore de decisão. Os valores ideais para cada parâmetro vão variar bastante dependendo de sua aplicação, então é recomendável ficar mexendo nessas grandezas para ver qual combinação funcionará melhor. Vamos mostrar também um exemplo de como aplicar um modelo de Bagging: o Random Forest. Porém, como já abordamos o tema mais aprofundado em um Turing Talks anterior, não vamos delongar muito na explicação. Se quiser rodar o notebook você mesmo, disponibilizamos ele no Google Colaboratory! Nesse post abordamos vários conceitos-chave (bootstrapping, boosting, bagging, stacking e random forest) para se entender Ensemble Learning. Um modelo mais complexo que não tratamos é o XGBoost (eXtreme Gadrient Boosting), que consiste na aplicação do Gradient Boosting juntamente com outras técnicas para fazer o algoritmo aprender de maneira mais eficiente. Por fim, gostaríamos de salientar que os métodos de Ensemble Learning não são nada engessados, o que permite muitas variações que vão depender de cada problema abordado. Portanto, lembre-se: entenda bem a situação que você está lidando e… abuse de sua criatividade! Para os amantes de IA que nos acompanharam até aqui, nossos mais profundos agradecimentos! Não se esqueçam de conferir nossas redes: Medium, Facebook, Instagram e LinkedIn. Até a próxima pessoal!"
https://medium.com/turing-talks/turing-talks-25-redes-neurais-com-keras-e-tensorflow-2-0-44fc0974c7fb?source=collection_home---------86----------------------------,Redes Neurais | Redes Neurais com Keras e TensorFlow 2.0,Redes neurais na prática de forma simples e rápida.,Lucas Reis,853,7,2019-11-17,"Nesse texto vamos ver como criar e treinar um modelo de rede neural utilizando a API Keras, do módulo TensorFlow 2.0, desenvolvido pela Google. Os códigos usados estão em Python, principal linguagem para trabalhar com TensorFlow e as bibliotecas necessárias foram baixadas usando Anaconda. Como esse post trata o tema de redes neurais é recomendado que se tenha um conhecimento básico sobre elas. Caso não se tenha nenhum, ou caso queira revisar o tema, leia a série anterior de Modelos de Predição que trata do assunto: A maior vantagem de se implementar Keras ao invés de escrever todas as funções na mão, como foi feito anteriormente no Turing Talks #22, é pela diminuição na complexidade, reduzindo para poucas linhas de código o que levaria dezenas de linhas. Também é possível obter um grande ganho em velocidade para rodar o código, com o fato da API ter sido criada para ser mais rápida possível, sem ganhar complexidade para criar modelos simples. Para usar essas ferramentas é necessário primeiro carregá-la no seu ambiente de programação, utilizando o código a seguir, lembrando que estamos usando a versão 2.0 do TensorFlow para esse tutorial: Agora que estamos com tudo que precisamos carregado vamos copiar o modelo usado para classificar a base de dados de números manuscritos MNIST presente no Turing Talks #22, montando ela usando Sequential. Sequential é como o Keras organiza seu modelo, com cada uma das camadas adicionada colocada em sequência (daí seu nome), sendo que a primeira camada recebe os dados de entrada do sistema e manda sua saída para a próxima camada, e a saída da última camada sendo a saída do modelo. Essas camadas são chamadas layers, podendo ser vários tipos diferentes, tendo como exemplo algumas das mais usadas: Vale a pena lembrar que esse são apenas uma pequena parcela dos layers disponíveis pelo Keras, e mesmo nos exemplos escolhidos é possível modificar muitos mais hiperparâmetros presentes dentro das camadas. A documentação do Keras é sempre recomendada para caso se queira aprofundar mais no assunto. Para se criar um modelo é necessário primeiro criar uma instância de Sequential e adicionar camadas a ele com o comando add(). Seguindo o mesmo modelo criado no Turing Talks #22, com apenas um hidden layer de 800 neurônios e um output layer de 10 neurônios, um para cada caractere, obtemos o modelo: Com o treinamento desse modelo obtivemos uma rede com acurácia de 90,8% na predição de números manuscritos. Pronto! Todo o processo de criar uma rede neural e treiná-la pode ser reduzido em apenas algumas linhas de código usando Keras. Uma ótima ferramenta disponível com o TensorFlow é o TensorBoard, que disponibiliza a visualização da performance de um modelo treinado, a comparação desse modelo com outros já treinados, a evolução dos pesos, e muito mais. Para conseguir utilizar essa ferramenta é necessário adicionar algumas linhas de código antes de dar o comando fit() e adicionar um parâmetro callbacks novo à ele: Agora é necessário abrir o terminal de comando na pasta em que o programa está e rodar o seguinte comando: O terminal não deve retornar nenhum erro e devolver que o TensorBoard está disponível no localhost:6006, sendo apenas necessário copiar e colar o link no browser para acessar a ferramenta, se deparando com algo desse tipo: Até agora lidamos com camadas de neurônios, que têm como entrada um vetor, uma sequencia de N valores, e tem como saída um outro vetor de tamanho diferente. Agora vamos lidar com uma camada de convolução, que tem como entrada uma série de matrizes NxM e como saída uma outra série de matrizes de tamanho diferente, explicado com detalhe no Turing Talks #23, leitura altamente recomendada para compreender os conceitos que serão abordados a seguir. A principal diferença de se tratar com Redes Neurais Convolucionais (Convolutional Neural Networks, em inglês) é o fato de se estar trabalhando com modelos que têm como entrada matrizes ao invés de vetores (note que no exemplo anterior seria possível tratar os dados para serem bidimensionais sem alterar o treinamento). O formato usual desses modelos é ter como as primeiras camadas de convolução e de pooling, depois os dados são achatados num vetor e tratados por camadas de neurônios. Utilizando a base de dados CIFAR10, composta por imagens de 32 por 32 pixels de 10 classes diferentes (avião, carro, pássaro, gato, cervo, cachorro, sapo, cavalo, barco e caminhão), é possível montar o seguinte modelo como exemplo: Foi possível conseguir uma acurácia de 96.1% seguindo apenas um modelo padrão de Rede Neural Convolucional. Agora que você sabe os básicos dessa ferramenta incrível é extremamente recomendável que você pegue o código completo aqui e brinque com ele o quanto quiser para ver como isso afeta a velocidade de treino e acurácia do modelo. Adicione mais camadas, mude os hiperparâmetros das camadas e de treinamento, mude função de ativação da saída da camada, mude o otimizador de treinamento, entre na documentação e pegue uma coisa que nem foi explicada nesse texto, o céu é o limite. Só lembre que a ultima camada deve ter uma saída de 10 números tanto para a base MNIST como para a base CIFAR10. Também vale ressaltar que a utilização da plataforma Anaconda para lidar com o TensorFlow e Keras é muito recomendada, tendo que rodar o comando conda install -c conda-forge keras no terminal do Anaconda para baixar todos os itens utilizados nesse tutorial, também é necessário avisar que para lidar com o python e TensorFlow baixados no anaconda deve ser usado o terminal do Anaconda. Curtiu o tutorial? Esperamos que você possa usar o que foi ensinado aqui na sua vida, aplicando seu conhecimento de inteligência artificial. Não esqueça de conferir nossos outros posts no Facebook, LinkedIn e Instagram para ver tudo sobre o mundo de IA e mais coisas que podem ser feitas com os conhecimentos fornecidos nesse texto. Até mais!"
https://medium.com/turing-talks/turing-talks-27-modelos-de-predi%C3%A7%C3%A3o-lstm-df85d87ad210?source=collection_home---------85----------------------------,Redes Neurais | LSTM,Como lidar com perda de memória com Machine Learning.,Fernando Matsumoto,717,8,2019-12-15,"Escrito por: Fernando Matsumoto, Guilherme Duarte e Leonardo Murakami. Fala, galera! Ansiosos para aprenderem mais sobre as Redes Neurais Recorrentes? Seguindo o último post a respeito do assunto — caso você tenha perdido, basta clicar aqui — , daremos continuidade no estudo das redes neurais com memória. Em particular, hoje conheceremos um dos modelos mais usados de RNNs, as LSTMs. Um dos atrativos das RNNs é a ideia de que elas podem conectar informações anteriores à tarefa atual, como o uso de frames de vídeo anteriores para auxiliar no entendimento do quadro atual. Se as RNNs realmente pudessem fazer isso, elas seriam extremamente úteis. Mas elas podem? Algumas vezes, nós precisamos apenas olhar para informações recentes para realizar a tarefa desejada. Por exemplo, imagine um modelo que tenta prever a próxima palavra baseada nas anteriores, como a função de autocompletar do Google. Se estamos tentando prever a última palavra na frase “as nuvens estão no céu”, não é necessário nenhum contexto, afinal, é bem óbvio que a última palavra será “céu”. Em tais casos, onde o gap entre a informação relevante e o lugar que ela é requisitada é pequeno, as RNNs podem aprender a usar informações passadas. Repare aqui que o contexto se refere apenas a termos que estão longe do presente, afinal, para a rede prever corretamente a palavra “céu”, precisamos saber que ela está antecedida das palavras “nuvens” e “estão”. Então temos esses casos onde o contexto não importa, mas também há casos onde ele é importante. Considere tentar prever a última palavra no texto “Eu cresci na Inglaterra… Eu falo fluentemente inglês”. A informação recente sugere que a última palavra é provavelmente um idioma, mas se queremos acertar qual exatamente é, precisamos saber que o sujeito que está falando cresceu na Inglaterra. Agora reparem nas reticências colocadas entre as duas partes do texto. É possível que elas representem um texto pequeno, mas também é possível que elas estejam escondendo um texto gigante. Ou seja, o gap entre a informação relevante e o ponto onde ela é requisitada pode se tornar bem grande. Infelizmente, conforme esse gap aumenta de tamanho, as RNNs comuns se tornam incapazes de aprender a conectar as informações. O que acontece na verdade é que elas sofrem do chamado vanish gradient problem. O gradiente é o valor usado para atualizar os pesos de uma rede neural. O vanishing gradient problem ocorre quando o gradiente desaparece conforme ocorre o backpropagation through time. Se o valor de um gradiente se torna pequeno, ele não contribui muito para o aprendizado. Então em RNNs, as camadas que recebem esse gradiente não aprendem. Geralmente essas são as primeiras camadas. É isso que leva ao “esquecimento” das informações mais antigas. Caso você queira saber mais sobre o vanish gradient problem, basta dar uma olhada nesse post. Felizmente, as LSTMs não têm esse problema! As redes Long Short-Term Memory (memória de curto e longo prazo), usualmente chamadas simplesmente de “LSTMs”, são um tipo especial de rede neural recorrente, pois são capazes de aprender conexões de longo prazo. Dessa maneira, elas têm um incrível poder de predição e funcionam muito bem em uma variada gama de problemas, sendo amplamente usadas atualmente. A arquitetura de uma LSTM, que veremos mais à frente, faz todo sentido do ponto de vista biológico. Como já sabemos, há diversas analogias entre as redes neurais biológicas e as redes neurais artificiais. Quando olhamos para o modo como o cérebro humano funciona, descobrimos que a nossa memória pode ser dividida em dois tipos: Intuitivamente, é assim que as LSTMs funcionam. Por uma célula desse tipo de rede, existem dois canais, que podem ser interpretados da mesma maneira como interpretamos a memória humana! Para entender o funcionamento das redes LSTM, vamos rever brevemente as camadas recorrentes básicas: Nesse diagrama, cada célula recorrente (os retângulos verdes) recebe um input x e o estado do instante anterior e gera um output o. Uma forma de explicitar como essa transformação do input para o output é feita, é dada no diagrama abaixo: Esse diagrama mostra uma única camada convolucional ao longo do tempo. Observe que o input x é combinado com o estado anterior h(t-1) e passado à função de ativação tanh, gerando uma saída h(t). Os pesos U e V ficam implícitos na função de ativação. Esse novo tipo de diagrama será muito útil para o entendimento de redes mais complexas, como a LSTM. No caso da RNN, a única informação passada de um instante de tempo para o próximo é a saída h, também chamada de hidden state. Essa saída corresponde a uma memória de curto prazo, devido ao vanishing gradient. Para a LSTM, como já vimos, é necessária a adição de uma memória de longo prazo. O principal trabalho se torna entender como uma memória deve influenciar a outra, ou seja, como a rede decide quais partes da memória de curto prazo devem ser lembradas e como a memória de longo prazo deve afetar o entendimento atual do texto. Para isso, serão utilizadas gates (ou portas) que controlam a passagem de informações entre os dois canais. O componente básico dessas gates é a seguinte estrutura: Essa estrutura aplica uma função sigmóide ao input b e multiplica o resultado pelo sinal a da linha horizontal. Como a saída da função sigmóide é um número entre 0 e 1, essa estrutura controla qual porcentagem de cada valor deve passar pela porta. Na LSTM, temos 3 gates. Em ordem, são elas: Por exemplo, suponha que o algoritmo queira prever a próxima palavra do texto e se depare com o seguinte trecho: “[…] Eu falo inglês e o meu amigo, ???”. Ao chegar na palavra amigo, esses passos se traduzirão em:1. esquecer o sujeito “eu” (forget gate);2. lembrar do sujeito “meu amigo” (input gate); e3. utilizar o cell state para lembrar qual idioma o meu amigo fala (output gate). Passados esses preliminares, vamos agora ver o esquema completo de uma rede LSTM: Parece bem mais complicado que o da RNN, mas vamos ver que pensando em termos de gates o comportamento dessa rede não é tão difícil de entender. Primeiramente, observe que há duas linhas principais. A de cima, em laranja é chamada de célula de memória, pois armazena o cell state (a memória de longo prazo). A de baixo, correspondente à memória de curto prazo, contém a entrada e o hidden state (a saída no instante anterior). Entre essas linhas, estão localizadas as 3 gates que controlam a passagem de informação. A forget gate, que determina quais partes do cell state são importantes e quais devem ser esquecidas. Conforme a figura abaixo, os vetores h e x são passados por uma função sigmóide (novamente, com uma matriz de pesos Wf implícita), dando um vetor f que é multiplicado pelo cell state. Em seguida, determina-se quais novas informações da memória de curto prazo devem ser colocadas no cell state, por meio da input gate. Para isso, são realizados o cálculo de: O último passo é utilizar o cell state para calcular a saída h da rede. Para isso, primeiramente, passa-se o estado C por uma função de ativação tanh. Em seguida, a memória de curto prazo é utilizada para determinar quais partes dessa saída são realmente relevantes nesse instante (output gate). No caso da RNN, o vanishing gradient ocorre por causa da função de ativação. Como na célula de memória não é aplicada nenhuma função de ativação (as funções são aplicadas só nas gates), não temos esse problema. Existem diversas variantes do modelo de LSTM descrito neste post. Algumas delas podem ser encontradas nesse post, que foi uma das inspirações para a explicação acima. As redes neurais baseadas em células de memória podem ser utilizadas para produzir textos, onde cada letra ou palavra gerada utiliza-se da memória de todos as outras geradas até então. O modelo utiliza as palavras iniciais para predizer a próxima com base na probabilidade dela ocorrer. As camadas de memória possibilitam o reconhecimento de escrita, seja juntamente às camadas convolucionais ou de maneira independente. Os pixels são passados da direita para a esquerda (caso seja o sentido da linguagem desejada) em batches. Com o passar do tempo, as células emitem as probabilidades, como no exemplo abaixo: Caso você tenha se interessado por essa aplicação, recomendamos esse link. Assim como textos são sequências de caracteres, músicas são sequências ordenadas de notas e sons. De tal modo, uma LSTM pode levar em conta as últimas notas tocadas para gerar a próxima. As redes com células LSTM são capazes de produzir resultados significativos no campo de séries temporais, seja para produzir predições do preço de ações, ou para identificar sazonalidade. Essas redes possuem um menor índice de erro que redes neurais recorrentes básicas, por exemplo. Sabemos que o caminho até aqui foi árduo e, muito provavelmente, seja necessária mais de uma leitura a respeito deste e do post anterior. Porém, não se preocupem. Todo esse esforço será recompensado em nossa 3ª parte, onde, finalmente, colocaremos as mãos na massa e iremos implementar nossa primeira RNN/LSTM, juntos, treinando e testando-a com o TensorFlow 2.0. Gostou desse texto? Então não deixe de mostrar seu suporte curtindo e compartilhando esse texto com as pessoas que você adora! Aproveite também para conferir nossos outros conteúdos no Facebook, LinkedIn e Instagram, para se aproximar da nossa comunidade, o Grupo Turing, e ficar sempre antenado com o que há de melhor em inteligência artificial."
https://medium.com/turing-talks/aprendizado-por-refor%C3%A7o-1-introdu%C3%A7%C3%A3o-7382ebb641ab?source=collection_home---------84----------------------------,Aprendizado por Reforço #1— Introdução,,Enzo Cardeal Neves,876,11,2020-02-23,"Bem-vindos a mais uma edição do Turing Talks! Nessa semana daremos início a nossa série de aprendizado por reforço, uma técnica de Machine Learning um tanto quanto diferente das abordadas nos posts anteriores. Nesta publicação trataremos de conceitos básicos e daremos uma visão geral sobre o tópico. Então, caso você já tenha alguma familiaridade com o assunto, recomendamos que acompanhe os próximos posts. A ideia básica é que um agente (explicaremos melhor este conceito a seguir) procura cumprir uma determinada tarefa, inicialmente com um abordagem de tentativa e erro. Posteriormente, os resultados de cada tentativa, independente de seu sucesso, são utilizados para treinar o agente por meio de um sistema de recompensa/punição e determinar se as ações tomadas são válidas ou não. Daí a ideia de aprendizado por reforço. O agente aprende tanto com os erros quanto com os acertos, e no final é esperado que ele execute a tarefa com maestria. Suponha agora que você nunca tenha jogado Pong, o famoso jogo de Atari. Você irá jogá-lo pela primeira vez, mas ninguém te passou as regras e as únicas informações sabidas sobre o funcionamento do jogo é que é possível mover a barra direita para cima ou para baixo e que a tela “Game Over” é o pior resultado possível. O jogo se inicia e, reparando na bola vindo em sua direção, você decide tomar a seguinte abordagem: desviar toda vez que a bola chegar perto de você. O jogo segue, até que o placar atinge 11:0 e a tela “Game Over” é exibida. Para próxima partida, com base em sua experiência anterior, você resolve mover a barra na direção da bola toda vez que ela se aproxima. Com essa nova estratégia você conseguiu vencer o jogo e evitar a tela de “Game Over”. Esse exemplo, apesar de um pouco absurdo na perspectiva de um modelo de Reinforcement Learning, pois a evolução do agente geralmente requer muito mais do que apenas um episódio (nesse caso, partida jogada) para aprender a tarefa desejada, ilustra como funciona a lógica do aprendizado por reforço. Comparando com os anteriores, Reinforcement Learning se assemelha com o primeiro no sentido de aprender com as experiências anteriores e com o segundo no sentido de aprender de uma forma não supervisionada, porém essas experiências geralmente são simuladas, como veremos a seguir, e o aprendizado se dá tanto com os acertos quanto com os erros. Antes de nos aprofundarmos no assunto é importante entender alguns conceitos-chave: A lógica do aprendizado por reforço é fácil de entender e visualizar pois se assemelha bastante com a forma como nós aprendemos. O desafio, então, é traduzir essa lógica para uma forma que a máquina possa executá-la. Para isso, é utilizado um modelo matemático conhecido como Markov Decision Process. O processo se dá da seguinte maneira: no instante inicial (t), o agente seleciona uma ação, muda para um novo estado, recebe uma recompensa e então o ciclo é reiniciado para o próximo instante (t+1). Para entender esse comportamento de forma probabilística, iremos trabalhar com a probabilidade de uma transição de estado. Essa probabilidade será função do estado e ação imediatamente anteriores, não importando todo o resto. Imagine um indivíduo hipotético que só pense em três coisas: dormir, correr ou tomar sorvete. Caso ele esteja correndo, há 60% de chance dele continuar, 30% de chance dele ir tomar sorvete ou 10% de chance dele ir dormir. Para cada um dos estados que esse indivíduo consegue almejar e atingir, existem as chances correspondentes de migração para outro pertencente a esse grupo. Essa é uma ilustração das probabilidades de transição. Formalmente, definimos a probabilidade de transição para o estado s’, com recompensa r por tomar a ação a, como: O retorno é um conceito importante pois ele representa o sucesso do agente. Ele é definido como a soma das recompensas futuras (repare que o que acompanha o primeiro termo é t+1 e não t), sendo o objetivo do agente maximizá-lo. É equacionado como segue: Mas e no caso de uma tarefa que não possui um final bem definido? Por exemplo, o jogo Flappy Bird, caso o jogador jogue de maneira perfeita (situação esperada de um agente bem treinado), o jogo segue indefinidamente. Nesse caso, o valor do retorno tenderá para o infinito, o que não é muito conveniente pois precisamos que ele convirja para um valor fixo. Para contornar esse fato utilizamos o retorno com desconto (discounted return). É definido um fator de desconto γ, entre 0 e 1, que vai determinar a importância dada as recompensas mais futuras. Matematicamente, escrevemos: Quanto menor o γ, mais relevante são as recompensas mais imediatas. A política é a função que mapeia um dado estado em probabilidades de se selecionar cada ação possível nele. A política é atualizada até atingir uma configuração ótima (quando o agente executa sua função corretamente). Então, se um agente segue uma política π em um determinado momento t, então π(a|s) é a probabilidade da ação a ser executada caso o estado correspondente seja s. Definida em função do estado, determina quão bom ele é caso o agente siga uma política π pelo resto do processo. O resultado dessa função é o retorno médio seguindo essas condições. É equacionado como segue: Definida em função do estado e da ação, determina quão bom é para o agente tomar esta ação específica, nesse estado, caso ele siga a política π pelo resto do processo. O resultado dessa função é o retorno médio seguindo essas condições. Equacionando: Repare que para as duas funções acima, como as ações tomadas pelo agente são influenciadas pela política, elas também são definidas em relação a essa política. Quando o agente executa sua função da forma esperada é dito que a política ótima foi atingida. Isso ocorre quando para todo estado possível a função state-value atinge seu valor máximo. Em outras palavras: Atrelado a política ótima há também a Q-function ótima, que é quando a função state-action atinge seu valor máximo para todo estado e ação possíveis. Expressando formalmente: Esse tópico é um tanto quanto complexo e será melhor elucidado em um Turing Talks futuro totalmente dedicado a ele. Por hora, iremos aceitar o seguinte fato: uma propriedade fundamental da Q-function ótima é que ela deve satisfazer a seguinte equação: conhecida como equação de Bellman, onde s’ e a’ representam o estado e a ação no instante t+1, respectivamente. Ela é útil pois, uma vez determinada a Q-function ótima, podemos determinar a política ótima porque, com q∗, para qualquer estado s, o algoritmo de reinforcement learning consegue encontrar a ação a que maximiza q∗(s, a). E como a gente aplica tudo isso pra poder chegar no nosso modelo esperado? Há vários algoritmos para fazer isso(SARSA, Deep Q-network, Deep Deterministic Policy Gradient etc), mas vamos nos ater a explicar como funciona o Q-learning. O objetivo do Q-learning é gerar a política que maximize o total de recompensa recebida depois de todos os estados sucessivos. O algoritmo atualizará iterativamente os valores de q para cada par estado/ação (s, a) usando a equação de Bellman até que o valor da função convirja para q∗. Para entender melhor, vejamos no exemplo. Suponha o seguinte jogo: há 9 posições possíveis, sendo que em cada uma pode haver maçãs ou armadilhas. O jogador é a cobrinha que pode se mover para cima, para baixo, direita ou esquerda. O objetivo é coletar o maior número de maçãs com no máximo 5 movimentos. Caso a cobrinha passe por uma armadilha, o jogador é derrotado. A cobrinha será nosso agente e o tabuleiro será o ambiente. Vamos então montar nossa estrutura de recompensas para treinar o agente: De início, o agente sabe apenas para quais direções pode se mover, porém, não tem noção alguma de como esse sistema de pontuação funciona e todo o tabuleiro é uma grande incógnita. Para saber mais sobre o jogo, a cobrinha deve explorar o tabuleiro. Sendo assim, o valor de q para cada par (s, a) é inicializado com 0 e após cada partida esses valores são atualizados. Podemos montar uma tabela com todas as ações possíveis(movimento da cobrinha) em cada estado(casas do tabuleiro) para armazenar os valores de q. Após cada partida, os valores de q na tabela se aproximam cada vez mais do valor ideal, possibilitando que o agente os use de referência para tomar as decisões adequadas e vencer o jogo. Se a política determina que o agente deve tomar as ações que maximizem o retorno, como a cobrinha irá tomar uma decisão na primeira partida se o valor inicial da função state-action é o mesmo para todo par (s, a)? Inicialmente o agente deve tomar ações aleatórias, sem buscar aumentar o retorno, para melhor entender o ambiente e ajustar os valores de q. Esse processo é conhecido como exploração (exploration). Com base nos conhecimentos adquiridos, a cobrinha passa a tomar decisões mais lógicas para cumprir seu objetivo (vencer o jogo). Essa abordagem fica conhecida como aplicação (note que aqui fizemos uma tradução livre do conceito que é visto nas literaturas em inglês, exploitation, para ficar de mais fácil entendimento para o leitor). De início, o comportamento predominante é o de exploração e posteriormente, passa a ser o de aplicação. Essa mudança gradual é importante pois se o agente apenas explorar, ele continuará a agir de maneira desmotivada, no sentido de nunca tentar cumprir a tarefa proposta. Se ele apenas aplicar, antes de ter explorado por completo o ambiente, ele poderá cair numa política pseudo-ótima. Imagine a seguinte situação: Antes de explorar todo tabuleiro a cobrinha encontra primeiro o par de maçãs. Para encontrá-las foram necessários 2 movimentos. Os outros 3 movimentos restantes não são suficientes para atingir outras maçãs e claramente esta não é a situação que a cobrinha mais comeu. Porém, como o tabuleiro não foi totalmente explorado, o agente acredita estar seguindo um política ótima, quando na verdade não está. Daí a importância tanto da exploração quanto da aplicação. Para conseguir esses pesos diferenciados para exploração/aplicação utilizamos um fator de exploração ϵ iniciado em 1. Esse fator representa a probabilidade de o agente tomar uma ação que fuja da política, ou seja, uma exploração. Com o passar do episódios, o fator decai e o agente toma uma abordagem mais “gananciosa” (greedy), buscando aplicar cada vez mais o que foi aprendido anteriormente. No decorrer do post nos restringimos em demonstrar o funcionamento do Reinforcement Learning apenas com sua utilidade em treinar agentes para jogos. Mas não se deixe enganar em achar que essa é a sua única aplicação (ufa!). Ela foi exaustivamente usada pois é a mais tangível para a maioria dos leitores. Vamos ver a seguir algumas outras. Robôs que aprendem sozinhos a realizarem tarefas complexas já fazem parte de algumas fábricas pelo mundo, como a Fanuc, fabricante japonesa de robôs. Ajustes de preços de acordo com a oferta e demanda podem fazer uso de técnicas como Q-learning, sendo utilizada durante as interações com os clientes. O tratamento de regime dinâmico (DTR) é tópico de várias pesquisas médicas da atualidade. Algoritmos de RL ajudam a processar dados clínicos para se chegar em um estratégia de tratamento, usando-se indicadores recolhidos de pacientes. A Pit.ai é pioneira em utilizar as técnicas de reinforcement learning para avaliar estratégias de investimentos, e tem se mostrado uma ferramenta bastante robusta. Reconhecer o perfil de cada cliente e sugerir preços e ofertas que terão mais chances de gerarem uma venda bem-sucedida. É importante que tenha ficado claro o seguinte: a partir do mapeamento de cada estado do ambiente, por meio do que é aprendido com as experiências anteriores, tanto favoráveis quanto desfavoráveis, é possível atingir uma política ótima que permitirá que o agente execute a tarefa proposta. Os episódios iniciais são majoritariamente para exploração enquanto os finais são focados em aplicação. Construímos aqui a fundação para que você possa acompanhar sem grandes dificuldades as próximas publicações dessa série. Caso não tenha entendido as minúcias da matemática, não se preocupe, o mais importante é entender a ideia geral do que cada função representa. Para os guerreiros que chegaram até aqui, nossos mais profundos agradecimentos. Claramente, assim como nós, apaixonados por Machine Learning! Não se esqueçam de conferir nossas redes: Medium, Facebook, Instagram e LinkedIn. Até a próxima!"
https://medium.com/turing-talks/aprendizado-n%C3%A3o-supervisionado-redu%C3%A7%C3%A3o-de-dimensionalidade-479ecfc464ea?source=collection_home---------83----------------------------,Aprendizado Não Supervisionado | Redução de Dimensionalidade,Aplicando PCA e CDA,Felipe Augusto de Moraes Machado,731,4,2020-03-01,"Hoje abordaremos com um experimento uma importante área da Aprendizagem Não-supervisionada: Redução de Dimensionalidade. Aplicaremos duas técnicas distintas, uma linear e outra não linear, em um problema para tentar transformar 4096 variáveis em… apenas uma! Parece desafiador, não? Antes de começarmos a nossa experiência, talvez o leitor esteja com uma dúvida: por que reduzir a dimensão dos nossos dados? A Maldição da Dimensionalidade é um fenômeno que aparece quando temos uma quantidade de variáveis bem grande em um problema: às vezes, possuímos muitas variáveis altamente correlacionadas, ou seja, elas são redundantes; ou podemos ter variáveis que não apresentam informação útil ao problema. Isso faz com que o modelo selecionado possua muitos parâmetros, o que pode causar overfitting. Então, para evitar isso, utilizamos técnicas de Redução de Dimensionalidade. Talvez a técnica mais conhecida de de Redução de Dimensionalidade seja o PCA. Sua popularidade é dada por dois motivos: O PCA é baseado na variância dos dados, ou seja, ele tenta criar uma nova representação dos dados, com uma dimensão menor, mantendo a variância entre eles. Seus novos eixos são descorrelacionados (ou seja, a esperança do produtos deles é nula). Cada eixo possui uma variância, normalmente dada em % em relação ao todo. Na imagem do início desse post, há um gráfico com cujos eixos são os dois primeiros Componentes Principais com um total de 73% + 22.9% = 95.9% da variância total. Note que os dados do Iris (banco de dados utilizado na imagem do início)possui 4 dimensões, porém o PCA consegue manter 96% de variância com apenas 2 novos eixos. Praticamente uma redução de 50% das variáveis! Não é difícil implementar essa técnica, esse tutorial ensina passo a passo. A biblioteca Sklearn já possui uma classe de PCA implementada. Futuramente, iremos preparar uma aula só para explicar o PCA, visto que ele possui uma grande importância nessa área, além de muitos outros algoritmos derivarem dele. Essa técnica é pouco conhecida, porém é uma das técnicas mais poderosas de Redução de Dimensionalidade por ser uma técnica não linear. Ela é bem complicada de implementar e possui um custo computacional bem elevado comparado ao PCA, pois é baseado em distância de grafos. Para a sua implementação, esse artigo contém sua formulação matemática. Esse algoritmo requer alguns parâmetros, incluindo a nova dimensão dos dados. Essa técnica e outras baseadas em distância serão explicadas em futuros posts. Para aplicar essas técnicas e comparar seus resultados, vamos fazer um experimento. Primeiro, pegamos o logo do Grupo Turing. Deixamos a imagem em Grayscale e reduzimos seu tamanho de 960x960 para 64x64. Depois, pegamos essa imagem e rotacionamos ela de 0º até 360º, 0.5º de cada vez, salvando a imagem rotacionada em um vetor de 4096 (64x64) variáveis. Obtemos assim uma tabela de 720 linhas e 4096 colunas. O código da função que rotaciona as imagens está abaixo, foi utilizado a biblioteca OpenCV (cv2 no Python) para manipulação das imagens: Temos então 720 dados de dimensão igual a 4096. Embora nosso problema tenha uma dimensão alta, só variamos o angulo entre as imagens. Será que conseguimos transformar esses dados para uma única dimensão? Parece impossível! Aplicando PCA, conseguimos reduzir de 4096 para 136 variáveis (com 99% da variância). Uma redução de 96.7% de variáveis! Porém, ainda não conseguimos chegar a uma única dimensão. Utilizamos CDA para tentar transformar essas 136 variáveis obtidas pelo PCA em apenas 1. O algoritmo convergiu em 14 iterações, formando uma base de 720 dados de uma única dimensão. Em seguida, foram pegos 6 pontos dessa nova base, deixamos em ordem crescente e selecionamos as suas respectivas imagens para fazer o gráfico abaixo: Podemos ver que, com essa única variável, somos capazes de achar a rotação da imagem! Conseguimos, então, transformar dados de dimensão igual a 4096 para uma única dimensão! Nesse post introdutório, vimos uma aplicação de duas técnicas de redução de dimensionalidade: PCA, linear e simples; CDA, não linear e complexo. Conseguimos transformar 4096 variáveis em apenas uma. Mas como saberemos, em outros problemas, até quantas variáveis podemos reduzir? Como saber qual técnica vai performar melhor em cada problema? Como quantizar o erro de cada modelo? Muitas perguntas difíceis, mas não se assustem: apresentaremos a vocês em outros posts como responder tudo isso. Então não perca as próximas semanas, teremos uma boa quantidade de algoritmos dessa área para mostrar. Gostou do texto? Que tal continuar aprendendo semanalmente sobre inteligência artificial? Basta seguir o Grupo Turing em uma das redes: Facebook, LinkedIn ou Instagram. Agradecemos seu engajamento com nossa publicação e até uma próxima!"
https://medium.com/turing-talks/aprendizado-por-refor%C3%A7o-2-processo-de-decis%C3%A3o-de-markov-mdp-parte-1-84e69e05f007?source=collection_home---------82----------------------------,Aprendizado por Reforço #2 | Processo de Decisão de Markov — Parte 1,Processos e recompensas de Markov,William Fukushima,861,9,2020-03-08,"Escrito por William Fukushima e Lucas Oliveira Reis Olá seres humanos (ou o que quer que sejam). Bem vindos a mais um Turing Talks! Neste capítulo, iniciaremos o tópico de Processo de Decisão de Markov, uma maneira simples de descrever problemas de Aprendizado por Reforço que requerem tomadas de ações com incertezas, como diagnósticos médicos, tratamentos de enfermidades, controle da movimentação de um robô, e muito mais. Nossa jornada será realizada por meio de dois textos, nos quais iremos destrinchar o máximo possível de cada conceito, então não se preocupe porque qualquer base de probabilidade será explicada sucintamente. Mas caso esteja bastante confuso e precise de uma base mais forte, confira nossa edição do Turing Talks em que ensinamos probabilidade para machine learning: Para explicar o Processo de Decisão de Markov, primeiramente precisamos compreender: - Propriedade de Markov; - Função de transição; - Processo de Markov; - Ambientes Parcialmente e Completamente Observáveis; - Tarefas Episódicas e Contínuas; - Recompensa e Retorno; - Função de Valor; - Processo de Recompensa de Markov; - Função de Política. Após tudo isso, poderemos entender: - O que é um Processo de Decisão de Markov (parte 1); - Funções de Estado-Valor (parte 2); - Funções de Valor Ótimas (parte 2); - Equações de Bellman (parte 2). Então mãos à obra! Tome o tempo necessário para ter uma visão clara de cada um dos conceitos pois eles dependem uns dos outros. Antes de começar, devemos revisar os seguintes tópicos abordados em nosso post anterior: Agente: Entidade que tomará decisões no ambiente, interagindo com ele tomando determinadas ações e recebendo as recompensas correspondentes. Ambiente: É a materialização (ou simulação) do problema a ser resolvido. Podendo ser real ou virtual, este será o espaço no qual o agente realizará suas ações. Estado (s): É como o sistema, agente e ambiente, se encontra em um determinado instante. Sempre que o agente realiza uma ação, o ambiente fornece um novo estado e uma recompensa correspondente. A Propriedade de Markov enuncia o seguinte: “Dado o presente, o futuro independe do passado.” Bem filosófico não é mesmo? Vamos meditar um pouco sobre isso. Basicamente, um estado seguirá essa propriedade caso todos os estados anteriores a ele não influenciarem na decisão do próximo estado, apenas o estado atual. Não são todos os estados que obedecem a Propriedade de Markov, mas caso obedeçam, o problema torna-se mais fácil. Passando essa relação ao matematiquês obtemos a seguinte expressão: Esse bando de letras significa algo muito simples, mas primeiro precisamos explicar o que esses símbolos significam. P(A | B, C) significa “probabilidade de A ocorrer dado que B e C ocorreram”. A probabilidade de eu passar para o estado St+1 dado que eu estou no estado St (parte esquerda da equação) é igual a probabilidade de eu passar para o estado St+1 dado todos estados em que estive em meu histórico. Significando que todos os estados antes do atual não são importantes para a passagem de um estado novo. Ainda tá difícil de entender? Segue o exemplo usado no post anterior para ver se clarifica o significado. Se o meu estado atual é correr, não importa quantas vezes eu já dormi ou se antes eu estava tomando sorvete, a probabilidade de eu ir tomar sorvete ainda é 0.3, a de dormir é 0.1 e a de continuar correndo é de 0.6 . Todas essas probabilidades de mudança de estado podem ser colocadas em uma matriz chamada de Matriz de Probabilidade de Transição, com seu modelo geral dado abaixo. As linhas da matriz representam o estado atual, com os seus elementos sendo as probabilidades de mudar para os respectivos estados, tendo eles que somar 1. Vale notar que estas mudanças de estado registradas pela matriz acontecem naturalmente e não são decisões do agente sendo dessa forma também parte do ambiente. A matriz criada para o exemplo anterior, com os estados Dormir, Sorvete e Correr sendo os estados 1, 2 e 3, respectivamente, seria a seguinte: Consiste em um espaço de estados S associado a uma função de transição P onde todos os estados seguem a propriedade de Markov. No exemplo anterior, S seriam os três estados possíveis e P a sua matriz de probabilidade de transição. Com processos de Markov (e as variações que veremos abaixo) podemos simular nosso ambiente em um problema de aprendizado por reforço. Ambientes Completamente Observáveis: são ambientes em que todas as informações estão disponíveis para o agente. Ex: jogos de informação perfeita como Xadrez ou Go. Ambientes Parcialmente Observáveis: nem todas as informações sobre o ambiente estarão disponíveis para o agente. Exemplo: jogos de informação imperfeita como jogos de cartas. Tarefas episódicas: São aqueles que chegam ao fim alcançando um estado terminal. Podemos dizer que eles tem estados finitos. Exemplo: Uma partida de Go. Tarefas contínuas: São tarefas infinitas, sem um estado terminal. Exemplo: simular o comportamento Humano… Para se definir Retorno deve-se primeiro definir Recompensa (R), um certo valor que é dado para cada estado para indicar se ele é desejável ou não, como por exemplo, correr pode ter uma recompensa positiva e tomar sorvete pode ter uma recompensa negativa. No Aprendizado por Reforço, a métrica que devemos maximizar trata-se da soma cumulativa de todas as recompensas que o agente recebe e não apenas a recompensa imediata do estado atual. A essa soma, damos o nome de Retorno (Gt). Podemos comparar a relação de recompensa e retorno com pontos em videogames. Dessa forma, toda vez que recebemos pontos é como se estivéssemos recebendo recompensas e nossa pontuação final seria nosso retorno. Exemplo: Considere o seguinte Processo de Recompensa de Markov (que explicaremos do que se trata mais a frente): Se percorrermos os estados da seguinte forma partindo de “Class 2” : “Class 2” -> ”Class 3” -> “Pass” -> “Sleep”. Teremos acumulado as recompensas gerando o seguinte retorno: Gt = -2 + -2 + 10 + 0 = 6. Para tarefas episódicas, o retorno é fácil de ser calculado, pois será a soma de todas as recompensas obtidas pelo agente. Mas para tarefas contínuas, como a atividade não tem fim e não podemos somar até o infinito, há a necessidade da inserção de um fator de desconto (γ). O fator de desconto é um hiperparâmetro (definido pelo programador) e consiste em um número entre 0 e 1 que define a importância das recompensas futuras em relação à atual. Valores mais próximos ao 0 dão mais importância a recompensas imediatas enquanto os mais próximos de 1 tentarão manter a importância de recompensas futuras. Na prática, um algoritmo com fator de desconto 0 nunca aprenderá uma vez que irá considerar apenas as recompensas imediatas e com 1 continuará somando recompensas futuras até o infinito. Portanto, um valor ótimo de um fator de desconto está entre 0.2 e 0.8. Utilizando o fator de desconto, podemos escrever o retorno da seguinte forma: Exemplo: Utilizando a mesma sequência do exemplo anterior com um fator de desconto de 0.5, temos: Gt = -2 + 0.5*-2 + 0.25*10 + 0.125*0 = -0.5. A função de valor basicamente determina o quão bom é estar em um estado específico, para o agente. A função de valor V(s) é o retorno esperado iniciando no estado s e seguindo a política pelos próximos estados até encontrar um estado terminal (e o valor da recompensa deste será 0). A função (também chamada de função de estado-valor) é definida por: (este E em probabilidade simboliza esperança) Esperança : É a média ponderada de um resultado utilizando a probabilidade de ocorrência como peso. Ex: Em uma partida de rpg, ao atacar um goblin, executamos uma rolagem de dados onde há 10% de chance de se causar 10 pontos de dano, 70% de chance de causar 5 pontos e 20% de chance de errarmos o ataque. Dessa forma, a esperança é de 0,1*10 + 0,7*5 + 0,2*0 = 4,5 de dano. Portanto, para calcular a função valor para um estado (s), devemos calcular a média ponderada pelas probabilidades das funções de retorno partindo de s até o estado terminal onde a recompensa terá o valor de 0. Bootstrapping (acelerando com um ponto de partida) Como calcular a média de todos os retornos é inviável para atualizar a função de valor, podemos acelerar a programação linear (BLP), utilizamos o valor do próximo estado para estimar o valor do estado atual. Programação linear: Em matemática, problemas de Programação Linear são problemas de optimização nos quais a função objetivo e as restrições são todas lineares. Será visto mais a frente, que esta estimativa é possível e converge pois conforme progredimos na Iteração de Valor, a magnitude do erro de Bellman (a diferença máxima entre dois Valores de dois estados consecutivos) diminui. Mas não é necessário se preocupar com isso por enquanto… Juntando todos os conceitos abordados até aqui, podemos definir o Processo de Recompensa de Markov, que é basicamente um Processo de Markov onde adicionamos também a função de recompensa que nos dá a próxima recompensa esperada para cada estado e o fator de desconto (S, P, R, γ). Com todas as informações armazenadas por um MRP, é possível calcular a função de valor para cada estado. Se utilizarmos operações com matrizes para o cálculo, obtemos: Dessa forma, podemos programar essas operações e acelerá-las utilizando bibliotecas como o numpy. Não abordaremos operações matriciais, mas é fortemente recomendado o estudo. Certo, então sabemos como calcular o valor de todos os estados e temos todos os dados modelados, mas ainda não sabemos quais são as decisões corretas a serem tomadas e como treinamos nosso algoritmo. Finalmente, o Processo de Decisão de Markov trata-se de um Processo de Recompensa de Markov com as ações que podem ser tomadas (S, A, P, R, γ), onde A é o espaço de todas ações possíveis em todos os estados existentes. Para um Processo de Decisão de Markov, além da função de valor V(s), há também o retorno esperado da tomada de uma ação, chamado de Q valor ou função de valor da ação. Basicamente é uma função de valor, mas para uma ação. Ela determina a esperança da recompensa que uma ação tomada pode devolver. Lê-se “Retorno esperado (ou simplesmente valor) da ação ‘a’ no estado ‘s’ é igual à recompensa da ação ‘a’ no estado ‘s’ mais o valor descontado do estado seguinte” Muito obrigado por acompanharem até aqui! A área de Aprendizado por Reforço não é tão simples de se entender, demanda a compreensão da modelagem utilizando probabilidade, algo não muito intuitivo para alguns, mas espero que tenham gostado. Em nosso próximo texto da série Aprendizado por reforço abordaremos em mais detalhes o Processo de Decisão de Markov. E como sempre, para ficar por dentro de Inteligência Artificial basta seguir o Grupo Turing nas redes: Medium, Facebook, Instagram e LinkedIn. Força! E até a próxima!!"
https://medium.com/turing-talks/aprendizado-por-refor%C3%A7o-3-processo-de-decis%C3%A3o-de-markov-parte-2-15fe4e2a4950?source=collection_home---------81----------------------------,Aprendizado por Reforço #3 — Processo de Decisão de Markov (Parte 2),Decisões de Markov e as equações de Bellman,Guazco,650,11,2020-03-15,"Bem vindos a mais uma edição dos Turing Talks! Hoje damos continuidade aos posts relacionados a Reinforcement Learning. Veremos agora como encontrar o comportamento do agente que faz ele ser melhor recompensado no ambiente em que ele foi colocado, ou seja, melhor resolve o processo de decisão de Markov. Vale muito a pena dar uma olhada no Turing Talks próprio de decisão de Markov, apesar de não ser totalmente necessário uma vez que alguns conceitos importantes serão revisados no início do texto. Atenção! O texto de hoje se assemelha em muito a uma aula, porém não há necessidade de se desesperar de antemão, nada aqui será demasiadamente complicado a ponto do leitor ter que fazer uma lista de exercícios. Apesar disso, talvez seja necessário ler mais de uma vez um conceito ou parar para analisar uma equação, mesmo assim vos digo que não se desesperem, é normal ao lidar com esse assunto ter que fazer uma segunda análise. Chegamos oficialmente à parte mais matemática e cheia de variáveis parecidas entre si de RL, as Equações de Bellman, nomeadas em homenagem a Richard E. Bellman, um matemático americano que introduziu o conceito de programação dinâmica, a base de aprendizado por reforço. Programação dinâmica é uma técnica de resolução de problemas complexos, na qual dividimos o problema em simples subproblemas e para cada um deles nós computamos e guardamos o resultado, caso o mesmo subproblema apareça novamente, nós não iremos refazê-lo, mas sim usa a solução já encontrada. A solução desses subproblemas no nosso caso são as tais equações. O objetivo de RL é a determinar a melhor política a ser adotada, isto é, a que maximize o retorno, aqui significando o total e não aquele que vocês verão como o recebido por uma única ação. Para resolver esse problema, introduzido em programação dinâmica por Bellman ele desenvolveu as chamadas Equações de Bellman, que vocês verão em detalhes durante esse post mas são, basicamente, modos de dizer o valor dos estados e ações do agente, afim de poder comparar qual caminho seria melhor que outro. Antes de entrarmos nos pormenores das equações devemos explicar seus componentes para evitar a confusão inicial de quem não tem muito contato com as notações matemáticas envolvidas. Para mais informações e um post sensacional acesse o Turing Talks de Probabilidade para Machine learning : Fundamentos de Probabilidade para Machine Learning O Processo de Decisão de Markov trata-se de um Processo de Recompensa de Markov com as ações que podem ser tomadas (S, A, P, R, γ), onde A é o espaço de todas as ações possíveis em todos os estados existentes. Considerando que o intuito do Processo de Recompensa de Markov é o cálculo das função de valor em todos os estados, podemos analogamente dizer que o Processo de Decisão de Markov serve para encontrarmos a Função de Política ótima. Tal política é estacionária, ou seja, não muda com o tempo. Mas pode ser tanto estocástica (que segue uma distribuição probabilística específica) quanto determinística (o comportamento ótimo ser bem definido). É preferível que se mantenha um comportamento estocástico para que o algoritmo possa explorar mais e mais o ambiente em vez de se fixar a uma única estratégia. Para encontrar a política ótima a ser seguida resolvendo o Processo de Decisão de Markov, utilizaremos as equações de Bellman. Relacionadas ao processo de decisão de Markov há ainda as equações que determinam as funções valor melhores descritas nos tópicos abaixo. Um valor quantitativo cujo valor depende de resultados aleatórios/desconhecidos. Como por exemplo um lançamento de dados. O resultado de um lançamento de dados pode dar um número inteiro de 1 a 6, porém os fatores que determinam o valor dessa variável no fim são aleatórios. São esse tipo de variável as da esperança. Esperança dentro desse contexto significa o valor esperado a ser recebido, os valores que serão obtidos estarão no entorno desse valor. Por exemplo, se jogarmos uma moeda comum duas vezes existirão 3 possibilidades, obter nenhuma cara, 1 cara ou 2 caras. Então qual seria o resultado esperado ao jogar? Bem, caso decidíssemos repetir esse experimento muitas vezes a média dos valores tenderá para 1, indicando que a esperança, o valor esperado, corresponde a 1. Não acredita em mim? Sem problemas, temos uma demonstração mais bonita mais a baixo. Agora, a definição matemática de esperança é a soma dos produtos dos valores possíveis pela probabilidade deles serem obtidos. Isso parece bem complicado mas quando vocês pararem para pensar um pouco sobre essa definição verão que ela é simplesmente uma média. Depois dessa definição, um exemplo simples serve muito bem para esclarecer tudo. Consideremos o mesmo lançamento de uma moeda duas vezes, esquecendo o argumento dos vários lançamentos. Qual o valor médio que deveríamos obter ? Para tal, vamos ver as probabilidades de cada caso, ter 1 cara, 2 ou nenhuma. Por tanto, o valor esperado de caras ao se jogar uma moeda duas vezes é 1, sua esperança é 1. Sempre que aparecer a notação ‘E’ indicando esperança pode-se ler ‘valor esperado para tal variável’, essa interpretação facilita em muito o entendimento das equações. Políticas são padrões de ação tomados pelo agente. Em outros termos, são funções que, quando providas do estado atual e uma ação escolhida, retornam a probabilidade de se tomar aquela ação. Podem ser definidas matematicamente como uma distribuição de probabilidades das ações a serem tomadas para todos os estados, e é definida por: Lê-se “probabilidade de se tomar a ação ‘a’ estando em um estado ‘S’”. Funções Valor descrevem o valor de um estado ou ação dentro de uma política específica e servem para determinar se essa política é a melhor a ser escolhida. São utilizadas pelas próprias políticas para determinar a próxima ação, tendo seus valores atualizados a cada recompensa retornada. Por exemplo, se a sua política sempre escolhe ir pro estado de maior valor ou tomar a ação de maior valor ela é chamada de política gananciosa ou ‘greedy’ e caso a escolha dessa política resulte numa recompensa negativa o valor do estado ou ação utilizado é atualizado com sua diminuição. Elas podem ser divididas em dois tipos, funções valor de estado e de ação. O primeiro retorna o valor do estado recebendo em qual estado está e o segundo recebendo o estado e a ação. A função de estado-valor ótima (V*) é a maior função estado-valor que se pode obter em um estado considerando todas as políticas. Define a recompensa máxima que se pode extrair do sistema a partir do estado ‘s’. A função de ação-valor ótima (Q*) é a maior função ação-valor que se pode obter em um estado considerando todas as políticas. Define a recompensa máxima que se pode obter a partir de um estado ‘s’ tomando uma ação ‘a’. Podemos deduzir as Equações de Esperança de Bellman a partir das de valor e esperança já apresentadas previamente. Modificando levemente as funções valor: Nessa nova forma a função de estado utiliza o conceito de esperança como somatório como apresentado na explicação de esperança. Já a função de ação muda de forma que exige uma breve explicação sobre seu significado. Como ressaltado na imagem o ‘R’ representa a recompensa fornecida por aquela ação e o somatório é a esperança dos estados possíveis, o valor esperado do próximo estado. Multiplicando a esperança do próximo estado há o fator de desconto representado pela letra grega gama (γ), ele serve, em simples termos, para determinar o quanto valorizamos os resultados futuros, seu valor pode variar entre 0 e 1. Se o agente usa gama igual a 0 é por que só valoriza as recompensas imediatas sem considerar as direções futuras e se usar 1 é por que dá total valor ao estado futuro. Agora, finalmente, juntando essas duas equações obtemos as Equações de Esperança de Bellman: Sem pressa nesse momento, pode pegar uns segundos para substituir uma equação na outra e chegar nas equações. Lembrando que o objetivo de RL é achar a política ótima (representada pelo símbolo “*”), o comportamento do agente no qual ele maximiza seus ganhos, podemos reescrever as funções valor nessas condições como: A política que sempre toma a melhor ação possível é em simples termos um agente que conhece as melhores decisões a se tomar e só toma elas, dispensando as outras um vez que já sabe que seu resultado não é tão satisfatório. Isso se traduz nas equações como a probabilidade de tomar a decisão ótima sendo igual a 1 e a probabilidade de tomar qualquer outra é 0. Considerando isso as equações se reduzem a: Não bastando as prévias deduções veremos também como percorrer esse caminho de uma forma mais visual, tanto para os leitores que tem mais facilidade quando há visualização quanto para quem já conhece o assunto ter uma abordagem diferente da usual. Faremos isso por meio de diagramas de backup. Diagramas de backup são representação gráficas de algoritmos com estados, transições, recompensas etc. Eles servem inclusive para representar Processos de Decisão de Markov (MDP) e facilitar seu entendimento e dedução das Equações de Bellman. Como pode ver pelo diagrama as vezes uma ação pode levar a diversos estados dependendo da probabilidade desses. Consideremos o caso de um robô que coleta lixo que possui 2 estados: E 3 ações: A consequência de não fazer nada é ter -1 de recompensa, ao coletar lixo sua recompensa é maior, porém gasta bateria. Caso sua bateria já esteja baixa existe a possibilidade dele ficar sem bateria, o que levaria a uma recompensa de -3 e ser recarregado manualmente. Isso pode ser modelado num diagrama de backup da seguinte forma: Se a política do robô for π(s,a) então a probabilidade de chegar no nó A é P(A) mostrado abaixo e analogamente P(B) e P(C) são: A recompensa da ação procurar é +3, dessa forma as trajetórias (sequências de ações e estados que passam por A) tem valor V(A) da figura abaixo e analogamente V(B) e V(C) são: E a partir dessas equações é possível escrever o V(W), o valor de W. Portanto, dado um estado inicial W é possível calcular as trajetórias possíveis a partir desse. O valor de W é então a média ponderada dos valores calculados com os pesos sendo as probabilidades das trajetórias. A ideia aqui contemplada é que as Equações de Bellman relacionam o valor de um estado com o valor de estados adjacentes. Analisemos o diagrama abaixo. Para calcular o valor de s devemos passar por todas as trajetórias a partir de s. Para isso, devemos passar por todos os valores de a e s’ possíveis. O valor de uma dessas trajetórias é r + γV(s’) e a sua probabilidade é π(a|s)P(a,ss’). Portanto, Considerando que: E substituindo na equação anterior a ela obtemos a Equação de Bellman de Esperança para V(s): Similarmente, para Q(s,a), temos o seguinte diagrama Observe que todas as trajetórias têm inicialmente uma recompensa média dada por R abaixo. Logo, a equação obtida passando por todas as trajetórias (todas as combinações de s’ e a’) é: E rearranjando os termos obtemos a equação abaixo dela, a Equação de Bellman de Esperança para Q(s,a). Até agora, vimos equações que lidam com o valor de uma política arbitrária π. No entanto, em geral, estamos interessados em uma política particular, a política ótima, essa que toma sempre as melhores ações, ou seja, as ações que maximizam Q(s,a). Associados a essa política, temos as funções de valor ótimas: Podemos obter as equações de Bellman para V ótimo e Q ótimo por meio de uma pequena modificação nos diagramas utilizamos acima. Para V ótimo, temos: Os arcos indicam que estamos tomando a melhor ação, ou seja, que a política é ótima. Assim sendo, a equação resultante é quase a mesma que a de V(s), mas tomamos a melhor ação ao invés de tomar a média sobre todas as ações: Para Q(s,a) ótimo, o diagrama fica: A recompensa obtida inicialmente (com média R abaixo) continua igual. No entanto, a partir dos estados s’, precisamos escolher a melhor ação a’ ao invés de tomar a média sobre a’ pertencente a A. Logo: Essa é a Equação de Optimalidade de Bellman. Chegamos ao fim de mais um Turing Talks. Parabéns por chegar até aqui! Nós sabemos que esse conteúdo é bem denso, principalmente para quem está no seu terceiro post de RL, não tenha vergonha de voltar aqui as vezes para dar uma revisada. Para resolução de problemas aqui vistos em escala maior vocês verão conteúdos como value iteration, policy iteration e Q-learning, tudo isso sem suar uma vez que vocês já passaram pela pior parte que é entender todo esse matematiquês. Se estiver interessado em mais conteúdo de IA, siga nossa página do Medium , Instagram e do Facebook, e acompanhe nossas postagens! Até mais, queridos leitores, continuem nesse caminho e acompanhando os lindíssimo Turing Talks, novos sobre aprendizado por reforço continuarão vindo."
https://medium.com/turing-talks/aprendizado-por-refor%C3%A7o-4-gym-d18ac1280628?source=collection_home---------80----------------------------,Aprendizado por Reforço #4— Gym,A caixa de areia da Inteligência Artificial,Enzo Cardeal Neves,858,10,2020-03-22,"Bem-vindos à mais uma edição do Turing Talks! Nessa semana abordaremos a primeira ferramenta que irá auxiliá-lo a montar um modelo de Reinforcement Learning. Um dos empecilhos iniciais para se aplicar ou desenvolver um algoritmo de RL é a necessidade de um ambiente (virtual ou real) para a atuação do agente. A tarefa de construir esse espaço de trabalho é dispendiosa, muitas vezes sendo a etapa mais difícil do projeto, o que pode acabar desmotivando entusiastas a aprender mais a fundo sobre o tema. A biblioteca Gym se propõem a sanar exatamente esse problema, fornecendo vários ambientes diferentes para o treinamento de um agente. A OpenAI é uma instituição sem fins lucrativos, fundada em 2015 pelo Elon Musk, Sam Altman e outros investidores, que tem como missão pesquisar e desenvolver novas tecnologias na área de IA e disponibilizar esses novos conhecimentos de forma gratuita para o uso de todos. Um de seus primeiros projetos foi a criação da Gym, uma biblioteca de Python feita para encurtar o caminho entre a ideia de algum modelo de Aprendizado por Reforço e a sua implementação. Ela fornece diversos ambientes diferentes para você testar um agente, sendo necessário apenas o desenvolvimento da lógica do algoritmo em questão. Imagine um boneco de ação articulado em vários pontos diferentes. Essas articulações permitem uma infinidade de posições para o brinquedo. A criança que brinca com ele é quem irá ditar os estados atingidos. O boneco, portanto, apresenta um comportamento diferente para cada brincadeira da criança. Podemos então, estabelecer uma analogia entre o boneco e a Gym e entre a criança e o algoritmo de RL que se deseja implementar. Além de ser possível adicionar ambientes feitos por terceiros (inclusive feito por você!), os oferecidos nativamente são divididos em categorias, sendo as principais: Atari, controle clássico, MuJoCo, robóticos , jogos de texto e Box2D. Vamos ver a seguir um exemplo de cada uma delas. Jogos clássicos convertidos para um formato que é possível treinar o agente. Problemas clássicos de mecânica que estão presentes na literatura de RL há bastante tempo. Uma das vantagens de usar Gym para esses problemas é que, como há uma consistência do meio utilizado, fica mais fácil comparar os resultados de estudos diferentes desenvolvidos no mesmo ambiente. No exemplo a seguir, tem-se um veículo com um motor que não é forte o suficiente para subir o morro maior sem um auxílio externo. O agente deve, portanto, aprender a aumentar sua energia potencial subindo o morro menor para assim ter energia mecânica suficiente para subir o morro maior. Ambientes feitos com base no simulador de física bidimensional Box2D. As tarefas em questão são contínuas, ou seja, não possuem um fim bem definido. No exemplo a seguir, ensinamos um robô bípede a andar. baseados em uma engine física com foco em simulações de biomecânica, articulações, gráficas e animação. Aqui também temos tarefas contínuas. No exemplo a seguir, temos um ambiente tridimensional no qual ensinamos um robô de 4 patas a andar. Ambiente que simula membros robóticos (uma mão ou uma garra) motivados por tarefas com metas bem definidas. No exemplo a seguir, uma mão robô manipula um bloco de madeira. Jogos simples com gráficos feito a partir de texto apenas. Aqui temos o jogo do táxi. O objetivo é recolher um passageiro num determinado ponto e deixá-lo em outro. Caso o passageiro for deixado em um local errado, o jogador é penalizado. Para nosso exemplo prático, vamos ensiná-lo a aplicar Q-learning no jogo do táxi. Caso você não saiba do que estamos falando, recomendamos o post anterior que introduz o tópico: Escolhemos esse algoritmo pois, além de já ter sido explicado anteriormente, é o mais básico de RL, o que facilitará o entendimento e a aplicação. Utilizaremos o jogo do táxi pois conseguimos mapear todos os estados possíveis, característica essencial para a aplicação de Q-learning. “Há 4 localizações (nomeadas por letras diferentes), e nosso trabalho é pegar um passageiro em um local e deixá-lo em outro. Nós recebemos +20 pontos por destino correto e -1 ponto para cada passo tomado. Há também -10 pontos de penalidade para tentativas de embarque e desembarque erradas.” Definimos aqui o universo de todos os estados (situação atual e imediata do jogo) possíveis do agente no ambiente. Como foi dito anteriormente, esse número é finito, então vamos calculá-lo. O tabuleiro é uma malha 5x5, o que permite 25 posições diferentes para o veículo. Há 4 locais (R, G, B e Y) de embarque e ainda o próprio carro como localização disponível para o passageiro habitar, o que contabiliza 5 lugares possíveis. Para finalizar a viagem, há 4 destinos diferentes (R, G, B e Y), portanto, vem: 5 x 5 x 5 x 4 = 500 estados possíveis! Podemos representar qualquer sequência de eventos imaginável com esses 500 estados. Surpreso com o resultado? As ações que podem ser tomadas em cada estado são: Note que sempre que o carro tentar atravessar uma parede o agente receberá uma penalidade de -1 e continuará no mesmo estado que estava anteriormente. Para começar nosso projeto vamos precisar de um ambiente de trabalho. Recomendamos o Jupyter Notebook, mas você pode usar o de sua preferência. Posteriormente, fazemos as importações necessárias. Pronto, temos nosso espaço de trabalho configurado e pronto para uso. Mas antes de continuarmos, vamos entender melhor alguns dos métodos e objetos principais da Gym. A primeira linha do código carrega o ambiente que vamos usar, no nosso caso “Taxi-v3”. Ele fica salvo na variável env. Com o método .render(), renderizamos o estado atual do ambiente. Outros métodos também muito importantes são: * observation: observações do ambiente. * reward: recompensa recebida após a ação tomada. * done: indica se foi o fim de um episódio. Acontece quando um passageiro embarca no táxi e desembarca no local correto. * info: informação adicional para debbuging. O agente não tem acesso a elas. Podemos também conferir as dimensões do espaço de ações e espaço de estados, como segue: O que condiz com o que calculamos anteriormente. Vamos agora recordar das recompensas para cada ação. Elas já estão pré-definidas na biblioteca da seguinte maneira: A recompensa de -1 é importante pois garante que o carro evite paredes (caso fique tentando entrar em uma, irá receber -1 indefinidamente) e que o agente busque se otimizar de forma a performar o menor caminho possível até o ponto de desembarque. Como dito antes, caso não saiba o que é Q-learning, leia o nosso post anterior. Precisamos determinar os q-values para cada tupla (estado, ação), que irão orientar a política do nosso agente, ou seja 500 x 6 = 3.000 valores. A partir da equação de Bellman, determinamos os q-values como segue: O que estamos fazendo é atualizar o q-value da ação tomada no estado atual. Tomamos o valor antigo com um peso de (1- α) e adicionamos o novo valor aprendido, que é a combinação da recompensa por realizar a ação tomada no estado atual, com recompensa máxima, descontada (γ), do próximo estado que estaremos assim que tomarmos a ação. Essa política fará com que o agente tome a rota que fornecerá a maior soma de recompensas possível. Para guardar esses valores vamos criar uma tabela Estados x Ações: Iniciamos a tabela preenchida com zeros e a cada iteração atualizamos os valores. Passo-a-passo, fica: Implementação em código Vamos então testar o nosso agente. Será que ele atingiu uma política ótima? Dos 100 episódios testados, nosso agente não recebeu nenhuma penalidade, o que é um ótimo parâmetro para medirmos o sucesso de seu treinamento. Em relação a média de passos, também obtemos um bom valor (metade das casas do tabuleiro percorridas) tendo em vista a existência dos obstáculos. Neste post apresentamos a Gym, uma ferramenta poderosa que facilita muito a vida de quem quer iniciar os estudos em Reinforcement Learning. Apresentamos aqui seus diferentes tipos de ambientes e suas principais funcionalidades fazendo a aplicação em um exemplo prático. Além de ser essa ótima ferramenta para iniciantes, ela também é de grande valor no meio acadêmico. Isso porque, ao normalizar o espaço de trabalho que as pesquisas são desenvolvidas, ou seja estudos diferentes desenvolvidos no mesmo ambiente, torna-se mais fácil e de maior confiabilidade se comparar os resultados obtidos. Trabalhamos para implementar o Q-learning, um dos algoritmos mais simples de RL, ideal para os propósitos didáticos do post. Sua aplicação se limita a cenários com um número de estados finito e suficientemente pequeno. Caso esse valor se torne muito grande ou indefinido, devemos procurar outros métodos mais complexos de implementação de RL. Atualmente, para resolver os problemas mais abstratos costuma-se usar as Deep Neural Networks que recebem na camada de entradas informações do estado e ações para poder aproximar um valor bom o suficiente para o q-value, mesmo sem levar em consideração todos os estados posteriores possíveis (até porque podem existir infinitos), aprendendo a retornar as ações corretas após treinamento suficiente. Esse é o tipo de abordagem que se usa para treinar braços robóticos, jogos sem finais definidos como Flappy Bird, decidir preços praticados por uma empresa de acordo com as flutuações do mercado, gerenciamento de estoques ou realizar previsões no mercado financeiro. Note, portanto, que apesar da maior complexidade, a ideia central é a mesma apresentada em Q-learning. Por fim, vamos voltar nossa atenção aos hiperparâmetros(α, γ e ε). Para decidi-los, tomamos uma abordagem de tentativa e erro. O ideal seria os ajustarmos de forma a diminuí-los com o passar do tempo, porém, por termos um espaço de estados tão pequeno, conseguimos atingir uma política ótima com valores estáticos. A justificativa para a diminuição de cada um é: Para os aficionados em RL que chegaram até aqui, nossos mais profundos agradecimentos. Não se esqueçam de conferir nossas redes: Medium, Facebook, Instagram e LinkedIn. Caso tenha interesse, você pode encontrar o notebook completo do post aqui. Até a próxima!"
https://medium.com/turing-talks/aprendizado-por-refor%C3%A7o-5-programa%C3%A7%C3%A3o-din%C3%A2mica-8db4db386b67?source=collection_home---------79----------------------------,Aprendizado por Reforço #5 — Programação Dinâmica,Crie um agente que aprende sozinho a navegar um ambiente.,Stephanie Miho Urashima,673,7,2020-03-29,"Escrito por: Stephanie Urashima e Bernardo Coutinho. Na edição dessa semana do Turing Talks, vamos dar continuidade à nossa série de Aprendizado por Reforço! Se você ainda não leu os textos anteriores dessa série, deixamos aqui o link para caso você tenha interesse. Caso contrário, vamos direto ao assunto! Nessa publicação, abordaremos o conceito de Programação Dinâmica, trataremos de suas aplicações no Aprendizado por Reforço e, por fim, apresentaremos como criar seu próprio agente que aprenda sozinho a navegar um ambiente. A Programação Dinâmica é um método muito útil de se resolver problemas de computação, que consiste em pegar um problema complexo cuja solução é difícil de ser calculada diretamente, e dividir sua resolução em diversos subp’roblemas mais fáceis de serem resolvidos. Seu uso objetiva diminuir o custo computacional do programa, bem como simplificá-lo. Contudo, esse método só pode ser aplicado quando nosso problema atende a dois requisitos: Esse método é chamado dinâmico pois esses requisitos pressupõem que o problema possui um aspecto sequencial/temporal, já que a solução de cada subproblema depende de um outro anterior. Mas como aplicamos a Programação Dinâmica em um problema? Vamos supor que nosso problema é calcular o n-ésimo número da Sequência de Fibonacci. Essa sequência é caracterizada da seguinte forma: seus dois primeiros valores são iguais a 1, e cada número seguinte é obtido calculando a soma dos dois anteriores. Logo, o início da sequência seria o seguinte: 1 1 2 3 5 8 13 21 Uma maneira bem simples de resolver esse problema é utilizando uma abordagem recursiva: cada vez que quisermos encontrar o n-ésimo termo da sequência, nós chamamos a mesma função para descobrir os dois anteriores para depois somá-los, da seguinte forma: Entretanto, essa é uma abordagem pouco eficaz, já que você está chamando a função várias vezes para um mesmo número da sequência. Por exemplo, quando calculamos o Fibonacci(5), nós chamamos as funções Fibonacci(4) e Fibonacci(3). Mas dentro do Fibonacci(4), estamos novamente chamando o Fibonacci(3), mesmo já tendo calculado ele. Dessa forma, o que podemos fazer é aplicar a Programação Dinâmica na Sequência de Fibonacci, já que é um problema que atende à Subestrutura Ótima, um termo depende de outros termos menores, e atende à Superposição de Problemas, um termo é usado diversas vezes para calcular os termos seguintes. Assim, podemos guardar os termos da sequência em um vetor f, sem precisar calculá-los várias vezes, como no código a seguir: Quando buscamos resolver um problema de Aprendizado por Reforço, nós estamos tentando calcular a Função Valor e a Política ótimas para o nosso ambiente. E, como vimos no último post, calcular a Função Valor envolve resolver a Equação de Esperança de Bellman: Só que as Equações de Bellman atendem os requisitos da programação dinâmica, uma vez que o valor ótimo V(s) de um estado supõe que o valor dos estados seguintes V(s’) seja ótimo (Subestrutura Ótima), e cada vez que calculamos um V(s), nós precisamos calcular os valores de todos os estados seguintes (Superposição de Subproblemas). Dessa forma, nós aplicamos Programação Dinâmica nas Equações de Bellman e guardamos todos os valores em uma tabela V[s], que atualizaremos ao longo do tempo até descobrir o valor ótimo de cada estado! A partir de Bellman derivam-se as seguintes formulas de atualização da função valor a partir de uma politica: E com isso definimos 2 ações possíveis: Atenção: Só é possível garantir a existência e a unicidade da função valor caso o processo tenha um fim (seja episódico) ou o gamma seja menor que 1 (constante que define a importância das recompensas do futuro, ou seja, mesmo que o processo seja infinito o conjunto de ações futuras relevantes são finitas). Com essas duas ações é possível encontrar a politica ótima no processo chamado de Generalized Policy Iteration. Generalized Policy Iteration (GPI) é o termo dado à ideia geral de intercalar processos de Policy Evaluation com processos de Policy Improvement (a ordem e a quantidade de cada um pode variar). Uma parte dos métodos de Aprendizado por Reforço podem ser classificados como GPIs, isto é, têm politicas e funções de valor identificáveis, com uma politica sempre sendo atualizada de acordo com a função valor enquanto esta é atualizada para uma nova politica. Um esquema simples seria o seguinte: A seguir, vamos explicar alguns exemplos específicos de GPI: Policy Iteration e Value Iteration. Para deixar as aplicações mais ilustrativas iremos fazer elas para resolver o ambiente Frozen Lake, do Gym: Nesse ambiente, o nosso agente deve percorrer um lago congelado com vários buracos até chegar em um objetivo. Se ele chegar no espaço final, ele recebe uma recompensa de +1, mas se ele cair em um buraco, ele perde aquela rodada. É a forma mais simples de se aplicar os dois processos discutidos previamente, a ideia é intercalar as duas ações até a política se estabilizar. Abaixo há o código sobre o passo a passo desta técnica para o Frozen Lake: Obs: Essa técnica sempre funcionará em um MDP finito, uma vez que como o próprio nome diz o processo tem ações finitas e estados finitos e portanto há um numero finito de políticas, possibilitando assim encontrar a política ótima em um numero finito de iterações. Ao ler o código acima provavelmente deve ter notado que Policy Evaluation é um processo que pode, dependendo das circunstâncias, exigir muito poder computacional se feita iterativamente, uma vez que só é possível garantir a convergência da função valor no infinito. Portanto, em Value Iteration, em vez de fazer Policy Evaluation até que a função valor estabilize, atualiza-se somente uma vez a função valor com o valor da ação de maior retorno. Para implementar isso foi utilizado a variável valor_ação que guarda os valores de cada ação possível. E no fim a função_valor só recebe o maior valor guardado em valor_ação. Depois de rodar o Value Iteration, nós obtemos a função de valor V para cada estado do Frozen Lake! Obs: A função valor é zero para o estado final, pois o ambiente do Gym dá a recompensa na transição entre os estados, por isso que somente o estado anterior a chegar no objetivo tem valor 1. Agora que obtivemos a Função Valor ótima, nosso agente conseguirá agir de forma a sempre chegar no objetivo, conforme o gif abaixo! Todo o código que utilizamos nesse post pode ser encontrado aqui! A Programação Dinâmica é uma ferramenta essencial para resolver diversos problemas computacionais, e agora você entende como esse conceito se relaciona com a área de Aprendizado por Reforço. Talvez você tenha percebido que para utilizar essa abordagem em problemas de RL é necessário conhecer todas informações sobre o ambiente, as quais nem sempre estarão disponíveis em outras situações, são elas: dinâmica de transição entre os estados, recompensas e/ou ações. O que deixa a aplicação desse conceito limitado a pouquíssimos problemas. Felizmente, existem alternativas aos problemas de falta de informações, as quais serão abordadas em futuros posts aqui no Turing Talks =). Se você estiver mesmo interessado em saber mais sobre, além de continuar nos acompanhando aqui, não se esqueçam de conferir nossas redes: Medium, Facebook, Instagram e LinkedIn e fique ligado nas postagens. Ficamos por aqui, Até a próxima!"
https://medium.com/turing-talks/redes-neurais-autoencoders-com-pytorch-fbce7338e5de?source=collection_home---------78----------------------------,Redes Neurais | Autoencoders com PyTorch,Uma aplicação em compressão de imagens.,Paulo Sestini,857,8,2020-04-05,"Sabemos que redes neurais podem ser utilizadas para diversos fins, como classificação, aprendizagem por reforço, detecção de objetos e pessoas, reconhecimento de fala, entre outros. Porém, existem aplicações bem diferentes do que estamos acostumados e que não são tão conhecidas, hoje falaremos de uma delas: os autoencoders! Um autoencoder é uma rede neural que é treinada para reconstruir, de forma aproximada, os dados que são passados para ela. Por exemplo, se o seu conjunto de dados for constituído por imagens, o autoencoder deve ser capaz de reconstruir cada uma destas de modo que as reconstruções sejam muito semelhantes às originais. A princípio, esta funcionalidade pode não parecer muito útil, mas o verdadeiro poder dos autoencoders reside em como a estrutura da rede neural é criada, que permite gerar representações codificadas dos dados, como veremos a seguir. Um autoencoder pode ser definido como a implementação de duas funções: Função de codificação: Função de decodificação: Onde a variável x é o seu dado de entrada e C e D são suas formas codificada e decodificada, respectivamente. Dessa forma, ao inserir um dado na rede, a saída será uma representação D dada por: Isto é, o trabalho do autoencoder é codificar um certo dado passado para ele e em seguida realizar sua decodificação, reconstruindo o dado original de forma aproximada. Para um autoencoder de imagens, poderíamos ter a seguinte estrutura: A rede neural recebe, de início, uma imagem. Em seguida, esta imagem é passada por camadas da rede, como camadas de convolução, que implementam a função de codificação, extraindo informações importantes da imagem e diminuindo suas dimensões, transformando a imagem de várias formas até que gera, finalmente, uma representação codificada da imagem de tamanho menor do que o tamanho da imagem original. No diagrama apresentado temos uma imagem no padrão RGB, que possui 3 canais (vermelho, verde e azul), sendo transformada em uma imagem menor de 12 canais que, finalmente, é transformada em uma imagem codificada pequena, de 3 canais. Após isto, a representação codificada é passada para outras camadas da rede, como camadas de convolução transposta, que implementam a função de decodificação, aumentando as dimensões da imagem e gerando uma imagem decodificada semelhante à imagem original. A importância do autoencoder vem do fato de que a função de codificação é construída de tal modo que a imagem codificada possui dimensões menores do que as da imagem original. Desse modo, o autoencoder é forçado a extrair somente as informações mais importantes da imagem recebida, de forma a representá-la de modo compacto. A capacidade de extrair informações importantes dos dados faz com que os autoencoders possuam aplicações como: A seguir, veremos uma implementação de autoencoder para compressão de imagens. Iremos realizar a implementação utilizando a biblioteca open source PyTorch, que possibilita grande flexibilidade na criação de redes neurais. A rede será aplicada ao conjunto de imagens CIFAR-10, que possui imagens no padrão RGB de dimensões 32x32, das seguintes classes: Nossa rede neural será compostas pelas seguintes camadas: A função de codificação será implementada pelas seguintes camadas: A função de decodificação será implementada pelas seguintes camadas: Com tudo definido, vamos ao código! Para criarmos uma rede neural em Torch, é necessário criar uma classe para ela seguindo os moldes da biblioteca. Iremos começar importando os módulos necessários: Nós utilizaremos os módulos torch.nn e torch.nn.functional da biblioteca Torch. O primeiro módulo contém as implementações das camadas e o segundo as funções de ativação que serão utilizadas. Em seguida, criamos a classe Autoencoder, herdando a classe nn.Module da biblioteca Torch, necessária para integração de nossa rede com a biblioteca. Também criamos as camadas de codificação e de decodificação da nossa rede, dentro do construtor. As camadas de codificação realizam as seguintes transformações na imagem: Tais transformações ocorrem devido ao tamanho do kernel da convolução e do stride escolhido. Em todas as camadas de convolução o kernel é 2x2. Nas duas primeiras camadas de convolução o stride é 1, já na última o stride é 2. As camadas de decodificação realizam as mesmas transformações em sentindo contrário, transformando a imagem 15x15 de 3 canais em uma de 32x32 de 3 canais. Além disso, toda classe de rede neural em Torch precisa possuir um método forward, responsável por propagar os dados pela rede. Primeiramente, para criação desse método, iremos criar outros dois métodos, o encode e o decode, responsáveis por implementar as funções de codificação e de decodificação, respectivamente. O método encode propaga a imagem através das camadas responsáveis pela codificação, utilizando funções de ativação ReLU, de modo que temos o seguinte caminho na rede: Conv. -> Batch Norm. -> ReLU -> Conv. -> Batch Norm. -> ReLU -> Conv. -> Batch Norm. -> ReLU O método decode propaga a imagem já codificada através das camadas de decodificação, também utilizando ReLU, de forma que temos este caminho: Conv. Transp. -> Batch Norm. -> ReLU -> Conv. Transp. -> Batch Norm. -> ReLU -> Conv. Transp. -> Batch Norm. -> ReLU Com estes dois métodos definidos, o método forward consiste em chamar o método encode na imagem e, em seguida, chamar o método decode na imagem codificada. Isso finaliza a criação do autoencoder, o próximo passo é o treinamento. Novamente, importamos os módulos necessários: Desta vez, o módulo torch.nn será utilizado para a criação da nossa função de custo. O módulo torch.optim disponibilizará o otimizador para treinamento da rede. Já o torchvision nos dará acesso ao dataset CIFAR-10. Em seguida, importamos nosso dataset e instanciamos nosso Autoencoder. Caso uma GPU esteja disponível, a rede a utilizará, caso contrário, a CPU será escolhida. Note que, ao importar o dataset, precisamos aplicar uma transformação nas imagens para que estas sejam armazenadas em objetos do tipo torch.Tensor, que são arrays multidimensionais utilizados nos cálculos dentro da biblioteca. Após isto, definimos a quantidades de treinamentos a serem realizados (epochs), escolhemos o nosso otimizador, que será o Adam, e a nossa função de custo, que será a Mean Squared Error (MSE). Agora vamos para o processo de treinamento! Para cada treinamento, nós mandamos nosso batch para a rede, calculamos nosso erro e utilizamos o método backward da função de custo para cálculo dos gradientes na rede. Com os gradientes calculados, utilizamos o método step do otimizador para atualizar os pesos na rede. É importante zerar os gradientes anteriores antes de calcular os novos gradientes, utilizando o método zero_grad do otimizador. Note que nós também levamos em conta a progressão do treinamento da rede, informando a porcentagem do conjunto de imagens que já foi processada ao longo de cada epoch. Ao final, salvamos a rede treinada em um arquivo para uso posterior. Teste do Autoencoder Com o autoencoder treinado, iremos realizar um teste em uma imagem aleatória do conjunto de imagens de teste. A biblioteca matplotlib será utilizada para visualização das imagens. Iremos importar os módulos necessários e o conjunto de imagens, escolhendo uma imagem aleatória para teste. Depois, exibimos a imagem de teste e carregamos o nosso autoencoder treinado. Após isto, passamos a imagem de teste pela função de codificação e exibimos sua forma codificada. Finalmente, enviamos a imagem codificada para a função de decodificação e mostramos a imagem recuperada. Ao final do código, algumas formatações de exibição são realizadas e a figura final é exibida. Assim, obtemos o seguinte resultado: Vemos que o autoencoder foi capaz de comprimir a imagem e recuperá-la de modo satisfatório! A diminuição das dimensões de 32x32 para 15x15 fazem com que a versão comprimida da imagem ocupe aproximadamente apenas 22% do espaço em memória ocupado pela imagem original! O código completo pode ser encontrado no github: https://github.com/paulosestini/Autoencoder Com o encerramento deste episódio do Turing Talks, esperamos que este artigo tenha servido para demonstrar o poder dos autoencoders e mostrar um pouquinho de como a excelente biblioteca PyTorch funciona. É importante notar que os autoencoders possuem também outras aplicações como as comentadas anteriormente e nós incentivamos que o leitor continue seus estudos! Se você estiver interessado em saber mais sobre, além de continuar nos acompanhando aqui, não se esqueçam de conferir nossas redes: Medium, Facebook, Instagram e LinkedIn e fique ligado nas próximas postagens. Agradecemos a atenção e até a próxima!"
https://medium.com/turing-talks/modelos-de-predi%C3%A7%C3%A3o-otimiza%C3%A7%C3%A3o-de-hiperpar%C3%A2metros-em-python-3436fc55016e?source=collection_home---------77----------------------------,Modelos de Predição | Otimização de Hiperparâmetros em Python,Técnicas de otimização para levar seu modelo preditivo ao próximo nível!,Lucas Miura,1292,7,2020-04-12,"Fala galera, como vocês estão? Hoje, vamos trazer um assunto um pouco diferente do que foi falado até agora sobre modelagem, mas que é essencial para que você possa obter o melhor desempenho do seu modelo - a otimização de hiperparâmetros. O que são hiperparâmetros e como otimizá-los? Por que se preocupar com isso? Em que etapa de um projeto de Ciência de Dados isso entra? Digamos que você esteja fazendo um projeto ponta a ponta de Ciência de Dados. Você passou muito tempo coletando e limpando os dados, depois analisou eles, tirou insights importantes e finalmente começou a modelar com Machine Learning. Olhando para alguma métrica, como acurácia ou R², você viu que seu modelo está razoável - dá para melhorar. Assim, você decide preparar ainda mais os dados para a modelagem, o que dá um up no modelo, mas ainda fica uma sensação de que dava para fazer melhor. E agora?Agora entra a otimização de hiperparâmetros. Parâmetros e hiperparâmetros são duas partes fundamentais de um modelo preditivo. Em um algoritmo machine learning, os parâmetros são ajustados diretamente pelo processo de aprendizado e influenciam diretamente na performance do algoritmo. Os coeficientes de uma regressão linear, os pesos de uma rede neural, as fronteiras das vizinhanças no kNN, todos esses são parâmetros que se ajustam ao treinar o modelo preditivo para um conjunto de dados. Por sua vez, hiperparâmetros são variáveis do algoritmo definidas antes do treinamento. Eles representam características mais construtivas, como a métrica de performance de uma regressão (MSE, R²,…), o número de neurônios de uma rede neural (ou camadas), o número de vizinhos do kNN. Os hiperparâmetros são muito importantes para a performance do modelo preditivo e caso escolhidos de qualquer jeito, podem torná-lo inútil, ou muito longe do ótimo. Do mesmo jeito que os parâmetros de um algoritmo são ajustados de forma a minimizar um erro ou maximizar uma performance, os hiperparâmetros também são. O problema é que testar hiperparâmetros exige que o algoritmo já esteja treinado no conjunto de dados. Assim, para otimizar um hiperparâmetro é preciso treinar um modelo diversas vezes, o que é muito demorado. Mas, isso não significa que não podemos ajustá-los — para isso existem diversos algoritmos que vamos falar a seguir. Vamos ver os algoritmos mais populares de otimização de hiperparâmetros. Para compará-los, vamos à prática utilizando um modelo preditivo e um conjunto de dados. Vamos usar uma Árvore de Decisão, um modelo preditivo que formula comparações e desigualdades sobre os dados de entrada, para predizer uma variável de saída. Um ótimo aspecto desse modelo é que fácil de interpretá-lo, como na imagem a seguir: Um hiperparâmetro muito importante para uma árvore de decisão é sua profundidade máxima, ou seja, o número de perguntas que podem ser feitas em sequência, pois ele ajuda muito a prevenir um problema comum deste modelo que é o sobreajuste, ou overfitting. Em relação aos dados, vamos usar o dataset de câncer de mama, onde devemos classificar se um nódulo é maligno ou benigno com base em características dele, extraídas de uma imagem digitalizada de um tecido mamário (via PAAF - Punção Aspirativa de Agulha Fina). Como neste caso é muito importante detectar os casos de câncer e evitar um alarme falso, precisamos prestar atenção tanto na taxa de positivos verdadeiros quando na de falsos positivos. Assim, a métrica de avaliação que vamos utilizar é o AUC-ROC, ou área sob a curva ROC, que representa a probabilidade do modelo distinguir corretamente a classe positiva da negativa. O melhor caso da AUC-ROC ocorre em 1 e o pior caso ocorre em 0.5 (por quê?). Além disso, vamos repartir o conjunto de dados diversas vezes entre partes de treino e partes de teste para garantir que o modelo performa bem em todo o conjunto - essa técnica é chamada de validação cruzada, ou cross-validation. Vamos usar essa estratégia para todos os algoritmos a seguir, também! Vamos medir o quão bem a árvore de decisão performa sem realizar nenhum ajuste: Aparentemente já conseguimos um bom resultado mesmo sem otimizar o hiperparâmetro. Isso é razoavelmente comum, mas estamos diagnosticando casos de câncer, então cada pontinho vale! Então, vamos às técnicas de otimização de hiperparâmetros: Possivelmente o caso mais ingênuo e mais simples. O Grid Search, ou busca em grade, é um algoritmo de busca que recebe uma conjunto de valores de um ou mais hiperparâmetros e testa todas as combinações dentro dessa vizinhança. O algoritmo tabela qual foi o desempenho de cada configuração e ao final de todos os testes, fala qual é a melhor escolha. Atualmente, existem alternativas muito melhores que o Grid Search, mas vale a pena citá-lo por sua simplicidade. Podemos ver que o modelo selecionou uma árvore de decisão com máximo comprimento 3 e performou melhor que a árvore sem otimização! Também vimos o tempo de execução, por volta de 1.15 s, que vai ser útil para comparar com os outros algoritmos. Este algoritmo é muito parecido com o Grid Search, exceto por um motivo: ao invés de testar todas as combinações na vizinhança, o Random Search, ou busca aleatória, testa combinações aleatórias de hiperparâmetros, conforme um número especificado de amostras a tirar. Ele é uma alternativa para o Grid Search quando o conjunto de dados é muito grande, ou há um número muito grande de hiperparâmetros para otimizar. Para mostrar essas vantagens, vamos aproveitar para otimizar também outros hiperparâmetros da árvore de decisão. Muito bom! Conseguimos aumentar o desempenho do modelo e ainda diminuir o tempo de execução. Para conjuntos de dados maiores, essa melhoria aumenta mais ainda. Porém, ainda existem alternativas mais sofisticadas e mais rápidas que Random Search… Subindo na liga dos algoritmos de otimização, nos deparamos com este método. Bayes Search, ou busca bayesiana, tenta estimar qual é a combinação de hiperparâmetros que resultará na maior performance, com base numa distribuição criada a partir das combinações testadas anteriormente. A grande sacada é forma que esse algoritmo faz as estimativas: ele procura as regiões onde há menor confiança na distribuição levantada e, dentro dessas regiões, qual é a que pode ter um valor mais elevado para a performance. Isso fica mais claro com a gif acima! Assim, a cada iteração da busca bayesiana, a configuração de hiperparâmetros com a maior chance de melhoria é escolhida e a distribuição da performance do modelo é atualizada, permitindo a escolha de um novo ponto. O ganho de desempenho que esse algoritmo proporciona é brutal em relação aos anteriores e é uma das melhores escolhas para datasets gigantes e modelos com muitos hiperparâmetros. Vamos buscar ainda mais hiperparâmetros e ampliar o espaço de busca dos que já definimos: Parece que chegamos no desempenho máximo: esse algoritmo de otimização gerou um modelo tão bom quanto o Random Search. No entanto, notamos uma demora maior de execução — o Bayes Search funciona melhor para treinamentos mais complexos e com espaços de busca muito grandes, então para nosso exemplo, Random Search bastaria. Mas fica o aprendizado para os projetos reais! Nessa aula, vimos a diferença entre parâmetros e hiperparâmetros e comparamos os algoritmos de otimização de hiperparâmetros mais populares: Grid, Random e Bayes Search. Aplicamos eles a um caso prático de predição de câncer com árvores de decisão. Por fim, refletimos um pouco sobre o escopo de aplicação de cada um desses algoritmos. Se quiser ir além, existem algoritmos ainda mais modernos para otimizar hiperparâmetros. E como sempre, para ficar por dentro de Inteligência Artificial basta seguir o Grupo Turing nas nossas redes: Medium, Facebook, Instagram e LinkedIn. Por hoje, é isso. Espero que tenha gostado!"
https://medium.com/turing-talks/como-machine-learning-consegue-diferenciar-heter%C3%B4nimos-de-fernando-pessoa-156d0d52a478?source=collection_home---------76----------------------------,Como Machine Learning consegue diferenciar heterônimos de Fernando Pessoa,O Projeto de Processamento de Linguagem Natural do Grupo Turing,Lucas Sepeda,3305,7,2020-04-19,"Projeto desenvolvido por: Fernando Matsumoto, Iago Nunes, Igor Câmara, Julia Pocciotti e Lucas Sepeda. Bem vindos a mais uma edição do Turing Talks! Hoje falaremos sobre um projeto desenvolvido pela área de Processamento de Linguagem Natural do Grupo Turing: um Classificador de Poemas do Fernando Pessoa. Todo o código para esse projeto (scrappers e modelos) está disponível no GitHub do Grupo Turing. A maioria pensa com a sensibilidade, e eu sinto com o pensamento. Para o homem vulgar, sentir é viver e pensar é saber viver. Para mim, pensar é viver e sentir não é mais que o alimento de pensar. — Bernardo Soares Processamento de Linguagem Natural (PLN), ou, em inglês, Natural Language Processing é uma área da inteligência artificial cujo objetivo é a interpretação e manipulação de linguagens humanas. O estudo de PLN envolve várias habilidades distintas. Alguns conhecimentos básicos de linguística são úteis, sobretudo para as tarefas ligadas a pré-processamento. Além disso, há técnicas clássicas, estatísticas e, por fim, de aprendizado de máquina. Há várias tarefas que são envolvidas por PLN, como análise de sentimentos e tradução automática, por exemplo. Nesse post, veremos mais um pouco sobre uma dessas tarefas, a Classificação de Texto. O projeto nasceu como aplicação de assuntos que estávamos estudando dentro do Grupo; os tópicos em questão eram dois: similaridade semântica e lexical e classificação de textos. Ou seja, juntamos ferramentas de PLN e modelos de classificação para comparar e mensurar o sentido de palavras, tanto isoladamente, quanto em um contexto mais amplo de documentos para então realizar uma tarefa clássica de Aprendizado de Máquina, na qual o modelo deve dividir as entradas em diversas categorias. A ideia surgiu a partir de um artigo discutindo a autoria do Federalist Papers — um conjunto de ensaios publicado por Alexander Hamilton, James Madison e John Jay, sob o pseudônimo Publius. Historiadores discutem a autoria de cada um dos 85 ensaios a partir de características textuais e outras informações pertinentes. No artigo, são utilizadas ferramentas de pré-processamento e clusterização para o problema de atribuição de autoria. A resposta para essa pergunta é bem simples. O poeta português Fernando Pessoa se notabilizou, entre outras coisas, por criar o fenômeno da heteronímia. Diferentemente de pseudônimos, os heterônimos criados por Fernando Pessoa possuíam uma personalidade completa e eram muito diferentes em suas próprias filosofias, e, mais especificamente, em seus estilos de escrita. Fazendo essa simples comparação, é possível notar que, apesar de escritos pelo mesmo autor, os heterônimos Alberto Caeiro e Ricardo Reis diferem muito em sua escrita. Sendo assim, a pergunta que motivou nosso experimento foi: será que, do ponto de vista de um classificador, os heterônimos são mesmo distintos, ou será que possuem características da mesma pessoa por trás deles? O primeiro passo para todo projeto de aprendizado de máquina é a obtenção dos dados. Nessa etapa, utilizamos os milhares de textos do Fernando Pessoa disponíveis no Arquivo Pessoa. Usando o framework Scrapy, extraímos os textos do site e formamos um dataset de textos, o qual está disponível no repositório do projeto. Para cada texto, estão disponíveis também informações como o autor, o título e a data de publicação. No processo de extração dos dados, o primeiro passo é localizar as páginas nas quais estão localizados os textos. Em seguida, passamos por cada uma dessas páginas e extraímos as informações de interesse. Por exemplo, o autor do texto encontra-se em uma tag de classe “autor”. Como o texto é recebido em um arquivo html, é necessário convertê-lo para texto simples. Isso foi feito com ajuda de expressões regulares e da biblioteca html2text. Por último, o Scrapy salva automaticamente essas informações em um arquivo csv. Para trabalharmos com os dados, utilizamos pandas para carregar os dados do csv no python. Além disso, retiramos os autores com menos de 80 textos do DataFrame. Fizemos isso para que o modelo fosse mais robusto e evitar sobreajuste. Também percebemos que existiam no DataFrame textos em inglês. Contornamos esse problema utilizando a função detect da biblioteca langdetect. É importante mencionar que o algoritmo de detecção de linguagem é não determinístico, ou seja, em alguns textos o resultado pode variar. Portanto, optamos por remover esses textos. Com os textos em mão, ainda era preciso processá-los para que ficassem utilizáveis por modelos de classificação. Esse processo começou pela retirada das stopwords utilizando nltk — palavras que, por serem muito comuns, acabam introduzindo ruído no modelo. Depois, lematizamos e stemizamos o que sobrou usando a biblioteca scpaCy. A lematização (ou lemmatization, em inglês) é o processo de extração da raiz das palavras para sua forma dicionarizada. Já stemização (ou stemming, em inglês) é um processo similar, na qual o radical da palavra é extraído. Assim duas conjugações diferentes de um mesmo verbo passam a ser a mesma palavra. Também deixamos todas as letras minúsculas, para fim de normalização. Outro ponto necessário é codificar a coluna que identifica a qual heterônimo pertence um dado poema para ser possível utilizar um algoritmo de classificação. Para essa tarefa, atribuímos números inteiros de 0 a 3 para cada um dos heterônimos que iremos classificar neste modelo utilizando a função LabelEncoder do scikit-learn. Daí, dividimos o DataFrame em teste e treino e concluímos a etapa de pré-processamento. Em seguida, é necessário codificar os textos lematizados para um formato que os modelos de classificação consigam lidar. Para isso, utilizamos uma matriz termo-documento com valores ponderados por tf-idf (term frequency-inverse document frequency, ou frequência do termo-inverso da frequência nos documentos) para todos os heterônimos. A matriz termo-documento é composta pelo vocabulário de todos os poemas nas linhas e pelos poemas nas colunas, e os valores são o número de vezes que um determinado termo ocorre naquele poema (term frequency) dividido pelo número de poemas em que o termo aparece (inverse document frequency). Essa divisão é feita pois termos que aparecem em muitos documentos diferentes são pouco significativas para classificar um texto. Por exemplo, se a palavra “casa” aparecesse em todos os poemas de Fernando Pessoa, ela ajudaria pouco na tarefa de atribuir um poema para um heterônimo do poeta. Agora se a palavra “janela”, por exemplo, ocorre apenas nos poemas de um heterônimo, ela nos ajudará muito na classificação. Para criação da matriz utilizamos o implementação já pronta de tf-idf disponível no scikit-learn. Perceba que treinamos o vetor apenas uma vez em todo o vocabulário disponível e então aplicamos a transformação nos DataFrames de treino e teste. Com a features extraídas e o pré-processamento realizado, podemos, finalmente, partir para a construção do modelo. Agora que temos o DataFrame pronto para receber modelos, precisamos decidir quais iremos aplicar. Nesse post iremos adotar dois modelos clássicos de PLN: Support Vector Machine e Naive Bayes. O Support Vector Machine busca, em pouquíssimas palavras encontrar o hiperplano que divide duas classes. Já o NaiveBayes se baseia em métodos probabilísticos utilizando o famoso teorema de Bayes, nos quais os parâmetros são os documentos e o vetor do tf-idf. Mais uma vez, se quiser se aprofundar nos detalhes desses modelos, basta conferir nossos Turing Talks sobre esses modelos. Tanto para o NaiveBayes quanto para a SVM utilizamos as implementações disponíveis no scikit-learn. Após fazer a validação com o DataFrame de teste, percebemos que o modelo de NaiveBayes obteve uma acurácia de 60.90%, e o modelo de SVM 82.71%. A SVM se mostrou um modelo muito mais robusto nesse caso por conseguir lidar melhor com a diferenças entre os heterônimos. É possível aplicar um algoritmo de redução de dimensionalidade no vetor tf-idf para melhor visualizar a diferença entre os 4 principais heterônimos (Alberto Caeiro, Ricardo Reis, Álvaro de Campos e Bernardo Soares). Nesse post, explicamos um pouco sobre como usamos corpus do Fernando Pessoa para aplicarmos técnicas de Classificação de Texto. Assim como muitas pessoas utilizam a linguagem para manifestar sua criatividade, também podemos utilizar Processamento de Linguagem Natural para explorar aplicações criativas. Como já citado anteriormente, uma aplicação recente de PLN em literatura foi a tarefa de atribuição de autoria dos “Federalist Papers”. Indo um passo além, também existem aplicações de geração de texto com Redes Neurais Recorrentes utilizando obras de escritores conhecidos, como foi feito com o poeta e dramaturgo William Shakespeare. Esperamos que vocês tenham gostado do texto! Se quiserem conhecer um pouco mais sobre o que fazemos no grupo, não deixem de seguir as nossas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Até a próxima!"
https://medium.com/turing-talks/criando-uma-ia-que-aprende-a-jogar-pong-f379b0170017?source=collection_home---------75----------------------------,Criando uma IA que aprende a jogar Pong,Como usar RL e Q-Learning para criar um excelente jogador,Ariel Guerreiro,2019,7,2020-04-26,"Texto escrito por Pedro Sacramento e Ariel Guerreiro. Bem vindos a mais uma edição do Turing Talks! Nesse texto veremos como criar um jogador do famoso jogo Pong, usando o versátil algoritmo de RL Q-Learning. No filme Groundhog day (“O feitiço do tempo”, 1993), o egocêntrico jornalista Phil Connors é obrigado a reviver o mesmo dia várias e várias vezes. Quanto mais vezes o personagem de Bill Murray vive o Dia da Marmota, mais ele aproveita os detalhes a seu favor, deixando de ser egoísta e alcançando no final seu objetivo de conquistar Rita. A maneira em que Phil aprendeu sobre seu ambiente se assemelha a mecanismos de Q-Learning, algoritmo de Reinforcement Learning (RL), onde o agente (Phil) repete o ambiente (Dia da Marmota), testando possibilidades de ações para cada estado e recebendo seu retorno, até encontrar o conjunto de ações que maximiza seu ganho. Neste texto, será apresentado um algoritmo de Q-learning tabular, aplicado ao clássico jogo Pong. Se você quiser uma introdução ao Aprendizado por Reforço, recomendamos ler nossa série de textos sobre RL. Antes de partirmos para a aplicação no jogo, uma explicação mais teórica. Q-Learning é um algoritmo de RL off-policy e com temporal difference. Temporal difference se refere à capacidade do algoritmo de aprender durante os episódios e não somente quando eles acabam. Já off-policy indica que a política usada para gerar o comportamento do agente não está relacionada com a política que estima a recompensa (a que será melhorada). Também é um algoritmo model-free, pois não depende da criação do modelo do ambiente que está sendo aplicado. Q-learning, como Phil Connors, trabalha geralmente dentro de dois loops. O externo é aquele que mantém o agente interagindo com o ambiente repetidas vezes, tal como Phil revive o mesmo dia (chamados em RL de episódios). O interno é aquele que mantém o agente interagindo com o ambiente durante o episódio, assim como Phil vive os acontecimentos da cidade de Punxsutawney que acontecem cada dia. Assim como as experiências de Bill permitem que ele amadureça, nosso modelo aprende e amadurece conforme os episódios passam. O Q (de Qualidade), mede o quão bom é tomar uma ação em um determinado estado. Como pode se ver pela fórmula acima, é calculado como a recompensa recebida logo após tomar a ação, somada à qualidade da próxima ação a ser tomada multiplicada por um fator de desconto gama. O aprendizado ocorre graças à Equação de Bellman, já mencionada em outros Turing Talks, mas que pode ser conferida na imagem abaixo: Nessa equação, existem, além da recompensa e o Q, dois parâmetros que podem ser ajustados para refinar o desempenho do agente: o alpha (α), learning rate, corresponde a quanto os valores são atualizados para cada iteração, e o gama (γ), discount factor, às vezes chamado informalmente de “fator de miopia”. Quando muito baixo, o agente desconsidera as recompensas futuras e, quando muito alto, considera tanto as recompensas atuais quanto as futuras. O método funciona buscando estimar um valor Q(s,a) para o valor verdadeiro de cada par ação-estado encontrado pelo agente. Faz-se isso a partir de atualizações sucessivas dos Q-values segundo a equação de Bellman. Na aplicação tabular do algoritmo, abordada nesse texto, utiliza-se a “Q-table”, uma tabela que guarda os Q-values. Inicialmente, as estimativas para os valores de cada estado-ação estão longe de serem corretas, contudo, após diversas iterações, o algoritmo melhora suas estimativas. Para cada iteração, os Q-values são atualizados, incrementados pela recompensa da ação escolhida nesse estado e por uma parte do quão melhor é avaliada a melhor ação do próximo estado em relação àquela tomada. Agora, podemos aplicar o algoritmo para o jogo em si. O ambiente utilizado não será o Gym, mas sim uma versão construída em cima da biblioteca Pygame. O código para a implementação do jogo e do algoritmo pode ser conferida no link. Porém, a aplicação é baseada no Gym, uma vez criado o ambiente, seu funcionamento é quase igual. Não será utilizado o Pong do Gym porque a forma que os estados são fornecidos são inviáveis para a aplicação do Q-Learning tabular. Se fosse utilizado o “Pong-ram-v0”, que é idêntico à versão do Atari, o número de estados possíveis tornaria essa aplicação impossível. Nessa implementação, o nosso agente possui como estado a distância vertical e horizontal da bola. Como ações, existem 3 disponíveis: não fazer nada, ir para cima ou ir para baixo. No vídeo acima, está o nosso agente (do lado esquerdo) antes de seu treinamento. Nosso agente não sabe como o jogo funciona ou o que deve fazer. O agente se comporta de forma aleatória, rebatendo às vezes a bola por sorte. A seguir, veremos como será o treinamento do modelo e seu resultado: Inicialmente, importamos as bibliotecas necessárias. Pickle é uma biblioteca para exportar objetos python, que será utilizada para salvar a q-table. “objects” é o arquivo que contém o ambiente criado. Iniciamos as variáveis de learning rate, epsilon (e seu valor minimo e decaimento) e o gama. Definimos também o número de episódios e iniciamos um dicionário “Q” e a lista “times”, que grava a duração de casa episódio. Também está presente o código para salvar a q-table no pickle. Definimos então uma função que é a estratégia epsilon-greedy, onde o agente tem uma chance epsilon de tomar uma ação aleatória. Conforme as estimativas vão sendo otimizadas e vão se passando os episódios, o valor de epsilon decai. Para cada step, o valor decai uma certa quantidade até atingir o valor mínimo. A função discretize tem como papel reduzir a possibilidade de estados, para facilitar o treinamento. Por exemplo, estados como [280, 163] e [283, 158] são extremamente próximos e, se forem tratados como iguais, reduzem muito o tamanho da q-table. Com a divisão de cada distância por 10, reduzimos em 100 vezes o tamanho da tabela. Em seguida, definimos uma função de treino, onde os estados ainda não explorados são adicionados a q-table e iniciando seus q-values para as 3 ações possíveis como 0. Também se atualiza os q-values segundo a equação de Bellman. Acima, vemos que o ambiente é criado. Em cada episódio (um dia na vida de Phil Connors), o ambiente é reiniciado e algumas variáveis são atribuídas. Enquanto o episódio não acabar, uma ação vai ser tomada, serão obtidas a recompensa e o próximo estado, levando à atualização da q-table. Um episódio termina quando o agente toma 5 pontos ou chega ao final de 1000000 de passos (cada interação do “while not done”). Ao final do treinamento, temos a q-table final. A seguir, podemos ver o comportamento do modelo treinado: Podemos ver o que o nosso agente (esquerda) aprendeu muito bem como se movimentar, sendo capaz de aprender a rebater a bolinha somente por meio da recompensa negativa ao tomar um ponto e a recompensa positiva de sobreviver por mais tempo. Abaixo, temos um gráfico mostrando a média de recompensa (considerando as 50 anteriores) que o modelo obteve. O modelo só teve um bom desempenho depois de 700 episódios e atingindo a recompensa máxima possível. Q-Learning é um algoritmo de aprendizado extremamente versátil e eficaz, já que não depende de uma modelagem do ambiente, somente de recompensa, para aprender o que deve ser feito. Porém, o Q-Learning tabular, utilizado nesse texto, possui uma grande limitação: à medida que as possibilidades de estados possíveis aumentam, o tamanho da tabela que armazena os estados e recompensas cresce exponencialmente. Para problemas mais complexos, essa implementação se torna inviável. Uma alternativa a uma tabela é a utilização de redes neurais para cálculo da recompensa esperada, chamada Deep Q-Learning, que será apresentada em um texto futuro. Caso queira ver o código utilizado neste texto, veja o repositório no Github. Não deixe de conferir o nossas redes sociais: Facebook, Instagram, LinkedIn, além do próprio Medium. Até a próxima!"
https://medium.com/turing-talks/an%C3%A1lise-de-sentimento-usando-lstm-no-pytorch-d90f001eb9d7?source=collection_home---------74----------------------------,Análise de sentimento usando LSTM no PyTorch,Como redes neurais recorrentes podem ser usadas para analisar texto e descobrir se estão falando bem de você.,Piero Esposito,1329,6,2020-05-03,"Post escrito por Ana Laura Moraes Tamais e Piero Esposito (ordem alfabética) Link para o repositório aqui. Processamento de Linguagem Natural (NLP), quando combinado com Deep Learning, se torna um dos tópicos mais sexys que existem dentre as áreas de estudo da Inteligência Artificial. Com a arquitetura correta, redes neurais podem ser treinadas usando bases de dados de textos (devidamente pré-processadas) e criar abstrações da semântica de línguas humanas, e essas podem ser extremamente úteis para extração de insights e tomada de decisão automatizada. Neste Turing Talks, vamos, primeiro, explicar um pouco da relação entre as Redes Neurais Recorrentes (RNNs — Recurrent Neural Networks) e Processamento de Linguagem natural (NLP). Depois, usando o dataset Amazon Fine Food Reviews (que vamos encodar pra funcionar numa rede neural), vamos treinar uma LSTM, usado PyTorch, para classificar uma análise como positiva ou negativa. Por fim, para conferir se tudo deu certo, vamos ver a acurácia desse modelo numa parte do dataset separada para teste. Podemos definir textos, em termos de dados, como sequências (de palavras ou, num nível de abstração mais baixo, de caracteres). O que importa é que, para podermos extrair o máximo de informação enquanto modelamos esse dataset, precisamos colocar a ordem das sequências no modelo. É aí que entram as redes neurais recorrentes: em tratamento de sequências, elas preservam uma parte do output como um “estado”, de forma a persistir a informação através do tempo: Para a modelagem do nosso dataset, vamos usar uma arquitetura específica de RNN, a LSTM. Nosso dataset é o Amazon Fine Foods Reviews. O arquivo que recebemos é um .csv, no qual usaremos as colunas “Text” e “Score”, que contêm, respectivamente, o texto de uma review e o score, que vai de 1 a 5 estrelas. Para podermos usá-lo com a nossa rede neural, vamos encodar (isso é, transformar) os textos como uma sequência de números, em que cada número corresponde ao índice da palavra dentro do nosso vocabulário, que é uma estrutura de dados do tipo set (conjunto) que contém todas as palavras presentes dentro do dataset. Para podermos treinar uma rede neural com dados de texto, precisamos, de alguma forma, encodar o texto como números. Existem diversas formas de fazê-lo, e, para nossa análise, vamos usar o de text-to-sequences. Na abordagem de text-to-sequences, o encoding do texto ocorre em duas etapas: primeiro, cria-se um vocabulário: ele é uma conjunto (set) que contém todas as palavras presentes em todos os textos, sem repeti-las. Depois disso, criamos um dataset que, no lugar de cada texto, tem vetor de números com o índice de cada palavra no vocabulário. No final, o input para nossa rede neural, para cada texto, fica mais ou menos assim: Para nossa sorte, já existe uma lib, o TorchNLP, que cuida disso. E ela também vai cuidar de um outro probleminha que aparece na hora de montar o tensor-dataset em NLP, que é o do tamanho das sequências: se forem de tamanhos diferentes, não podem virar tensor e passar de forma otimizada pelo nosso modelo. A solução para isso é o padding: determinamos um tamanho máximo para a sequência. Nas menores, colocamos 0 ao final até que atinjam o tamanho; as maiores nós só cortamos fora. Dito isso, vamos ao código do preprocessamento, que vamos começar com os imports. Depois, vamos importar o dataframe e tirar dele tudo que não seja texto e label. Como as pontuações estão de 1 a 5 estrelas, vamos tirar as 3 e 4 e adotar: 1 e 2 estrelas como negativo e 5 estrelas como positivo. Lembre-se de baixar o dataset pelo site ou pela API do Kaggle: kaggle datasets download -d snap/amazon-fine-food-reviews Também vamos remover aleatoriamente dos textos com análise positiva até que o dataset esteja equilibrado. Por fim, aproveitamos para, no mesmo snippet, transformar isso tudo em listas — o que vai facilitar a manipulação desses dados nos próximos passos. Então, mas especificamente, vamos tirar as colunas que não queremos com df.drop , remover as pontuações 3 a 4 de formas diferentes, pra mostrar as formas que isso pode ser feito com o pandas, e depois transformar em lista, pra poder encodá-los. Vamos agora fazer o encoding dos dados. Nesse passo, o gargalo é a criação do nosso encoder. Isso acontece porque o SpacyEncoder vai iterar pelo dataset e criar o vocabulário. É necessário também analisar a possibilidade de termos textos muito longos (outliers na distribuição dos tamanhos de textos) e, caso isso ocorra, eliminá-los. Faremos isso porque, caso contrário, os textos com muitos 0-paddings vão enviesar a predição e os gradientes. Vamos também plotar a distribuição dos textos e ver se isso é verdade. Vimos também que o percentil 90 dessa distribuição é 191. Vamos remover os textos cujo tamanho ultrapasse esse valor para garantir que nossa RNN treine de forma menos enviesada. Por fim, podemos fazer o padding e criar o dataset. Vamos também fazer a divisão entre os datasets de treino e teste: Feito isso, podemos ir à criação do modelo. Para nosso modelo, vamos usar uma camada nn.Embedding, que vetorizará as sequências de índices do vocabulário, seguida de uma nn.LSTM para tratamento de sequência e uma nn.Linear para lidar com a complexidade do modelo. Veja que, após passar pela LSTM, pegamos só a última predição de cada sequência para passar pela camada Linear. Dito isso, podemos instanciar nossos objetos (a rede, otimizador, DataLoader, etc…) e ir para o loop de treinamento. Estamos indo para os últimos passos. Nos preparando para o loop de treinamento e teste, vamos criar os objetos que usaremos nele. Além da rede, do classificador e do loss-object, vamos criar um DataLoader a partir de um torch.TensorDataset , que, por sua vez, vai ser construído a partir dos nossos X_train , y_train , X_test , y_test que montamos antes: E podemos, por fim, ir para o loop de treinamento. Veja que usamos o tqdm para ter outputs mais bonitos e uma barra de progresso. Com nosso learning rate de 0.02, otimizador Adam, vamos treinar a rede por 10 épocas e com batch size de 64. Veja o loop de treinamento: Dito isso, podemos concluir esse post. Com nossa arquitetura e nossos hiperparâmetros, você deve obter uma acurácia perto de 85%. Considerando que temos um vocabulário gigante e uma arquitetura de rede pouco complexa e com poucos nós, é possível dizer que obtivemos um bom resultado. Por fim, como trabalho posterior, pode ser interessante, com base na simplicidade do preprocessamento dos dados e do modelo, pensar em como dar deploy nele e criar uma API para análise de sentimento de texto."
https://medium.com/turing-talks/introdu%C3%A7%C3%A3o-ao-processamento-de-linguagem-natural-com-baco-exu-do-blues-17cbb7404258?source=collection_home---------73----------------------------,Introdução ao Processamento de Linguagem Natural com Baco Exu do Blues,,Luísa Mendes Heise,1905,9,2020-05-10,"Texto escrito por Julia Miho Ichikawa Pocciotti e Luísa Mendes Heise. Fala, pessoal, tudo bem? Pra quem curtiu o texto do Fernando Pessoa, agora mais um texto sobre processamento de linguagem natural! Eu não gosto de você, sorrio ao te verNão quero não te ver jamaisEu pareço com você, no espelho está vocêNão me enlouqueça mais Processamento de Linguagem Natural é a área da ciência da computação focada na geração e compreensão das línguas humanas. Como a comunicação entre nós, humanos, fica restrita praticamente ao uso das línguas naturais, muitas são as tarefas que envolvem PLN (ou NLP em inglês). Hoje, já é possível detectar várias atividades do cotidiano das pessoas que envolvem essa área, seja através do uso de assistentes pessoais em celulares, filtros de spam na caixa de emails, tradutores automáticos e corretores ortográficos, por exemplo. Neste Turing Talks, iremos introduzir os primeiros passos com Processamento de Linguagem Natural. Para isso, estaremos analisando as músicas coletadas na internet do artista Baco Exu do Blues. Baco Exu do Blues é um cantor, compositor e rapper brasileiro. Em 2017, Baco lançou o seu primeiro disco solo, Esú, muito elogiado pela crítica. As músicas são pautadas fortemente em temas sociais, mas abordando também outras temáticas. Entre as músicas mais conhecidas, estão “Girassóis de Van Gogh”, “Me Desculpa Jay Z”, “Te amo desgraça” e “Flamingos”. Nesse texto, vamos utilizar ferramentas básicas de PLN para analisar a composição lexical das letras das canções do artista. Antes de iniciar a análise em si, precisamos adquirir os dados. Para isso, nós extraímos as letras do Baco Exu do Blues disponibilizadas pelo site vagalume. Você pode fazer isso com web scrapping (que foi o que fizemos) ou utilizando a API do vagalume. Agora que temos os nossos dados coletados, precisamos organizá-los antes de partirmos para a nossa análise. Com os pré-processamentos, conseguimos reduzir o tamanho do nosso vocabulário e simplificar algumas formas lexicais, garantindo, assim, que o nosso algoritmo consiga extrair informações relevantes e que de fato representam o nosso artista. Expressões regulares Expressões regulares podem ser consideradas uma sequência de caracteres utilizada para encontrar ou substituir padrões em uma string. Nesta etapa inicial, usaremos deste recurso para manter apenas os caracteres que representam letras do alfabeto. Para trabalhar com Expressões Regulares em Python, vamos importar a biblioteca re e utilizar o método .findall(). Nele, passamos o padrão a ser procurado e a string em que ele deve buscar esta informação. Neste post não entraremos em detalhe sobre a sintaxe de expressões regulares, mas você pode encontrar mais informações sobre elas aqui. Como em Python palavras como “oi” e “Oi” são interpretadas como palavras diferentes pelo sistema, nesta etapa também transformamos o texto para que ele contenha apenas letras minúsculas usando o método .lower(). Outra etapa frequente em pré-processamentos de PLN é a tokenização. Tokenização significa pegar cada uma das palavras do texto (tokens) e armazená-las em uma lista. Para fazer isso, poderíamos ter usado tokenizadores de bibliotecas como NLTK , entretanto, o método .findall() já nos retorna uma lista com as palavras do texto. Por isso, iremos pular esta etapa. Stopwords são palavras que, apesar de muito frequentes, carregam pouca relevância semântica. Entre elas, podemos encontrar artigos como “o” e “uma”, ou preposições como “de” e “em”, entre outras palavras frequentes no idioma. Para removê-las do texto, utilizamos uma lista de stopwords em português disponível na biblioteca NLTK. Em português, não encontramos nessa lista algumas formas contraídas de verbos ou preposições, como “tá” e “pra”. Sendo assim, adicionamos manualmente esses exemplos. Nesta etapa, iremos passar o texto por uma simplificação lexical. Para isso, há dois processos possíveis: Stemização e Lematização. O primeiro refere-se ao processo de reduzir as palavras flexionadas à sua raiz. O segundo é responsável por representar as palavras através do infinitivo para os verbos, e do masculino singular no caso de substantivos e adjetivos. No caso da Stemização, por retornar apenas a raiz da palavra, é possível que o resultado seja uma forma não dicionarizada desta. Como iremos utilizar as palavras para a nossa análise, escolhemos usar a Lematização. Para esta tarefa, utilizaremos o lematizador da biblioteca spaCy. Entretanto, ainda existem formas verbais que não são detectadas por essa ferramenta em português. Sendo assim, adicionamos manualmente também uma lista com alguns desses verbos. Agora, criaremos a função limpar_texto para compilar todas as etapas que descrevemos acima: Todos os passos anteriores de limpeza dos dados são essenciais para que a análise agora seja feita da melhor forma possível. Nosso objetivo é analisar as escolhas de vocabulário do Baco Exu do Blues em suas letras. Para isso, utilizamos técnicas de análise lexical. Mas o que é léxico? Bom, o léxico pode ser definido como um conjunto de palavras existente em uma língua. Assim, o vocabulário de cada um é um recorte dentro do léxico possível para aquele idioma. Plots de frequência Frequência lexical é nada mais nada menos do que avaliar quantas vezes uma dada palavra aparece dentro de um corpus (ou corpora). Na linguística, um corpus (plural corpora) de um texto é um conjunto de “subtextos” menores, geralmente armazenados e processados ​para análises ou predições. Para realizar uma análise de frequência lexical correta, nós precisamos fazer os devidos pré-processamentos, já que não é desejável que conjugações distintas de um mesmo verbo sejam contabilizadas como palavras diferentes. Também não queremos que stopwords constem na nossa amostra, pois certamente constariam entre as palavras mais usadas sem que isso nos acrescentasse na análise. Para o código vamos utilizar a função .FreqDist() do nltk. Nela passamos como parâmetro o texto que estamos analisando. Na hora de plotar, passamos o número de palavras mais frequentes que queremos representar. Assim, obtemos o seguinte plot, com as 30 palavras mais frequentes das músicas: Podemos fazer a mesma análise segmentada por álbum. Para isso, basta filtrar as letras de dado album, fazer a limpeza e passar para a função a string com as canções deste. Para fazer esse filtro vamos utilizar métodos do pandas. Obtemos, então, os seguintes plots: Para o álbum Esú Para o álbum Bluesman: Para o álbum OldMonkey: Wordcloud, ou “nuvem de palavras” é uma outra forma (mais visual e mais legal rsrs) de representar frequência lexical. Você já deve ter se deparado com alguma wordcloud na sua vida, mas você sabia que você pode usar o Python para gerar a sua? Para criar nossa nuvem de palavras, utilizamos a biblioteca wordcloud, que recebe uma string com todo o nosso texto e partir disso gera a figura. Dentro dessa biblioteca, teoricamente, já estão implementados pré-processamentos de stopwords. O problema aqui (e com muitas outras coisas em PLN) é que as coisas estão principalmente implementadas para o inglês, então nós precisamos fazer o nosso próprio pré processamento e concatenar nossos tokens em português em uma grande string para gerar a wordcloud. Assim, obtemos a seguinte nuvem de palavras: Essa função tem uma quantidade incrível de parâmetros que você pode utilizar para embelezar a sua nuvem de palavras. Para ver todos eles basta usar o comando ?WordCloud. Brincando um pouco com a função, utilizamos os parâmetros mask, colormap e font_path para criar uma máscara (um desenho) para conter nossas palavras, as cores delas e a fonte. Para fazer a máscara a partir da foto do Baco, utilizamos o seguinte código: O colormap escolhido foi o “copper”, você pode ver outros aqui. Para o font_path, basta você referenciar um arquivo no seu computador com a extensão .ttf . O resultado final fica assim: Nessa seção, a tradução livre dos termos não joga muito a nosso favor. Quando você lê a palavra “concordância”, talvez se lembre de uma longínqua aula de gramática em que tinha que verificar se um texto era coerente na escolha das pessoas verbais e seus pronomes ou na conjugação dos verbos. Aqui o conceito passa bem longe disso. No contexto de PLN, concordância (do inglês concordance) de uma palavra é, em geral, um método que nos retornará os vários contextos que essa palavra aparece no corpus deste texto. Entender o contexto e a distribuição das palavras em um dado texto é uma tarefa que se provou extremamente útil dentro do escopo de PLN. Ilustremos o porque disso: Você pode pensar em várias formas de definir o significado de uma palavra. O jeito que nos vem de forma mais óbvia é o mesmo procedimento adotado por um dicionário. Formas parecidas com essa, dentro de PLN, são as abordagens com semântica denotativa (do inglês denotational semantics), em que se utilizam sinônimos/relações semânticas dadas para definir o significado de uma palavra. Esse tipo de abordagem é adotada nas chamadas wordnets, que são bases de dados de léxicos de um dado idioma que estabelecem essas relações. Uma Wordnet em inglês de Princeton. Wordnet em português (USP de São Carlos) As wordnets foram por muito tempo a melhor maneira que tínhamos de estabelecer computacionalmente relações entre as palavras. Entretanto, uma abordagem que superou as wordnets e se mostrou extremamente eficaz em estabelecer significado (e a partir daí implicar relações semânticas entre as palavras) foi a de analisar os contextos. Essa ideia vem de uma área chamada Semântica distribucional (do inglês distributional semantics) e deu origem a algoritmos como o word2vec (que provavelmente será abordado em um post futuro). Agora que entendemos porque é tão importante avaliar as palavras no contexto que aparecem, vamos fazer essa análise dentro da obra do Baco Exu do Blues. Para isso, vamos utilizar dois métodos da biblioteca nltk. O primeiro deles, já explicado acima, é o concordance(), que nos retornará uma lista de contextos diferentes em que uma dada palavra aparece. O segundo é o similar() que nos retorna quais palavras mais aparecem no mesmo contexto que uma dada palavra. Vamos utilizar as palavras “mulher” e “dinheiro”. Esse código nos retorna: Para a palavra “dinheiro”, a função nos retorna: Agora, uma parte bem interessante é ver quais as palavras que mais aparecem no mesmo contexto que essas. Vamos usar então o método similar() para isso. O código acima nos retorna: Para a palavra “mulher”, temos: Isso significa que dentro do contexto dessa obra, as ideias entre essas palavras costumam aparecer no mesmo contexto. O que pode nos dizer muito sobre as críticas e problemáticas que Baco Exu do Blues aborda em suas músicas. Em conclusão, as ferramentas básicas de processamento de linguagem natural são muito úteis para uma primeira análise de textos. Não somente, elas podem ajudar muito na performance de modelos de machine learning em PLN (assunto para um próximo Turing Talks). Esperamos que vocês tenham gostado do texto! Se quiserem conhecer um pouco mais sobre o que fazemos no grupo, não deixem de seguir as nossas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Até a próxima!"
https://medium.com/turing-talks/construindo-uma-estrat%C3%A9gia-de-investimentos-quantitativa-time-series-momentum-7e60a40636bd?source=collection_home---------72----------------------------,Construindo uma Estratégia de Investimentos Quantitativa — Time Series Momentum,"O algoritmo de investimento que teve 951,62% de rentabilidade",Lucas Leme,1631,8,2020-05-17,"Texto escrito por: Lucas Leme Santos e Eduardo Netto. Agradecimentos: Alberto Chapchap e Giant Steps Capital Esse texto foi produzido principalmente com base no artigo Time Series Momentum de Tobias Moskowitz. Todo o código para esse projeto está disponível no seguinte repositório do GitHub do Grupo Turing. No mundo dos investimentos, a maneira “clássica” de se investir em empresas de capital aberto segue o seguinte processo: Outra maneira de se investir, que vem se tornando muito popular nos últimos anos, é criar algoritmos que definem a forma de operar no mercado financeiro, executando tudo automaticamente. É isso que fundos quantitativos fazem, seguindo o seguinte processo: Existem muitas estratégias diferentes utilizadas por gestores quantitativos. Neste texto iremos abordar uma estratégia baseada numa anomalia de mercado, que é o Momentum. “We search through historical data looking for anomalous patterns that we would not expect to occur at random.” — Jim Simons Momentum é uma palavra de origem latina, que na física clássica significa quantidade de movimento ou embalo, ele pode nos indicar duas coisas: a direção e a magnitude de um corpo em movimento. Além disso, outro elemento importante é a lei da inércia, que é a tendência natural de um objeto manter seu estado de repouso ou movimento. Com esses dois conceitos, podemos dizer que o momentum nos indica para onde e o quão rápido um objeto está se movendo, e que se não houver interferência externa, será constante. Trazendo para o contexto de finanças, o momentum é uma estratégia de investimento fundamentada no embalo dos preços, que considera a tendência dos preços para uma tomada de decisão de um investimento. Esse fator pode ser descrito por meio da seguinte equação: A formula acima compõe o que chamamos de retorno de um ativo, sendo o parâmetro k o período ao qual desejamos avaliar o retorno, podendo ser diário, semanal, mensal, etc. Podemos extrair o momentum a partir da equação do retorno apresentada, já que o retorno armazena tanto informação de intensidade de aumento ou decréscimo, quanto de tendência positiva ou negativa. Assim, podemos avaliar o momentum de 12 meses, tomando um período mensal e fazendo k = 12. De uma maneira simplificada, é uma teoria de investimento que diz que os preços do mercado sempre refletem corretamente e imediatamente as informações relacionadas ao ativo. Uma característica importante do momentum é o fato de ser uma anomalia da hipótese do mercado eficiente. Isso acontece porque, de acordo com a hipótese, o aumento ou decréscimo no preços de ativos deveria estar acompanhado de novas informações. Porém, o momentum indica que há tendência de movimento dos preços independentemente de novas informações. Um conceito frequentemente associado ao risco de um ativo é a volatilidade. Pois como o risco precisa ser mensurado, a volatilidade acaba sendo uma das formas mais simples de mensurar o risco. A partir dos dados histórico de uma série temporal, podemos utilizar uma medida descritiva estatística chamada desvio padrão para determinar a variação dos dados entorno de sua média histórica. Um valor baixo de volatilidade indica uma estabilidade no investimento. Já uma alta volatilidade indica incerteza, porém com um potencial de lucro maior, se o investidor estiver disposto a suportar o risco. Além do desvio padrão, existem diversas outras maneiras de estimar risco: Um estimador um pouco mais complexo é advindo do modelo de médias móveis, o exponentially weighted moving average. Esse estimador basicamente calcula o desvio padrão com um peso ponderado maior para os dados mais recentes. Essa característica é muito importante, pois em longos períodos a série temporal pode perder a estacionalidade, o que prejudica nosso modelo se considerarmos valores muito distantes. O Número de Parkinson foi desenvolvido pelo físico Michael Parkinson em 1980 e tem como objetivo estimar a volatilidade de uma série temporal a partir dos preços de High e Low. A principal vantagem em relação ao EWMA é a adição de variação intra diária, pois no cálculo leva-se em conta os valores de alta e baixa dos preços. Para calcular essa estimativa são necessários dois parâmetros N e n. Sendo N um fator de escala dos períodos, se for necessário um desvio padrão diário atribui-se N = 1, já para o desvio padrão anual N = 365. O segundo parâmetro n é o tamanho da amostra de dados passados que serão utilizados no modelo. Escolhido n realiza-se o somatório dos n períodos passados, da seguinte operação: logaritmo da divisão do preço de alta e baixa dos n períodos, e por fim eleva-se ao quadrado para realizar a soma. Garman é uma extensão do número de parkinson, porém agora com a inserção de mais dados no modelo, que são preços de abertura e fechamento do dia. Esses dados ajudam a aumentar a precisão do estimador, principalmente em ativos com maior estresse em momentos de abertura e fechamento. O cálculo é similar ao estimador anterior, segue abaixo a equação: Para efeito de comparação construímos o gráfico das 3 volatilidades em um preço de contrato futuro. Podemos perceber que, principalmente em momentos de maior estresse, os modelos Parkinson e Garman Klass têm uma resposta mais rápida e mais precisa. Um período que pode-se destacar é o ano de 2008, que os dois modelos acabaram captando mais informações intra diárias da crise da bolha hipotecária. A estratégia de investimento do Time Series Momentum é dada pela seguinte equação: E pode ser dividida em 3 principais etapas: Para determinar a posição de compra ou venda de um ativo, é necessário algum sinal de previsão, neste caso é utilizado o momentum (retorno) de 12 meses. Com o sinal em mãos classifica-se a posição a partir da direção do momentum. Quando se trata de alocação de portfólio no mundo quantitativo, existem diversas ferramentas, tais como: matrizes de covariâncias, autovalores, autovetores, PCA, autoencoders e entre outros. Mas para simplificar nosso modelo, iremos controlar as alocações para que elas estejam, em média, entorno de uma meta de risco. Definiremos o desvio padrão anual médio da carteira como meta, pois estamos dispostos a ter um risco médio de um contrato futuro, que no caso dos nossos dados é de 40%. Para o risco estimado, podemos utilizar as volatilidades discutidas nos tópicos anteriores. Com os dois parâmetros necessários (risco estimado e risco médio), temos uma razão que é inversamente proporcional ao risco de um ativo, ou seja, ativos muito voláteis acabam tendo uma alocação menor, e o contrário ocorre para ativos com baixo risco, que têm uma maior parcela na carteira. Vale ressaltar que ambos os casos tendem a se normalizar para o risco do portfólio (40%). Por exemplo: um ativo com 20% de volatilidade, o cálculo da alocação seria o seguinte: 40%/20% = 2, ou seja, esse ativo teria um alavancagem de 2 vezes, o que dobraria seu risco. Por fim, temos o número que realmente importa, o rendimento total do investimento. Podemos calculá-lo no fim do período, com os retornos de cada ativo. Graças a simplificação de alocação, podemos calcular a média do retorno de todos ativos (St), esse será o retorno do portfólio, conforme a imagem acima. Para realizar o backtesting da estratégia, utilizamos um dataset de preços dos mais diversos tipos de contratos futuros. No total são mais de 20 anos de preços históricos, com mais de 50 ativos. Afim de comparação, foi elaborada uma segunda estratégia de momentum. Porém, essa é Long only, ou seja, não depende do sinal do momentum, na qual todas as posições são compradas (Long). Em ambos os casos utilizamos o estimador de risco EWMA. E aqui finalmente temos os resultados do TSMOM. Ao total, obtemos 382.36% na estratégia original e 563.65% na long only. Que anualizando os retornos temos respectivamente 8.63% e 10.47%. Apesar de, no longo prazo, o TSMOM não superar o seu índice (Long only), é importante ressaltar a performance entre os anos 2000 e 2009. Período no qual ocorreram duas importante crises: bolha da internet (“dot-com”) e bolha hipotecária. Uma das principais características desse algoritmo é a sua resiliência em períodos de crise. Aqui temos novamente o caso da crise de 2008, quando o mercado foi acumulando quedas constantes nos preços, refletindo numa tendência de momentum negativo. E no auge da crise, em outubro, os preços de commodities e ações caíram drasticamente. Propiciando assim um momentum muito negativo. Nessa situação, o TSMOM se posicionou majoritariamente vendido, gerando grandes lucros no último trimestre de 2008. Com o intuito de adicionar mais informações no modelo, utilizamos o estimador de risco Garman-Klass e construímos um momentum modificado. Que é basicamente a média de momentums com períodos diferentes, os escolhidos foram 3, 6 e 12 meses. Essas modificações fizeram nosso algoritmo performar melhor que o Long Only, tal feito pode ser justificado pela a adição de informações mais recentes ao modelo, fazendo-o reagir mais rapidamente a grandes variações de preço. Com essas modificações alcançamos o resultado de 951,62%! Que anualizado implica em 13,18%. Resultados bem expressivos né? Porém essa estratégia parte de um pressuposto muito simples, que é de que o momentum de 12 meses é suficiente para decidir a compra ou venda um ativo pelo próximo mês, assumindo que o seu momentum será mantido! Podemos aprimorar esse algoritmo com o auxílio de feature engineering, técnicas de cross validação em séries temporais e machine learning, que podemos abordar em um futuro artigo. Esperamos que vocês tenham gostado do texto! Se quiserem conhecer um pouco mais sobre o que fazemos no Grupo Turing, não deixem de seguir as nossas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Até a próxima!"
https://medium.com/turing-talks/o-que-%C3%A9-o-turing-talks-c17b2aab62b3?source=collection_home---------71----------------------------,O que é o Turing Talks?,,Bernardo Coutinho,711,2,2020-05-17,"Inteligência Artificial para todos Este é o lema do Turing Talks, a publicação do Grupo Turing no Medium, onde artigos a respeito de diversos temas de Inteligência Artificial são postados semanalmente. Desde nossa gênese, temos como objetivo ensinar IA de forma compreensiva para qualquer pessoa interessada, independente do seu nível de conhecimento prévio. Atualmente, contamos com textos sobre os mais variados assuntos, como Programação, Data Science, Modelos de Predição, Redes Neurais, Aprendizado por Reforço, Processamento de Linguagem Natural e Finanças Quantitativas. Dessa forma, esperamos produzir conteúdo para indivíduos com os mais diversos interesses. Além disso, todos os códigos do nossos artigos estão presentes no nosso Github, um ótimo lugar para explorar os textos do Turing Talks. Por fim, caso você queira conhecer melhor o nosso trabalho, recomendamos ler os seguintes artigos: Como Machine Learning consegue diferenciar heterônimos de Fernando Pessoa Explicação sobre o excelente projeto da área de Processamento de Linguagem Natural do Grupo Turing. Modelos de Predição | Decision Tree Introdução a Decision Tree, um dos modelos de predição simples mais utilizados no Aprendizado de Máquina. Algorimos Genéticos Como construir um modelo de Machine Learning baseado na seleção natural para jogar o Chrome Dino Runner. Redes Neurais com Keras e TensorFlow 2.0 Um ótimo texto para aprender a programar Redes Neurais de forma simples e rápida."
https://medium.com/turing-talks/pouse-um-m%C3%B3dulo-lunar-com-deep-q-learning-1f4395ea764?source=collection_home---------70----------------------------,Pouse um módulo lunar com Deep Q-learning,"Um pequeno passo em Q-learning, mas um grande salto em IA!",William Fukushima,988,9,2020-06-07,"Texto escrito por William Abe Fukushima e Stephanie Miho Urashima. Bom dia, boa tarde ou boa noite. Bem-vindos a mais um Turing Talks! Prepare sua dose de cafeína porque hoje implementaremos o famoso Deep Q-Learning, um algoritmo que se baseia em Q-learning, e utiliza conceitos de Redes Neurais e RL juntos para solucionar problemas, criar jogadores ou resolver ambientes com estados contínuos. É o início da abordagem de aprendizado profundo na área de Aprendizado por Reforço, que abre muitas portas para otimizações mais eficientes assim como uma maior versatilidade em aplicações. Técnicas de aprendizado profundo na área de aprendizado por reforço podem ser aplicadas em outras áreas de machine learning (Processamento de Linguagem Natural, Ciência de Dados, Visão Computacional) para obter melhores resultados. Neste post, abordaremos os seguintes tópicos: E para tanto, estaremos resolvendo o ambiente ”LunarLander-v2” da biblioteca gym utilizando Tensorflow 2.0. Neste ambiente, o agente é um módulo lunar que está tentando pousar suavemente na lua. As possíveis ações do módulo lunar são: E os estados do ambiente são listas “s”: Para compreender como nosso algoritmo funciona, vamos primeiro recordar o algoritmo de Q-learning (caso não lembre do algoritmo, vale a pena dar uma olhada em nosso post anterior sobre Q-learning onde criamos uma IA que joga pong): inicializamos todos os valores das ações “Q” em cada estado aleatoriamente em uma tabela e conforme adquirimos informação sobre o valor das ações de cada estado, atualizamos da seguinte maneira: iterando para cada “dia da marmota”. A diferença entre o método tratado no post anterior e o tratado neste é que faremos uma abstração da tabela Q-valor para uma rede neural. “- Mas como assim?? Uma tabela pode ser substituída por uma rede neural??”, você deve estar se perguntando. Vamos desenvolver um pouco isto: “Para cada valor discreto de s, temos valores de ação Q(s,an) correspondentes” Bom, a tabela é simplesmente uma maneira de guardar associações entre estados e valores de ações de maneira discreta, ou seja, para cada valor de “s” que temos anotado na tabela, temos um conjunto de valores de ações associados. Essencialmente, isto é uma função. Entram alguns valores (s = (s1, s2, sn)) e saem outros (Q(s,a1), Q(s,a2), Q(s,an)). No entanto, como estávamos trabalhando com tabelas, só era possível para nosso algoritmo de Q-learning olhar para alguns pontos dessas funções. Em uma função, “Para valores contínuos de s, temos valores de ação Q(s,an) correspondentes” A rede neural é basicamente a representação de uma função contínua que estará fazendo o papel da tabela associando variáveis contínuas de estado à valores de ações, o que significa que não trabalharemos com um número fixo de estados, mas sim com uma gama contínua de estados. A rede neural utilizada é chamada de Deep Q-Network(DQN) e terá a seguinte estrutura: Os parâmetros são: Caso queira relembrar a teoria por trás de redes neurais visite a nossa série sobre Redes neurais: Redes Neurais | Teoria — Turing Talks, ou caso queira algo mais direto veja nosso post sobre Tensorflow 2.0: Redes Neurais | Redes Neurais com Keras e TensorFlow 2.0 Agora que nosso “Phil Connors” ganhou um cérebro em vez de uma planilha para aprender, precisamos definir algumas modificações da Equação de Bellman para adaptar à função de custo da rede neural. Como toda rede neural, para aplicar o gradiente descendente, precisamos de uma função de custo que envolve o que estamos tentando prever. Neste caso, o que estamos prevendo é o valor de uma ação dado um estado. Como já visto em Q-learning, durante as iterações, conseguimos obter os valores de Q a partir da seguinte atualização: Utilizaremos para nossa função de custo, esta definição como Q target e tentaremos aproximar o Q previsto do Q target, obtendo: Note que, para calcularmos o valor de Loss, precisamos do Q valor futuro. Por isso, para cada treino, são necessárias 2 predições. Uma para os Q valores presentes (Q(s,a)) e outra para os futuros (Q(s’,a)). No caso da imagem anterior, está sendo implementado o erro quadrático. No entanto, utilizar tanto erro quadrático quanto o erro quadrático médio (MSE) faz com que nosso agente tente corrigir erros grandes de maneira muito acentuada. Como as DQNs utilizam como métrica a própria predição para verificar se sua predição foi correta, não podemos ter correções grandes nos parâmetros dos neurônios pois isto torna o aprendizado muito instável. É como se a rede neural estivesse “perseguindo a própria cauda” já que ao tentar corrigir uma predição errada, ela também está afetando as futuras predições que serão usadas como Q target. Por isso, adotamos a função de perda de Huber (Huber loss) que se comporta de maneira mais linear conforme o erro aumenta enquanto mantém seu comportamento quadrático quando próximo de 0. Propiciando dessa forma, alterações menos bruscas na rede neural e estabilizando o aprendizado. Veremos mais a frente que essa situação da rede neural está “perseguindo a própria cauda” tem outras formas de estabilização, mas para nos mantermos mais próximos de Q-learning por hora, vamos apenas mexer na função de custo. A ideia de utilizar uma rede neural no lugar de uma tabela para abranger os problemas com espaços contínuos foi importante, no entanto, a ideia que tornou DQN realmente aplicável é o uso de Experience Replay ou, em outras palavras, pegar dados passados de uma memória criada para guardar transições de estado (o conjunto de estado passado, ação tomada, recompensa e novo estado). Para facilitar a implementação da Repetição de Experiências, criamos uma classe da memória que possui um tamanho máximo de experiências armazenadas de tal forma que, quando a memória é lotada, as experiências mais antigas passam a ser deletadas a fim de abrir espaço para novas experiências: As variáveis internas representam: A memória precisa ter as seguintes funcionalidades: Experience Replay é importante devido ao fato de que redes neurais precisam treinar informações em ordem não correlacionada para obter bons resultados. Sem a memória, as informações que são fornecidas para a rede neural treinar estariam fortemente correlacionadas uma vez que o estado atual tem correlação com os estados passados afetando o aprendizado da rede neural que necessita de amostras independentes e bem distribuídas (De que adianta uma ferramenta de treino que se adapta a várias situações se você está treinando a mesma situação várias vezes seguidas sem variações). Por isso, alternar o treino da rede neural com dados tanto do presente quanto do passado minimiza a correlação e traz resultados melhores além de que faz um uso mais eficiente das experiências passadas já que elas são utilizadas múltiplas vezes para treinar o modelo. Finalmente, juntaremos todos esses conceitos para criar nosso agente. Da mesma maneira que em nosso Talks de Q-learning, faremos um agente com a estratégia epsilon-greedy. Nosso agente deve possuir: ações, epsilons (inicial, decaimento e final), um fator de desconto, uma memória de experiências e uma DQN. Adicionalmente, uma vez que a execução de algoritmos de Aprendizado por Reforço profundo costumam levar bastante tempo, será passado também uma string com um nome de arquivo para implementar persistência de arquivo. Nosso agente terá os seguintes métodos: O que nosso agente deve fazer? Seguindo a política epsilon-greedy, nosso agente deve em ordem: Ao final, teremos o agente treinado salvo em ‘dqn_save.h5’, a curva de aprendizado do algoritmo e também a curva de decaimento do epsilon. Como podemos perceber, o agente obteve resultados aceitáveis, mas a curva de aprendizado possui certa instabilidade ainda. É possível notar que fizemos o epsilon decair muito rapidamente, mas grande parte do aprendizado ocorre enquanto estamos tomando as ações ótimas. Em ambientes mais complexos, o Deep Q-learning da forma implementada não funciona tão bem pois há uma instabilidade muito grande provocada pelas constantes mudanças no agente e, consequentemente, a inconsistência dos valores de Q target. No entanto felizmente há formas de estabilizar a DQN sendo uma delas com o uso de Fixed Q-targets, tanto ele como as outras formas serão explicadas em um dos próximos Turing Talks. Então fiquem ligados com os próximos posts no Medium, esperamos que vocês tenham gostado do texto! Se quiserem conhecer um pouco mais sobre o que fazemos no grupo, não deixem de seguir as nossas redes sociais: Facebook, Instagram, LinkedIn."
https://medium.com/turing-talks/blitz-uma-lib-de-deep-learning-bayesiano-no-pytorch-48f96fd907f6?source=collection_home---------69----------------------------,BLiTZ — Uma lib de Deep Learning Bayesiano no PyTorch,Simples e extensível para criar camadas de Deep Learning Bayesiano no PyTorch,Piero Esposito,850,6,2020-06-14,"Esse post é uma tradução de BLiTZ — A Bayesian Neural Network library for PyTorch, escrito por mim e publicado no towards data science. O link para o repositório da lib é: https://github.com/piEsposito/blitz-bayesian-deep-learning. Neste post, vamos apresentar como usar a primeira lib de Deep Learning feita por um membro do Grupo Turing — e focaremos mais no uso dessa lib do que na teoria do Deep Learning Bayesiano. Se você for novo no tema, pode querer procurar um dos inúmeros posts do Medium sobre ele ou ir à seção da documentação da lib dedicada a teoria. Existe uma demanda crescente, tanto no mercado como no meio acadêmico, para formas de extrair a confiabilidade de predições de modelos de Deep Learning. As redes neurais Bayesianas surgiram como uma abordagem bem intuitiva para resolver essa demanda — e isso se confirma pelo fato de aparecerem cada vez mais como “trending-topic” em Deep Learning. Ocorre que, a despeito de o PyTorch estar ganhando cada vez mais espaço como um framework (de pesquisa principalmente) de Deep Learning, não há nenhuma lib que permita que se introduza camadas Bayesianas de redes neurais de forma fácil como se faz com a nn.Linear e nn.Conv2d, por exemplo. Logicamente, isso cria um gargalo para qualquer um que queira iterar de forma flexível com abordagens Bayesianas em suas modelagens de dados, assim como para qualquer um que tenha que criar camadas Bayesianas do zero em detrimento de focar na arquitetura e hiperparâmetros do modelo. A BLiTZ veio para resolver esse gargalo. Ao ser completamente integrada com o PyTorch (incluindo os módulos nn.Sequential ) e fácil de se extender, nossa lib deixa o usuário introduzir incerteza em suas redes neurais sem muito esforço, podendo focar mais na arquitetura e hiperparâmetros do modelo. Neste post, vamos discutir como treinar e inferir sobre modelos de redes neurais Bayesianas com incerteza em seus pesos. Como se sabe, a ideia principal em Deep Learning probabilístico é a de, ao invés de se ter pesos determinísticos (isso é, aquele tensor de pesos), a camada tem uma distribuição de probabilidade para seus pesos que, a cada operação feedforward, são extraídos com processo de sampling dessa distribuição. Consequentemente os parâmetros treináveis dessas camadas bayesianas são a média e a variância da distribuição (para cada escalar que compõe o tensor). Matematicamente as operações vão de: Implementar no PyTorch camadas onde nossos ρ e μ são parâmetros treináveis pode ser complicado — e fazer com que sejam integradas com o resto do framework é pior ainda. Vamos mostrar como facilmente podemos usar a camada linear Bayesiana BayesianLinear do BLiTZ, que pode ser facilmente integrada no modelo: O modelo que essa classe representa funciona como um nn.Module simples do PyTorch, mas sua camada BayesianLinear vai fazer a inferência e ser treinada levando em consideração a incerteza explicada acima. Conforme proposto no paper que introduziu as redes Bayesianas, a função custo é uma combinação do custo de “fitting nos dados” com um “custo de complexidade”, isso é, que mede quanta incerteza está presente nessa distribuição. Esse custo de complexidade é a verossimilhança das predições obtidas pela amostragem da distribuição numa função densidade de probabilidade P(x) bem mais simples que a que define nossos pesos. Adotamos como Q(x) a função que corresponde à operação feedforward que nossa rede neural aplica sobre os inputs para obter a predição. Extrair um custo de complexidade dependente do tensor de pesos da camada é algo que exige a manipulação do framework de Deep Learning a ser usado num nível de abstração bem baixo, o que pode ser complicado e demandar tempo. Para evitar esse problema, BLiTZ traz um decorator, variational_estimator , que adiciona um método para extrair a soma dos custos de complexidade de todas as camadas Bayesianas do modelo. Para extraí-la, esse custo do modelo, é simples assim: Redes neurais bayesianas costumam ser otimizadas fazendo uma amostragem da loss várias vezes e depois o backprop. Isso acontece porque, como nossos pesos não são determinísticos, fazemos uma estimação da nossa predição por métodos de Monte Carlo (isto é, vários feedforwards, cada um com os pesos extraídos da distribuição, e depois a média). Isso também pode complicar a vida do praticante de Deep Learning, e para isso, o decorator variational_estimator introduz também o método sample_elbo (Evidence Lower Bound, nome atribuído a função custo em Deep Learning Bayesiano). Com esse método, introduzido o criterion (objeto do Torch que calcula a função custo), os inputs , labels e sample_nbr (número de operações feedforward para obter o custo), é possível calcular o custo sem verbosidade no código. Veja só como é fácil otimizar uma rede usando BLiTZ: Agora vamos mostrar um exemplo, passo a passo, de como usar BLiTZ para criar uma rede neural Bayesiana e estimar intervalos de confiança para os preços do toy-dataset de preços de casa de Boston. Isso corresponde a esse exemplo, da lib. Se você quiser ver os outros, estão aqui. Além dos imports usuais de quando se usa PyTorch, vamos importar, da BLiTZ, o decorator variational_estimator , que vai nos ajudar a performar as operações específicas de Deep Learning Bayesiano sem afetar a integração com o PyTorch e, é claro, BayesianLinear , que é nossa camada Bayesiana correspondente à nn.Linear do PyTorch. Nada de novo sob o Sol aqui, vamos só importar e fazer aquele scaling básico nos dados: Vamos criar nossa classe herdando de nn.Module como normalmente se faz com PyTorch. Nosso decorator vai introduzir de forma não invasiva o que precisamos para operar essa rede neural como Bayesiana. Pra isso, duas camadas, já que os dados são simples e queremos ter resultados rápido. Para poder avaliar nosso modelo, queremos ver a precisão de nosso intervalo de confiança. Criamos então uma função que performa, nos dados de teste, a operação feedforward X vezes e depois, escolhendo um multipicador para o desvio padrão, vemos se os valores corretos estão dentro do intervalo (média — multiplicador * desvio padrão, média + multiplicador * desvio padrão). Note que, a despeito do decorator, criamos nosso BayesianRegressor como faríamos com um nn.Module — e o usaremos da mesma forma: Nosso loop de treinamento segue, em grande parte, o que se faz em treinamentos padrão no PyTorch — a única diferença é que obtemos nosso custo com o método sample_elbo . O resto das operações é feito normalmente. Decidimos, por fim, que a cada 100 iterações ele vai printar o custo e acurácia de nosso intervalo de confiança. A BLiTZ é uma lib útil para iterar experimentos em Deep Learning Bayesiano sem muito esforço. Bem integrado com PyTorch e com poucas mudanças para o código usual, é bem intuitivo para o usuário comum do framework usar sem muita dor de cabeça, podendo extrair o custo de complexidade e obter amostras do custo sem muito trabalho. Por fim, aqui está novamente o link para o nosso repo: https://github.com/piEsposito/blitz-bayesian-deep-learning. Se você gostou, deixe seus claps e uma estrelinha lá no repositório — isso ajuda muito nosso trabalho a ter mais visibilidade e ajudar mais pessoas com seus experimentos em Deep Learning."
https://medium.com/turing-talks/usando-deep-learning-para-jogar-super-mario-bros-8d58eee6e9c2?source=collection_home---------68----------------------------,Usando Deep Learning para jogar Super Mario Bros.,Criando uma rede neural que aprende a jogar Super Mario.,Bernardo Coutinho,1103,11,2020-06-21,"Todo o código do programa está neste repositório. Se você se interessa por programação, com certeza já deve ter ouvido falar de Deep Learning, um dos assuntos mais badalados dos últimos tempos. Esse aprendizado é utilizado nas mais variadas aplicações, desde reconhecimento de imagens até processamento de linguagem. No Turing Talks de hoje, vamos aplicar Deep Learning de uma das maneiras mais interessantes: ensinando um programa a jogar Super Mario Bros. O texto de hoje não precisa de tanto conhecimento prévio, estamos colocando todas as explicações mais complexas ao final do texto em um apêndice. Aprendizado por Reforço é o nome da área que estuda programas que aprendem a realizar tarefas complexas por tentativa e erro, de forma similar aos seres humanos. Esses programas são jogados em um mundo e vão experimentando diferentes ações, recebendo um feedback depois de cada uma. Assim, eles se tornam capazes de entender quais ações costumam ter um melhor resultado, aprendendo a agir com um comportamento ideal. Esses programas que devem aprender a tomar decisões são chamados de agentes, e o mundo com o qual eles interagem é chamado de ambiente. Essa interação acontece por meio de ações, e cada uma destas recebe uma recompensa diferente, que informa quão boa foi aquela ação naquele determinado estado — o estado é a configuração do ambiente naquele instante. Tomando o gif acima como exemplo, podemos dizer que o ser humano, o agente, tenta interagir com a mangueira, que faz parte do seu ambiente, por meio de uma ação: ligar ela em sua direção. Depois de tomar essa ação, o ser humano recebe uma recompensa bem negativa, seu estado vai de seco para molhado, e ele aprende a nunca mais tomar aquela ação de novo — assim espero. Resumidamente, o nosso agente interage com o ambiente por meio de uma ação, e recebe de volta uma recompensa e um novo estado, no qual ele deve se basear para tomar a próxima ação. No caso do jogo de Super Mario Bros., o nosso agente é o jogador, o programa que controla o Mario, enquanto o ambiente é todo o mundo do jogo com o qual o Mario interage: os blocos, os canos, os Goombas, etc, e o estado do ambiente, sua configuração atual, é o frame atual do jogo. É com base nesse frame que o agente deve decidir a melhor ação a se tomar. As ações que o nosso agente pode tomar são os controles do Mario: pular, andar, ficar parado, etc. E, a cada frame do jogo, o agente recebe uma recompensa indicando quão bem ele agiu: se ele morrer, ele recebe uma recompensa negativa; se ele chegar mais perto do final, ele recebe uma recompensa positiva. Ufa! Depois de toda essa teoria, finalmente entendemos como o nosso programa aprende a tomar as melhores decisões em um determinado ambiente. Em 2013, a Deepmind, uma das maiores empresa de desenvolvimento de Inteligência Artificial do mundo, desenvolveu a chamada Deep Q-Network (DQN), uma rede neural capaz de aprender a jogar jogos de Atari melhor que seres humanos. Essa invenção revolucionou o Aprendizado por Reforço, tornando possíveis resolver problemas muito mais complexos do que conseguíamos anteriormente, até mesmo aqueles em que o nosso programa precisa aprender a interpretar os frames de um jogo. No passado, utilizamos uma DQN para ensinar um módulo lunar a pousar na Lua. Desta vez, utilizando alguns avanços nessa técnica, vamos desenvolver uma rede neural que aprende sozinha a jogar a primeira fase de Super Mario Bros. Antes de pensar em como aplicar uma IA para jogar Super Mario Bros., precisamos de um ambiente, um emulador, com o qual nosso agente irá interagir. O gym-super-mario-bros é exatamente isso, um ambiente de Aprendizado por Reforço no qual podemos criar programas que jogam quaisquer fases dos dois primeiros Super Mario Bros. Nesse ambiente, nós recebemos uma imagem do frame atual do jogo (esse é o nosso estado), e devemos escolher uma ação para tomar com base nesse frame, como pular, andar para a esquerda, ficar parado, etc. Com o gym-super-mario-bros, podemos escolher diferentes fases e visuais para o jogo, dependendo do nosso objetivo. Nesse caso, vamos utilizar o ambiente SuperMarioBros-1–1–v1, em que jogamos uma versão da primeira fase do Mario com um plano de fundo mais simplificado, para facilitar o reconhecimento de imagem. Para entender melhor como funciona esse ambiente, vamos programar um agente simples que toma ações aleatórias para jogar essa primeira fase. O primeiro passo para criar qualquer agente é criar o nosso ambiente, nesse caso, o gym-super-mario-bros. Esse ambiente vem por padrão com 256 ações possíveis para controlar o Mario, sendo que a maioria delas não movimenta o jogador. Para restringir as nossas ações apenas às ações importantes, que controlam o nosso personagem, o gym-super-mario-bros recomenda usar um JoypadSpace e um SIMPLE_MOVEMENT, como fizemos abaixo. Agora, vamos entender como funciona para rodar um episódio dessa fase, criando uma função run_episode. Primeiro, usamos o método .reset() no nosso ambiente, que reseta ele para a posição inicial e retorna o primeiro frame do jogo, que guardaremos em state. Depois, vamos criar um loop que toma ações aleatórias no ambiente até que a fase termine — quando o Mario morre, o tempo acaba, ou ele chega no final. Para escolher uma ação aleatória, o ambiente possui um atributo .action_space que contém todas as ações possíveis. Se utilizarmos um .sample() nesse action_space, ele retorna aleatoriamente uma dessas ações possíveis. Por fim, usamos o método .step(action) do nosso ambiente, que toma a ação escolhida e retorna o novo frame do jogo, a recompensa recebida, e uma variável que diz se a fase terminou ou não. Pronto! Agora temos um Mario que toma ações aleatórias, mas que não consegue aprender com seus acertos e erros, como você pode ver no gif a seguir. Agora que já sabemos como trabalhar com o nosso ambiente, podemos finalmente programar o agente que aprenderá a jogar a primeira fase. Como já explicamos o que é uma DQN mais a fundo nesse post passado, vamos explicar mais uma intuição de como ela funciona. De maneira simples, uma rede neural nada mais é que uma função que recebe algo como entrada e vai aprendendo a calcular uma saída com base nos exemplos que ela recebe. Em um caso de reconhecimento de imagem, uma rede neural pode receber uma imagem como entrada e aprender a classificar o que está representado nela com base em exemplos. Como o objetivo do nosso problema é receber uma imagem do estado do jogo e definir qual ação devemos tomar, a entrada da nossa rede vai ser uma imagem do frame atual do jogo, e as nossas saídas vão ser as Qualidades das ações que podem ser tomadas, sendo que a ação com maior Qualidade deve corresponder à melhor decisão. A Qualidade (Q) de uma ação é definida como o retorno esperado daquela ação, que equivale à média da recompensa total que recebemos ao tomar aquela ação naquele estado. Simplificadamente, é um número que representa quão bom é tomar uma determinada ação em um estado. Explicamos melhor esses conceitos no post de Introdução ao Aprendizado por Reforço. Dessa forma, a arquitetura da rede neural que utilizamos está representada simplificadamente no diagrama abaixo: Ela funciona da seguinte forma: No início, nossa DQN será péssima tanto em interpretar as imagens que ela recebe quanto em calcular a Qualidade de cada ação com base nessa interpretação. Entretanto, após ser treinada, ela entenderá melhor o que deve fazer, e será capaz de distinguir bem as melhores ações em cada estado. A implementação dessa rede não é tão simples, mas se você quiser dar uma olhada, ela está presente no início deste arquivo. Também estamos colocando ela em um Apêndice ao final do texto com várias explicações mais avançadas! Agora que explicamos o funcionamento da nossa rede neural, só precisamos mostrar como ela interage com o ambiente. Lembra de quando criamos um agente que interagia com o ambiente com ações aleatórias? Agora vamos fazer a mesma coisa, só que com um agente que realmente vai aprender as melhores ações a se tomar. Antes de tudo, vamos criar uma função state_reshape(state) que normaliza e modifica os nossos frames para facilitar as contas da rede. Depois, podemos criar a função que treina o nosso agente: Temos algumas diferenças com relação à função do agente aleatório. Deixamos a explicação mais a fundo de cada um desses método no apêndice ao final do post Finalmente! Agora é só rodar o código e o nosso agente aprenderá sozinho a jogar Super Mario Bros. Depois de várias horas treinando, o nosso melhor resultado com a DQN foi este, que você viu no início do post: Mas quando utilizamos algoritmos mais complexos de Aprendizado por Reforço, o nosso resultado ficou melhor ainda! Esperamos que você tenha gostado do nosso projeto! O Aprendizado por Reforço é uma das áreas mais interessantes de Inteligência Artificial, e nosso objetivo é interessar mais pessoas nela apresentando os nossos projetos. Caso se interesse por aprofundar mais no projeto, estamos deixando um Apêndice com maiores explicações sobre ele, e você sempre pode dar uma olhada no próprio código. Se quiser conhecer um pouco mais sobre o Grupo Turing, não deixem de seguir as nossas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Até a próxima! Durante o texto, simplificamos ou evitamos falar de algumas partes um pouco mais complicadas do texto, as quais iremos elaborar mais sobre a seguir: A rede que utilizamos no projeto foi escrita em Torch, e possui três camadas de convolução e duas interconectadas. Nesse caso, usamos um tipo específico de DQN chamado Dueling Deep Q-Network, em que dividimos a nossa rede em duas partes no final: uma que calcula o valor (V) do estado e outra que calcula a vantagem (advantage) de cada ação. Esse é um avanço que melhora a performance da rede dividindo a Qualidade que ela deve aprender em duas partes distintas. Deixamos essa parte mais complicada do post pro final. Se você quiser entender melhor o agente, recomendamos ler nosso post de Q-Learning, em que ensinamos uma IA a jogar Pong, e o de DQN que mencionamos anteriormente. Primeiro, vamos criar uma classe Agent que vai ter todos os atributos e métodos importantes do nosso agente: Não se preocupe se você não entender muito do código do agente, ele é bem complexo Como mencionamos antes, nosso agente deve escolher a ação de maior qualidade se ele quiser agir de acordo com o que ele acredita ser o melhor comportamento. Entretanto, como nosso agente começa sem saber nada, é possível que ele esteja bem confiante de que uma determinada ação ruim é a melhor. Se, por exemplo, nosso agente acreditar que ficar parado é a melhor ação a se tomar, ele nunca sairá do lugar e nunca experimentará as outras ações do jogo. Para impedir isso, fazemos com que o nosso agente escolha uma ação aleatória com uma probabilidade epsilon, variando as ações que ele toma. Conforme o nosso agente vai conhecendo melhor o ambiente, vamos decrescendo essa probabilidade epsilon exponencialmente, já que ele não precisará mais explorar tanto o ambiente. Contudo, ainda deixamos um epsilon mínimo para que o agente sempre busque experimentar novas ações. Dessa forma, o nosso agente agiria por meio do seguinte método: Depois de cada ação, vamos guardar suas informações na memória, para depois conseguirmos calcular a qualidade de todas as ações reutilizando experiências do passado. Nesse método, guardamos o estado atual, a ação tomada, a recompensa da nossa ação, o estado seguinte a nossa ação e se a fase terminou. O nome de cada conjunto dessas informações é chamado experiência. Depois de guardar muitas experiências na nossa memória, podemos finalmente treinar a nossa rede neural. O método a seguir pega as experiências na nossa memória e otimiza nossa rede neural com base nelas. O código dessa otimização é um tanto quanto complexo, já que além da otimização normal de uma DQN ainda são usadas as técnicas de Double Q-Learning e Prioritized Experience Replay. Para entender a otimização de uma DQN, recomendamos o nosso texto sobre o assunto."
https://medium.com/turing-talks/como-avaliar-seu-modelo-de-classifica%C3%A7%C3%A3o-acd2a03690e?source=collection_home---------67----------------------------,Como Avaliar Seu Modelo de Classificação,As principais métricas de avaliação de classificadores,Gustavo Korzune Gurgel,1070,9,2020-07-06,"Texto escrito por Gustavo Gurgel e Guilherme Fernandes. Imagine que você foi selecionado por um hospital para desenvolver um programa que identifique quais pacientes são portadores de COVID-19 usando seus exames, e que para fazer isso você decide criar um classificador. Ao pensar que esse classificador pode ser determinante na vida de inúmeras pessoas, você se faz a seguinte pergunta: como saber se meu modelo é uma maneira segura de dizer se alguém realmente está doente? O objetivo desse Turing Talks é expor as métricas que mais são utilizadas para medir a performance de modelos de Inteligência Artificial, seus prós e contras e quando usá-las. Depois de treinar seu classificador você decide usar os dados de teste para verificar quão bom está o modelo. A métrica mais usada para mensurar o desempenho de classificadores é a acurácia. Ela é bem direta e pode ser usada quando se deseja ter uma noção geral de quão bem seu modelo está funcionando. Define-se acurácia pela fórmula abaixo. No caso do modelo que foi implementado, poderíamos usar uma função do scikit-learn que retorna a acurácia, a partir de uma comparação entre os valores preditos e os verdadeiros. Veja o exemplo abaixo implementado usando a biblioteca scikit-learn: Um dos maiores problemas ao usar a acurácia é a falta de flexibilidade para dar ênfase no tipo de projeto que se está realizando, e acabamos por algumas vezes fazendo uma análise muito superficial. Por exemplo, existem projetos onde é mais importante acertar todos os resultados negativos, e.g. no nosso classificador de pacientes, afinal nenhum médico quer que um paciente doente volte para casa acreditando que está saudável. Começaremos a entender em seguida como levar essas considerações em conta. A matriz de confusão nos permite analisar o desempenho do nosso modelo de forma mais cuidadosa, especialmente no que concerne ao problema da acurácia discutido anteriormente. Cada quadrante da matriz de confusão representa uma contagem referente aos tipos de acertos ou erros cometidos. Usando a Matriz de Confusão fica mais fácil formalizar o problema anterior, pois se um classificador previsse alguém doente como saudável ele estaria cometendo um erro do tipo False Negative. Então, de acordo com a lógica que estávamos tendo anteriormente, em um classificador de portadores de COVID nosso objetivo não é simplesmente ter o menor número de erros possível; na verdade queremos o menor número de False Negatives possível. Também existem casos em que o tipo de erro que queremos minimizar são os False Positives. Se pegarmos o exemplo de um classificador de spams, seria pior se um e-mail importante fosse para caixa de spams do que se você recebesse alguns spams na caixa de entrada de vez em quando. Para avaliar um classificador com relação aos False Positives ou False Negatives usam-se algumas taxas de desempenho, cujos valores possíveis variam de 0 a 1, em que 1 é o valor ótimo. Recall, True Positive Rate (TPR) ou Sensitividade Mede uma proporção dos valores que são de facto positivos e que foram preditos corretamente. É uma proporção entre os positivos verdadeiros (TP) e os falsos negativos (FN), conforme apresentado na fórmula. Mede a proporção de predições positivas que estão corretas, ou seja, quão bem o modelo predisse os valores positivos. É uma proporção entre os positivos verdadeiros (TP) e os falso positivos (FP), conforme apresentado na fórmula. Precision e Recall estão entre os principais indicadores para avaliação de modelos, isso acontece porque ambos comparam TP com os erros cometidos. E conforme vimos anteriormente, dependendo do problema desejamos minimizar apenas um desses erros, FP ou FN. False Positive Rate (FPR) Diferentemente do Recall, a FPR é uma taxa que caracteriza os valores falso positivos (FP) com relação aos verdadeiro negativos (TN). Indica, portanto, quanto da classe negativa foi predito incorretamente. Podemos calcular cada uma dessas taxas usando a matriz de confusão gerada usando o scikit-learn. Apesar das Taxas de Desempenho serem uma maneira mais circunspecta de analisar o desempenho de um modelo, não seria interessante usar apenas o Recall e ignorar a Precision, por exemplo. O ideal seria ter um valor que conseguisse integrar ambas as taxas. Como dito anteriormente, às vezes a Precision é mais importante que o Recall ou vice-versa, mas ainda assim não é possível abrir mão de nenhuma delas. A F-measure é uma média (harmônica) ponderada justamente para resolver esse problema, pois variando os pesos dessa média é possível dar mais ênfase ao valor do Recall ou Precision, ao mesmo tempo que não se fecham os olhos para a nenhuma das duas taxas. Se ponderarmos as taxas com um peso α arbitrário a fórmula geral da F-measure fica assim: Contudo, nem sempre é trivial determinar o valor desse peso α. Suponha que no projeto no qual você está trabalhando o Recall é n vezes mais importante que a Precision, qual valor de α você deve escolher de modo que a exigência do projeto seja atendida? Para resolver esse problema é usado o parâmetro β, que torna o uso da F-measure bem mais intuitivo. β é definido de maneira que o valor escolhido seja o número de vezes que o Recall é mais importante do que a Precision para o seu projeto, ou seja, Por exemplo, em um projeto em que a Precision é duas vezes mais importante que o Recall (i.e. Recall tem metade da importância da Precision). Você teria que usar β = 1/2 para ponderar sua F-measure, enquanto, se estivesse usando α, seria α = 4/5. Usar β é mais conveniente, não? Vamos supor que no classificador do COVID você queira dar duas vezes mais importância para o Recall do que para a Precision, ou seja, é duas vezes mais importante que as pessoa com Corona Vírus sejam detectadas pelo classificador do que que todas as pessoas detectadas pelo classificador estejam realmente com o vírus. Segue abaixo uma implementação disso no scikit-learn. Tanto a Acurácia quanto a F-measure tem um grande problema com datasets desbalanceados. Um dataset está desbalanceado quando existem muito mais entradas (entries) de uma das classes que da outra, e.g. imagine que o hospital tenha nos dado um dataset contendo informações de 5000 pacientes, dos quais 4700 estavam com COVID. Caso nosso modelo fosse fortemente enviesado, ou seja, praticamente sempre classificasse os pacientes como portadores de COVID, ele conseguiria uma acurácia de 94% e um F2 de 98.7%. Veja como podemos ter esses resultados no código abaixo. Um modelo fortemente enviesado certamente não é bom, apesar disso, caso nosso dataset esteja muito desbalanceado, as métricas que vimos até agora podem nos fazer acreditar o contrário. A relação das métricas numéricas com dados desbalanceados é um tema muito sútil e que mereceria um Turing Talks inteiro para ser discutido. Em problemas de classificação binária, precisamos que o modelo distingua uma observação de dados entre 0 e 1. Contudo, os modelos geralmente não retornam esse resultado de maneira discreta, isto é, ou ‘0' ou ‘1’. O que acontece é que a resposta (Y) do modelo é representada por um valor numérico entre zero e um, que garante uma “confiança” para a classificação. Mas você deve estar se perguntando: se o modelo retorna um (Y) entre zero e um, como se distingue uma resposta entre ‘0’ e ‘1’? Isso acontece por meio de um limite, ou threshold (t). Esse limite determina qual valor separa um resultado ‘1’ e um ‘0’. Por exemplo, se atribuirmos t = 0.5, significa que os valores abaixo de 0.5 serão ‘0’ e os valores acima serão ‘1’. O threshold é extremamente importante em modelos de classificação, pois como veremos, ele está diretamente relacionado com a performance de um modelo. A curva ROC mede a performance de um modelo para diferentes thresholds. Essa curva é definida com os seguintes eixos: Dessa forma, para cada variação no threshold que fizermos, obteremos um ponto para a curva ROC a partir de uma matriz de confusão. Ao final do processo, esperamos uma curva similar a abaixo. Analisando essa curva, podemos decidir qual threshold (t) usar, escolhendo conforme as necessidades do problema. Por exemplo, para o problema de identificar pessoas com COVID-19 é necessário garantir um TPR (True Positive Rate) alto, para que ninguém que esteja com a doença não seja identificado com a doença. Para isso, poderíamos escolher um ‘t’ de forma que o TPR seja 1.0, ou tão próximo deste valor quanto desejarmos. Infelizmente, temos o problema de que quanto maior quisermos o TPR, maior será o valor de FPR, aumentando portanto a presença de falso positivos. Outra maneira de avaliar a curva ROC é analisando a baseline, pois ela representa onde TPR e FPR são iguais, e isso indica que é impossível escolher um threshold ‘t’ que aumente TPR sem piorar o FPR. O ideal seria encontrar um threshold que garante máximo de TPR e mínimo FPR, obtendo uma distribuição de TP e TN disjunta. Assim, teríamos algo similar ao que é apresentado na curva abaixo. No entanto, o que costumamos encontrar na realidade é uma interferência entre as distribuições de TP e TN, tornando este problema um trade-off, ou seja, para obter melhor resultado em um dos indicadores devemos perder eficácia no outro Outra métrica para avaliar a performance de modelos é a AUROC, geralmente tratada como AUC (Area Under The Curve), que é simplesmente a área sob a curva ROC, como indicado ao lado. Podemos compreender esse indicador como a habilidade do modelo de categorizar corretamente os dados, com valor máximo em 1 e mínimo em 0. Dado as características dessa métrica, ela é comumente utilizada para comparar a performance de modelos, por exemplo, verificar se uma regressão logística é melhor que um SVM para identificar pessoas com COVID-19. Assim sendo, desejamos a maior área possível, que por sua vez fornece o melhor modelo para o problema em questão. Para calcular esse indicador, podemos recorrer novamente ao scikit-learn: As métricas comentadas nesse Turing Talks são a base da análise de performance da maioria dos classificadores que existem e cada uma delas tem seus pontos fortes e fracos. Portanto, é sempre importante pensar como se pode gerir o modelo a fim de que, em situações importantes como a criação de um classificador de pacientes com COVID-19, seu modelo possa fazer o maior bem possível. As métricas são as ferramentas que você precisava para que isso seja possível. Esperamos que vocês tenham gostado do texto! Se quiserem conhecer um pouco mais sobre o que fazemos no Grupo Turing, não deixem de seguir as nossas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Até a próxima!"
https://medium.com/turing-talks/construindo-uma-rede-neural-do-zero-pytorch-671ee06fbbe1?source=collection_home---------66----------------------------,Construindo uma Rede Neural do zero | Pytorch,"“Neurons that fire together, wire together”",Enzo Cardeal Neves,1212,9,2020-07-13,"Bem-vindos à mais uma edição do Turing Talks! Nessa semana abordaremos como desenvolver um modelo de Deep Learning usando Pytorch. Para entender bem o post é esperado que o leitor tenha uma boa noção dos seguintes tópicos: Em relação a redes neurais, as seguintes fontes são suficiente para conseguir acompanhar o post: Ou para quem entende inglês, pode assistir a playlist a seguir que é bem didática: Com essas habilidades, vamos fazer um passo a passo do “Hello World” das redes neurais (RNs): a classificação de números escritos a mão. Uma biblioteca open source para criar redes neurais que permite tanto o uso da CPU quanto da GPU para o treinamento do modelo. Por ser mais flexível que o TensorFlow e muito bem documentada, é ótima para se fazer pesquisas (na verdade, é o framework mais usado no meio acadêmico para construir RNs) e para montar a sua primeira rede neural :) Algumas das suas principais características são: Para cada etapa da construção, iremos mostrar a aplicação em código e explicar o que está sendo feito, dessa forma ficará mais fácil de acompanhar o que está acontecendo. Antes de mais nada, você deve instalar a biblioteca, recomendamos que use o Anaconda para isso. Como ambiente de trabalho, utilizaremos o Jupyter Notebook. Primeiramente importamos todas as bibliotecas necessárias. Por ser um dataset tão famoso, o MNIST já vem incluso na biblioteca, então podemos baixá-lo diretamente a partir do import datasets. Um tensor é como se fosse uma matriz, a diferença é que suas dimensões não estão limitadas a 2. Ele pode tanto ter dimensão 0 (escalar) quanto dimensão n. Precisamos converter os dados para tensor, pois é assim que a biblioteca trabalha com as informações. Vamos abrir um dos itens do dataset para termos uma ideia de seu formato. Cada item possui uma imagem e uma etiqueta, este ultimo é o digito que a imagem representa. Analisando a dimensão do tensor imagem e do tensor etiqueta(um número de 0 a 9): O tensor etiqueta não possui dimensão nenhuma por ser um escalar. Já o tensor imagem possui 3 dimensões. Por quê? A primeira dimensão é o número de canais da nossa imagem. Como é uma imagem preto e branco, possui apenas um, que diz respeito a intensidade do preto em cada pixel. Imagens coloridas costumam ter 3 canais (vermelho, azul e verde). As duas ultimas dimensões representam a quantidade de pixels, nesse caso 28 x 28 = 784 pixels. Sendo assim, para o Pytorch, cada imagem é uma “matriz” (na verdade é um tensor, nunca se esqueça) 28 x 28 com valores para cada elemento variando de 0 a 1 de forma continua. Uma boa prática é construir uma classe para conter a estrutura da nossa RN. Essa modularizarão facilita adaptações futuras no nosso modelo. Por sorte, o Pytorch possui uma superclasse que auxilia nessa tarefa, a nn.Module. A rede que queremos montar é a seguinte: Na camada entrada temos 28 x 28 = 784 neurônios, um para cada pixel da imagem. Esses neurônios irão receber o valor entre 0 e 1 de acordo com a intensidade do preto no pixel correspondente. Nas camadas internas 1 e 2, temos 128 e 64 neurônios, respectivamente. Essa escolha foi arbitrária e, ao final do post, veremos que foi boa o suficiente para conseguirmos um resultado satisfatório. Utilizaremos a ReLU como função de ativação das camadas: entrada → interna 1 e interna 1 → interna 2. Essa é uma função do seguinte formato: f(y) = y, se y>0, caso contrário, f(y) = 0. Por ser um problema de classificação, no tensor devolvido pela camada saída(10 valores entre 0 e 1, cada um correspondendo a chance, segundo o modelo, de a imagem fornecida ser o número do índice correspondente, como exemplificado na imagem abaixo), aplicaremos a função LogSoftmax, que devolve um tensor com as mesmas dimensões da saída. A partir deste tensor e do target(nesse caso é a etiquete pois ela define qual o digito que a imagem sendo analisada representa), aplicamos a função negative log-likelihood loss(NLLLoss) e calculamos a perda. A lógica é a mesma do erro quadrático médio(MSE), quanto mais próximo de zero, melhor. Definimos então a classe Modelo: A função forward(self, X) nos retorna o tensor que será utilizado para calcular a perda. Isso é o que o modelo retorna. Repare aqui que todas as funções que utilizamos são do import torch.nn.functional as F, funções especiais para trabalhar com tensores, e o tipo das camadas foram definidos a partir do import nn. Utilizamos camadas lineares por simplicidade. Para classificação de imagens, o ideal seria também utilizarmos camadas de convolução 2D. Vamos agora definir a estrutura de treino do modelo. As principais etapas são: Graças ao Pytorch, para defini-las será um passeio no parque. A partir dos grafos computacionais dinâmicos, por padrão o Pytorch armazena uma espécie de histórico das operações realizadas. No exemplo a seguir isso fica mais claro. É possível calcular as derivadas parciais da loss = ((y-(a+b*x))**2).mean() em relação a a e b(caixas azuis) já que foi armazenado a “história” dessas variáveis. Assim, calculamos o gradiente facilmente, pela regra da cadeia, da perda em relação aos pesos e bias. CALMA! a biblioteca fará todo esse processo para você. A primeira vista você pode pensar em atualizar os pesos e as bias a partir de um loop, calculando cada elemento da seguinte forma: a = a - lr*grad (onde lr é a taxa de aprendizado e grad é a derivada parcial da perda em relação a a). Isso não é necessário. A partir do import optim, podemos escolher qual otimizador usar para fazer essas atualizações. Nesse caso vamos usar o SGD (Stochastic Gradient Descent). Seus parâmetros principais são: Definimos então a função treino: Explicando sucintamente o que está sendo feito: Vamos agora definir a função validacao. Como esse não é o foco principal do post, deixaremos as explicações apenas nos comentários do código. Primeiro vamos inicializar o modelo Colocamos para rodar na GPU(se possível) para demonstrar essa funcionalidade. Claro que depois de todo esse trabalho, queremos guardar a evolução da RN. Para visualizar a uma predição aleatória do set de validação, você pode executar a célula a seguir no seu notebook: Com esse post, construímos uma base boa para conseguirmos treinar Redes Neurais mais complexas utilizando o Pytorch sem grandes dificuldades. Apresentamos as principais características da biblioteca mas sem entrar muito nos detalhes já que nosso foco foi montar a rede em si. Existem diversos pacotes para facilitar o trabalho com os diferentes tipos de dados, sendo eles: Independente do tipo dos dados, depois de carregados e convertidos para tensores, o processo para montar a RN é bem similar (usando as camadas, otimizadores e função perda mais adequados) com o que fizemos aqui. Outra ferramenta interessante e bem integrada com Pytorch é o TensorBoard, que permite visualizar, a partir de gráficos, como o modelo está evoluindo. Caso tenha interesse em conhecê-la, suas funcionalidades foram demonstradas em um Turing Talks passado. Por fim, esperamos ter tirado um pouco desse misticismo que existe em volta da “buzzword” Machine Learning e mostrar que, com as ferramentas que temos a nosso dispor, qualquer um pode montar uma rede neural do zero. Numa abordagem quase que de tentativa e erro para determinar a estrutura do nosso modelo, conseguimos um preditor com uma taxa de acerto de 97% (o estado da arte para esse problema é 99,79%). Claro que apenas reproduzimos técnicas existente há décadas e, se você quiser se aventurar de verdade por esse mundo, montando modelos com alto grau de complexidade, há uma longa jornada de aprendizado a ser percorrida. Código completo: https://github.com/enzocardeal/clasificacao-de-digito Nossas redes: Facebook, LinkedIn, Instagram Até a próxima! [1] Pytorch documentation [2] Pytorch - Training a classifier [3] CNN with Pytorch for MNIST [4] Pytorch git examples [5] PyTorch for Deep Learning: A Quick Guide for Starters [6] Understanding PyTorch with an example: a step-by-step tutorial"
https://medium.com/turing-talks/como-fazer-uma-limpeza-de-dados-completa-em-python-7abc9dfc19b8?source=collection_home---------65----------------------------,Como fazer uma limpeza de dados completa em Python,"“Garbage in, garbage out”",Vitoria,1548,11,2020-07-20,"E aí, pessoal! Bem-vindos a mais um Turing Talks! No artigo de hoje “we are going back to the basics” para abordar um assunto que não trata propriamente de IA, mas se relaciona MUITO: Limpeza de dados. Bem, se você ainda é novo no mundo da programação talvez não entenda onde exatamente a limpeza está situada, muito menos qual sua relação com IA, mas digo que eles se relacionam, e muito! Em qualquer área de conhecimento da programação que você esteja, provavelmente vai se deparar com algum problema que requer essa tarefa, já que um profissional dessa área lida com uma quantidade enorme de dados o tempo todo. Por exemplo, quando pensamos em Modelos de Predição, na área de Machine Learning, uma frase muita usada é “Garbage in, garbage out”, isso significa que, afim de se garantir ou melhorar o desempenho do seu modelo, o pré-processamento, ou seja, a limpeza, é essencial. Você pode ver mais sobre o assunto de predição em específico neste post aqui. Certo, mas o que seria limpar seus dados? Quando falamos sobre isso, estamos falando de remover qualquer dado modificado que contenha inconstâncias, esteja incorreto, incompleto, duplicado, formatado incorretamente ou até mesmo seja irrelevante. Mas por que fazer isso? Apenas com dados corretos podemos realizar uma análise completa e confiável, e por consequência, todas as outras etapas que dependem da limpeza e dela, serão feitas corretamente. Afinal, esses erros podem gerar um problema em cascata! Você não quer estar no meio da sua análise e descobrir que, por exemplo, na coluna da categoria gênero, existem três tipos de dados: “Mulher”, “Homem” e “Criança”, já que, afinal, “Criança” não é um gênero! Isso seria um exemplo perfeito de dados incorretos. E acredite, isso acontece frequentemente, já que a coleta dessa informações pode ser feita incorretamente. Para este artigo, decidimos limpar e analisar um arquivo com dados relacionados a músicas do spotify. Porém, não pegamos o arquivo original e sim uma versão modificada pelo Grupo Turing, assim podemos limpá-lo e analisá-lo de forma mais abrangente. Você pode encontrar e baixar o arquivo neste link. E clicando aqui você pode ver a descrição das colunas assim como suas respectivas definições. Essas informações foram retiradas desta imagem com tradução livre. Então vamos começar! Primeiro, importaremos as bibliotecas necessárias. Para isso, vamos utilizar os quatro cavaleiros do apocalipse de uma limpeza bem feita: Pandas, Matplotlib, Seaborn e Numpy. Com nossas bibliotecas já importadas é hora de ler nosso arquivo. Como você deve ter notado o arquivo está no formato .csv, uma maneira econômica de guardar dados em tabelas. A biblioteca pandas possui uma função específica para ler esse tipo de arquivo e transformá-lo em um DataFrame, chamada .read_csv(). Então vamos ler e depois visualizar nossos nos com a função df.head(). Aqui, também vamos mudar a ordem do nosso df de acordo com a popularidade, assim podemos ver mais claramente alguns erros. Fazemos isso com a função .sort_values que ordena os valores de uma coluna e que requer três argumentos: 1) a coluna pela qual você quer ordenar; 2) o ascending que indica se será ordenado do menor para o maior ou vice-versa — aqui definido como False, já que não queremos que seja ordenado dessa forma e sim do maior para o menor; 3) o inplace definido como True, para que as mudanças feitas nessa função sejam aplicadas ao dataframe original e não apenas a uma cópia. Vamos ver como fica: Observem quantas vezes a música ‘I love it’ aparece, isso é estranho, certo? Também há uma música classificada como ‘não_sei’, e várias colunas com unidades de medida como ‘mol’ ou ‘kg’ — vale lembrar que elas foram alteradas, como falamos anteriormente. Todos esses erros que saltam aos olhos são a razão pela qual a limpeza do nosso DataFrame é tão essencial. Antes de partirmos para a limpeza, vamos analisar cada uma das nossas features (a.k.a colunas). Dando uma olhada nos nomes conseguimos ter uma ideia sobre o que se trata cada uma delas, mas não podemos obter todas as informações, afinal algumas possuem uns nomes bem estranhos. Por isso, podemos pesquisar mais detalhadamente a respeito de cada feature do nosso dataset (essa parte já foi feita para você e o link está no começo do texto na parte de leitura do df). Nosso dataset contém as mais diversas informações sobre cerca de 19000 músicas distribuídas entre 15 features. A única coluna em forma de string é a song_name, enquanto as outras são numéricas. Bom, para começar vamos ver algumas funções que vamos usar em toda nossa análise, a nossa tríade sagrada: Essas funções são muito importantes justamente porque servem como uma bússola para nos orientar, ou seja, para sabermos se estamos ou não indo para o caminho certo na limpeza. Sem uma orientação isso acaba ficando um pouco complicado, pois na limpeza de dados não existe um gabarito, você que deve perceber o que está ou não errado e o que talvez não seja conveniente para o tipo de tarefa que você quer realizar. A df.info()é a nossa queridinha, ela retorna algumas observações que podem ser muito úteis, já que analisa coluna por coluna, mostrando a quantidade de dados nulos do DataFrame, número total de entradas e datatypes de cada feature. Só com essas informações você já sabe para onde caminhar com sua análise. Por exemplo, observe que a maioria das colunas estão armazenadas como ‘object’ — que é como o pandas guarda informações de texto — porém, boa parte dessas colunas são dados numéricos, portanto já podemos perceber que algo está errado com nossos datatypes, vamos explicar mais para frente como corrigir esses erros. E agora, finalmente, essa função maravilhosa que nos dá mais detalhes sobre informações numéricas, tal como contagem, valores dos quartis, média, desvio padrão, máximo e mínimo. Todas essas informações podem ser usadas para descobrir coisas estranhas que podem estar aparecendo no seu dataframe. No nosso caso, colunas com os valores tabelados entre 0 e 1 com algo fora desse intervalo é automaticamente algo a ser corrigido e o describe nos ajuda nessa tarefa. Uma observação importante é que esse método pode receber como parâmetro o datatype com a especificação ‘include’, podendo ter informações mais específicas se estivermos trabalhando com categorias por exemplo. Algo que devemos nos atentar durante nossa limpeza são as duplicatas do df. Resumidamente, duplicatas são as linhas dentro do nosso dataframe que foram clonadas, ou seja, existem duas linhas com exatamente o mesmo valor para cada uma das features. Isso significa que essa informação não está agregando em nada para o nosso modelo, pois é uma mesma observação feita de maneira repetida. Dados duplicados devem ser removidos do nosso dataframe, pois podem atrapalhar nossa análise e todos os passos que dependem dela. No nosso df em questão os dados estão BEM poluídos por duplicatas, podemos jogar fora essas ocorrências de repetições pois elas realmente não importam já que são apenas observações repetidas, portanto, podemos excluir utilizando apenas uma linha de código. Porém, é importante deixar a primeira ocorrência da repetição para não desperdiçarmos dados. Podemos fazer isso com o argumento keep = ’first’ da função drop_duplicates. Vamos ver um exemplo: Certo, conseguimos sobreviver às duplicatas, e agora? Para o próximo tópico vamos tratar das inconsistências, primeiramente vamos remover as coisas mais grotescas, como as unidades de medidas inadequadas ‘mol/L’ E ‘kg’. Vamos fazer isso com uma função: Certo, em seguida vamos usar np.nan para declarar valores faltantes nos nossos dados, o NaN, que significa Not a Number, é um valor especial definido no numpy que pode decodificar um valor faltante, mas ainda assim ser lido como um numeral, pois é definido como float. Dessa forma, é interessante trocar tanto dados faltantes sem uma formatação (‘nao_sei’) quanto alguns valores que imediatamente não fazem sentido para nossa análise pelo NaN, isso pode ser feito pela função replace do pandas: Observe que vários destes valores foram achados um tanto empiricamente, por exemplo ao tentar plotar um gráfico podemos perceber esses valores destoante dos outros, ou ainda simplesmente pelo tipo de dado que estamos trabalhando, como a coluna ‘time_signature’, por exemplo, que é uma medida quantitativa de quantos pulsos temos por unidade de tempo musical, sendo inviável ser um valor “quebrado” ou tão grande quanto 2800000000. Bom, o próximo passo é ajustar os tipos das nossas variáveis, geralmente pode acontecer de colunas que deveriam ter apenas algum tipo específico de dados estarem armazenadas com o tipo de variável incorreta. Como por exemplo, uma coluna com dados numéricos deve ser armazenada como int ou float, e não como object, que é como o pandas guarda strings (ou seja valores de texto). Em resumo, nossas colunas devem estar com os dtypes condizentes, o que claramente não é o caso do nosso Dataframe. A maioria das nossas informações numéricas estão sendo tratadas como strings e não queremos isso, vamos então construir uma função para arrumar: Podemos checar se deu tudo certo usando a função info() novamente e verificando os data types. Agora vamos tratar de outliers, que são dados que fogem muito do nosso padrão e que por conta disso acabam dificultando o processo de generalização do seu modelo de predição. Podemos pensar, por exemplo, na seguinte situação: queremos generalizar a renda média de cada pessoa em uma determinada região e temos acesso a uma amostra para fazer um levantamento de dados, se o Bill Gates estiver no meio, provavelmente, ele será um outlier, pois ele vai “puxar” a média do nosso grupo muito para cima, o que complicaria nossa análise, podendo comprometer observações e até agravar insights. Em suma, outliers são valores que desequilibram o dataframe e comprometem a capacidade de extrair insights com base nos dados: O primeiro passo para saber se um dado é ou não um outlier é o bom senso.Isso pode parecer um pouco vago em determinados casos, porém o bom senso é o primeiro caminho que você deve tomar ao abrir um novo dataset, sendo assim é crucial conhecer a história por trás dos seus dados, seja de onde ele foi retirado, as fontes por trás, até talvez qual o parâmetro de medição de alguma feature. Pesquisar sobre o assunto que você está trabalhando também é um trabalho importante para um data scientist. Claro que algumas informações podemos extrair diretamente do nosso conhecimento de mundo, como idades que sempre são números maiores que 0 e dificilmente maiores que 110, no caso do nosso dataset, por exemplo, estamos trabalhando com músicas, então já podemos presumir que o tempo de uma música deve estar em torno de 2,5 a 4 minutos, ou seja, dados fora desse intervalo provavelmente são outlier. No nosso caso até temos um facilitador, pois várias das nossas features são medidas entre 0 e 1, podendo excluir automaticamente qualquer coisa fora desse intervalo. A segunda forma de detectarmos outliers é através de métodos estatísticos (tava demorando para aparecer uma matemática né heheh). Bom, para isso, vamos dar uma olhada nos intervalos de confiança de uma normal, que é geralmente como nossos dados aparecem distribuídos: Dessa forma, para termos um intervalo de confiança que exclua apenas os outliers, vamos trabalhar com dados que estão no intervalo: Portanto, abrangemos a grande maioria dos nossos dados e removemos os que destoam do normal. Então vamos fazer uma função que faça esse processo de remoção automaticamente: Agora vamos separar nossas colunas numéricas e remover os outliers delas aplicando a função e trocando esses valores absurdos por NaN (not a number) que representa valores faltantes: Algumas colunas podem ser consideradas desnecessárias para nossa análise, isso porque elas não nos passam informações relevantes a respeito do que queremos descobrir, ou até mesmo porque possuem tantos dados faltantes que mais atrapalham do que ajudam. Nesses casos uma forma rápida e fácil de solucionar esse problema seria excluí-las. Mas será que essa é a melhor solução? A resposta mais segura para isso é: depende. Isso porque essa decisão pode variar conforme alguns aspectos e você precisa ter certeza absoluta de que aqueles dados que vai excluir não farão falta — e nem sempre podemos ter essa certeza. Vamos abordar uma forma de lidar com dados faltantes mais abaixo. Nesse DataFrame em questão, não vamos excluir nenhuma coluna porque para nosso próximo post de análise todas serão utilizadas. Mas decidimos abordar esse assunto apenas por ser uma parte importante de um pré-processamento bem feito. Por fim, vamos falar sobre dados faltantes ou nulos. Em algumas situações, podemos ter muitas informações incompletas no nosso df, ou, como vimos anteriormente na parte de Inconstâncias, pode acontecer de precisarmos substituir valores errados por NaN. Essas informações faltantes podem prejudicar nossa análise e outras etapas que dependem dela e do pré-processamento, portanto, precisamos removê-los ou substituir esses valores por outros. Vamos ver detalhadamente cada uma das opções: Nossa primeira opção é substituir esses dados pela média da coluna, entretanto, às vezes, a média pode ter sido afetada pelos valores destoantes da coluna, então podemos substituir também pela moda ou mediana. Podemos fazer isso com a função .fillna que preenche todos os campos com dados ausentes. Vamos criar alguns loops como exemplo. O primeiro passa por algumas colunas e substitui os valores faltantes pela moda: Depois, vamos substituir os valores nas outras coluna pela mediana: Em segundo lugar temos a opção mais rápida e prática: excluir as linhas com dados faltantes. Essa opção é boa quando não temos muitas colunas com observações vazias, garantindo assim que não apagaremos muitos dados. Vamos ver um exemplo prático: Muitas vezes parece ser tentador utilizar o comando .dropna, já que ela resolve nossos problemas com dados vazios muito facilmente. Entretanto, o grande problema é acabar perdendo valiosas linhas de informação com isso, portanto, é recomendável tentarmos achar algum método de substituição de valores que consiga preencher esses dados sem comprometer o dataset. Existem várias formas de fazer isso, como as que vimos acima substituindo os valores alterados pela moda, média, mediana. Temos também outros métodos mais sofisticados como estatística de janela, clustering imputer ou random imputer, mas não vamos entrar muito nesse mérito, já que acabaríamos nos estendendo demais e esse tema é tão amplo que merece um Turing Talks só para ele. Nesse Turing Talks abordamos os pontos fundamentais para uma limpeza bem feita, entretanto, para cada conjunto de dados existe uma abordagem que funciona melhor, esse é o tipo de coisa que aprendemos melhor na prática. Portanto, não tenha medo de botar a mão na massa e explorar outros datasets para treinar a limpeza! Semana que vem voltamos com a continuação desse Turing Talks que abordará a próxima parte depois da limpeza: A análise. Até lá! Agradecimentos especiais aos nossos mentores Felipe Azank, Julia Pociotti, Camila Lobianco, Leonardo Muramaki e William Fukushima, que nos ajudaram no que começou como um Mini Projeto e virou um Turing Talks! Obrigado também a Camilla Fonseca e ao Guilherme Fernandes."
https://medium.com/turing-talks/como-visualizar-e-analisar-dados-com-python-f209bfbae68e?source=collection_home---------64----------------------------,Como visualizar e analisar dados com Python,Ollhando para as principais features de músicas do Spotify,Thiago Bopp Resnitzky,1209,14,2020-07-27,"Texto escrito por Thiago Bopp e Guilherme Colasante Sejam bem-vindos a mais uma edição do nosso amado Turing Talks! No texto de hoje, iremos continuar a discussão da semana passada sobre o dataset de faixas do Spotify, com o intuito de elucidar a Visualização e Análise de Dados, dois processos fundamentais na vida de qualquer cientista de dados. Para baixar o dataset do Spotify já limpo para fazer a análise, clique aqui. O primeiro passo para entender qualquer conjunto de dados, é compreender o contexto em que estão inseridos. Para isso, recomendamos que você dê uma breve lida na descrição das features padrão de músicas do Spotify. No mundo em que vivemos, temos acesso a uma quantidade imensurável de dados e conjuntos de dados, cada qual com sua especificidade. Mas a mente humana não é capaz de digerir e processar tabelas enormes se forem representadas de maneiras pouco visuais, mesmo estando repletas de informações valiosas. Logo, fica evidente a importância das ferramentas de visualização, que facilitam nosso entendimento dos dados para tomadas de decisão. Os gráficos são parte essencial da análise, porque, de acordo com o célebre e ancestral ditado popular entalhado nas pedras de ruínas do século quinto antes de Cristo no interior da China: A utilização de gráficos e demais estratégias de visualização facilitam a interpretação e identificação de problemas ou padrões, porém, caso seu gráfico fique esteticamente prejudicado (vulgo feio), é provável que dificulte ainda mais o processo de entendimento. A moral da história é: entre estética e informação, escolha sempre os dois! Nosso artigo fará uso de apenas duas bibliotecas de visualização de dados: Matplotlib e Seaborn. Contudo, não se engane, os quatro cavaleiros do apocalipse estarão sempre presentes na vida de um bom cientista de dados. Apesar de utilizarmos o Matplotlib e o Seaborn para visualização, Pandas e Numpy foram essenciais para a limpeza e tratamento anteriormente. Se você ainda estiver inseguro com os dois primeiros cavaleiros (Pandas e Numpy), dá uma corrida no Turink Talks anterior! Os comandos estruturais da Matplotlib estão relacionados com a formação de figuras, através do plt.figure(), sendo suas dimensões definidas no parâmetro figsize. Além disso, parte importante dos gráficos está na inserção de títulos e legendas. Para a tarefa de inserir um título, basta aplicar o comando plt.title(). Para inserção de legendas, é preciso codar plt.legend(). De maneira geral, o Seaborn foi constituído com base no Matplotlib, com o intuito de simplificar e dinamizar a plotagem de gráficos. Trata-se de uma ferramenta super ágil de visualização! É importante ressaltar que também existem outras bibliotecas de visualização, com diferentes propostas: como a OSMnx, para análise de redes urbanas, e a Folium, para visualização de dados geoespaciais em gráficos interativos. Existem milhares de maneiras de visualizar dados, mas nesse artigo vamos explorar apenas algumas delas. Incentivamos que você explore as documentações do Matplotlib, Seaborn e outras bibliotecas relacionadas! No início, é sempre positivo visualizarmos a distribuição dos nossos dados, com o objetivo de compreender como se comportam. Em modelos de predição, costuma ser preferível que configurem uma distribuição normal. Mas o que raios seria uma distribuição normal!? De maneira simplificada, uma distribuição normal, também denominada distribuição Gaussiana, possui probabilidade normal ou contínua, ou seja, para cada desvio padrão a partir da média, existe a mesma chance de ocorrência do evento. Além disso, a média, a moda e a mediana da distribuição são equivalentes. Para os matemáticos sofredores e interessados, segue a função densidade desse tipo de distribuição. Através do acompanhamento das distribuições das nossas features, é possível determinar se elas aproximam-se de uma normal. No dataset do Spotify, as mais variadas distribuições podem ser encontradas, vamos explorar algumas delas! Para visualizar a distribuição de uma feature, seu melhor amigo é o sns.distplot(), devido à facilidade e praticidade do código. Ele plota diretamente a curva de distribuição sobre o histograma, do qual você ainda pode definir o número de bins (grupos de intervalos idênticos). Caso o histograma não seja necessário, basta definir hist=False. A primeira feature a ser explorada será a danceability, cuja distribuição se assemelha a uma normal, fator muito satisfatório! Outra feature que se assemelha ligeiramente a uma normal é a loudness. Apesar de meio torta, ainda é fácil notar uma certa proximidade de uma curva normal. Contudo, apostamos que a maioria das pessoas que você conhece não são assim tão normais, assim como nossas features. Dá uma olhadinha na instrumentalness. Esse formato de distribuição nos indica uma concentração absurda de músicas de instrumentalness nula. Vamos excluir essa zona de nossa observação por meio do método query e ver o que acontece. Realmente, nem assim as coisas foram facilitadas. Esta é a realidade de grande parte dos dados, pois nem todos os fenômenos possuem natureza normal. Você sabia que, por exemplo, para lançamentos de dados, a distribuição probabilística é uniforme? Além disso, fatores como outliers, distribuições misturadas, insuficiência de dados e coleta incorreta de dados podem levar a resultados distoantes de uma distribuição normal. Contudo, esse não é nosso foco neste momento. Vamos prosseguir com a análise e visualização do nosso dataset! Como complemento à análise das distribuições, é possível analisar a densidade das features, ou seja, as zonas de concentração. Para isso, faremos uso do sns.jointplot(), que possui os tipos scatter, reg, resid, kde, hex. Nesse caso, vamos explorar apenas o scatter e o kde. O scatter é o mais simples dos joint plots e, justamente por isso, podem haver confusões de visualização caso haja uma quantidade muito grande de dados, como no exemplo a seguir. Pela grande quantidade de pontos, eles se sobrepõe de maneira a dificultar nossa percepção da concentração. Por isso, para esse tipo de observação, a melhor técnica é plotar o kde. Aviso: esse gráfico não é um portal para outra dimensão, nem uma piscina, é apenas um gráfico. Basicamente, o gráfico evidencia a concentração de pontos em diferentes níveis numéricos, além de explicitar a curva de distribuição de cada eixo. Nele, podemos visualizar a maior concentração de músicas em níveis de audio_valence entre 0.4 e 0.8 e loudness entre -7.5 e -5. Para entender a dinâmica da análise de correlações , vamos olhar para um par colunas da forma mais simples possível, chamando sns.scatterplot(), função da biblioteca seaborn que simplesmente plota valores de uma coluna no eixo x e de outra no eixo y. Para esse exemplo específico, vamos usar energy e loudness. Para potencializar a visualização, podemos também substituir o sns.scatterplot(), que é mais tradicional, pelo sns.regplot(), outra opção da Seaborn, mas que já mostra uma linha de tendência, calculada por regressão linear, da correlação entre nossas colunas. Lembra que falamos que a Seaborn era construída em cima da Matplotlib? Para mexer em algumas características do sns.regplot(), vamos acessar o plt.scatter() que é a sua base, usando o parâmetro scatter_kws da função. Esse parâmetro recebe informação em formato dicionário. Aumentando ou diminuindo o alpha, por exemplo, é possível alterar a transparência dos pontos e ter melhores noções da concentração dos dados. Mudando color, também diferenciamos os pontos da linha de tendência, o que ainda melhora a estética da visualização. Pudemos identificar pelo regplot() uma correlação positiva entre energy e loudness, observando a reta de tendência crescente (coeficiente angular positivo). Parece fazer sentido, né? Mas claro que demoraria muito para criar um gráfico desses para cada par entre as colunas, e não temos tanta paciência assim… Vamos então chamar sns.pairplot(), outra função que plota de uma vez todos os scatterplots possíveis do nosso dataset. No pairplot não vamos colocar as colunas key, audio_mode e time_signature, pois são categóricas. Elas não agregariam nada nessa etapa específica e tornariam o gráfico ainda maior, dificultando a visualização. Você concorda que o par de uma coluna com ela mesma não agregaria nada na nossa análise? Por esse motivo, nos espaços que seriam ocupados por esses gráficos, se encontram de novo as distribuições de cada coluna, o que torna o pairplot uma ferramenta bastante rica. De início ele parece ser meio assustador, e nesse caso, por trabalharmos com um grande número de colunas, vai requerer um bom zoom na sua tela para possibilitar uma leitura cuidadosa. Ainda não satisfeitos, queremos entender quais são todas as correlações que existem, pois apesar do pairplot trazer boas noções, não é fácil entender a presença e intensidade das correlações por meio dele. Afinal de contas, não devemos depender somente da nossa percepção para achá-las, pois sabemos que o computador erra muito menos nos cálculos. A função sns.heatmap() veio para facilitar a nossa vida! Como no pairplot, ela pareia todas as colunas, mas em vez de nos mostrar gráficos, mostra a intensidade da correlação entre as features, numericamente e pela escala de cores. Se você se interessar pelos cálculos do heatmap(), vale ler sobre o coeficiente de pearson, que define o grau de correlação entre duas variáveis na estatística. Para efeitos práticos, agora só é necessário entender que quanto mais perto o coeficiente estiver de 1, mais positivamente correlacionadas as variáveis estarão. E quanto mais perto de -1, mais negativamente. É importante ressaltar que sempre vamos ir e voltar entre a parte gráfica e numérica da análise, até então representadas pelos gráficos e heatmap. O motivo é a complementaridade entre elas na construção do raciocínio do cientista de dados, nosso grande fio da meada! Vamos então identificar de uma vez as principais correlações do heatmap (as moderadas e fortes, já que no dataset não temos nenhuma muito forte). E olha lá! Uma das mais gritantes é justo o par das famigeradas energy e loudness, que pegamos aleatoriamente no início para o nosso scatterplot! Tá, vai… Podemos confessar que já sabíamos disso quando as escolhemos… Vamos plotar abaixo as três correlações identificadas, numa mesma figura. Podemos concluir por esses regplots que quanto mais acústica é uma música (acousticness), menor a probabilidade dela ter altas energia (energy) e amplitude sonora (loudness). Ao mesmo tempo, quanto mais energética é uma música, mais provável que sua amplitude seja mais intensa. Além das possíveis associações de correlação entre as variáveis numéricas, outras formas de visualização podem e devem ser exploradas. No universo da representação categórica, por exemplo, gráficos de contagem são muito usuais. Os dados categóricos são aqueles com possibilidade de valores finita, ou seja, que classificam as rows em determinadas categorias. No dataset de Spotify, as features categóricas são key, audio_mode e time_signature. O sns.countplot() realiza a contagem dos dados em categorias, como pode ser visto no gráfico a seguir. A partir da análise do gráfico, pode-se notar que existe uma predominância de músicas de audio_mode 1, em detrimento de audio_mode 0, isto é, a maior parte das músicas do dataset possuem escala Maior. Vale destacar que o countplot não se limita à aplicação em variáveis categóricas, ele também pode ser utilizado com sucesso na análise de variáveis numéricas. O showbusiness no mundo da música é cruel, poucos vingam na caminhada para o sucesso. Nesse sentido, seria interessante analisar a distribuição das faixas em termos de popularidade, variável numérica distribuída entre 0 e 100. O gráfico a seguir justifica as falas de renomados cantores que lutam para não se iludir com o sucesso, já que ele é tão raro e passageiro, como nas palavras de MC Hariel “Onde nós passamos, banguelo põe dentadura pra sorrir, mas não me iludo com o sorriso, eu sou mais eu”. Surpreendentemente, existem várias músicas na faixa de popularidade nula, enquanto, nos níveis próximos a 100, quase nenhuma é encontrada, o que reforça o ponto levantado de que o sucesso é uma trilha difícil. A grande maioria das músicas se encontram em um intervalo entre 40 e 70, ou seja, majoritariamente as faixas no Spotify tem baixa ou média popularidade. O modelo da plataforma do Spotify abre portas para bandas e cantores de menor publicidade, que podem facilmente ingressar na rede de conteúdo musical dos mais famosos artistas. Essa diversificação de popularidade permite um maior alcance da plataforma, além de garantir um mix de músicas bem diversificado. Retomando os gráficos que abordam variáveis categóricas, pode-se destacar o swarmplot, também chamado de “beeswarm”. Ele, simplesmente, desenha um scatterplot categórico sem sobreposição de pontos, o que possibilita uma visualização mais harmônica e ilustrativa. A partir do comando sns.swarm plot(), você deve atribuir os eixos x e y e o dataframe utilizado, bem como uma série de outras funcionalidades caso deseje, como paleta de cores, ordem das colunas, entre outras. É possível observar uma concentração clara de pontos posicionados entre 0 e 0.4 de speechiness, independentemente da key. É importante ressaltar que muito poucos pontos se encontram acima do nível de 0.4, o que indica uma presença maior de músicas com alto nível instrumental, em detrimento da menor quantidade de raps e músicas faladas em geral. Ademais, a grande concentração dos pontos posicionados na key 3 entre 0 e 0.2 nos permite inferir que as músicas com maior presença de vocal (rap em geral) se manifestam em menor escala, em Ré Sustenido ou Mi bemol. Geralmente, o swarm plot pode ser complementado pelo sns.violinplot(), para que a distribuição possa ser visualizada mais claramente. O violinplot desempenha um papel semelhante ao boxplot, pois evidencia a distribuição de dados quantitativos em vários níveis de uma (ou mais) variáveis categóricas, para fins de comparação. Apesar de semelhantes, o violin se diferencia de maneira significativa do sns.boxplot(), pois enquanto o primeiro evidencia estimativas de núcleos de densidade da distribuição subjacente, o segundo lida com data points para formação de marcos visuais. Os gráficos evidenciam questões interessantes acerca das variáveis. No violin, nota-se a ausência de músicas com time_signature igual a 0. As músicas de time_signature 1 e 3 apresentam grande semelhança de distribuição. Apresentam dois picos de distribuição bem definidos: no intervalo (0,0.25) e no intervalo (0.75, 1). As músicas de time_signature 4 estão concentradas na faixa de 0 de acousticness com grande intensidade. Apesar de tal padrão de concentração se dar com as músicas de time_signature 5, ele ocorre de maneira significativamente menos intensa. Através da análise do boxplot, é possível visualizar grandes semelhanças entre os dois grupos de audio_mode. Em ambos, o box de time_signature 4 possui o maior valor de tempo, com a presença de outliers em um contexto de audio_mode 0. Pode ser notada, ainda, uma semelhança grande entre a mediana dos boxplots de time signatures 3 e 4, com o terceiro quartil do 3 superior ao terceiro quartil do 4 em ambos audio_mode=1 e audio_mode=2. É possível visualizar dados categóricos até em plots onde os dois eixos são numéricos. Essa mistura advém principalmente da utilização dos parâmetros size e hue, os quais proporcionam, respectivamente, a alteração do tamanho dos elementos gráficos e a sua coloração para uma divisão categórica em meio a uma análise numérica. Algumas análises podem ser facilitadas a partir da observação de amostras menores de um determinado dataframe, pois a segmentação permite mais aproximação com os dados utilizados e um maior nível de personalização. Você pode ter milhares de conhecidos, mas os parceiros de verdade você conta em uma mão. Com essa filosofia em mente, isolamos uma parcela do dataframe, contendo as 50 músicas mais populares. A partir dela foi traçado um sns.relplot() no formato scatter com diferenciação de cor baseadas na time_signature. É possível visualizar a preponderância clara de músicas com time_signature igual à 4 entre as mais populares, ou seja, maior a probabilidade da música alcançar um alto grau de popularidade caso ela possua uma time_signature equivalente à 4. Também é perceptível no gráfico uma correlação linear fraca entre audio_valence e danceability, ou seja, o nível de danceability da música não possui relação direta com o grau de felicidade que ela proporciona. Apesar de ser perfeitamente aplicável para variáveis categóricas, pode-se destacar que a técnica de utilização do hue também pode ser explorada com variáveis numéricas, como no exemplo a seguir. No gráfico em questão, nota-se a já explorada correlação entre loudness e energy, além de informações adicionais proporcionadas pelo hue e size. Nota-se que o gráfico fica mais escuro à medida em que aumentam as variáveis loudness e energy. Nesse sentido, pode-se depreender que altos níveis de energy e altos níveis de loudness podem ser um bom indicativo para um maior nível de audio_valence (nível de felicidade ocasionado pela música). Quanto ao tamanho, representante da variável instrumentalness, não é possível traçar nenhuma relação conclusiva, visto que os pontos de mesma proporção estão significativamente difusos ao longo do plot. A beleza da visualização de dados é o espaço para criatividade. No livro “Storytelling with data, a data visualization guide for business professionals”, Cole Nussbaumer Knaffic afirma que os seres humanos não são naturalmente bons na construção de gráficos, pois nossa formação não explora de maneira satisfatória a união entre o universo das palavras e dos números, o que prejudica nossa capacidade de contar uma história através dos dados. Somente a prática pode tornar alguém proficiente na construção de ferramentas de visualização. Continue praticando, dataframe após dataframe! Sempre existem maneiras de melhorar a visualização de dados. Não existe uma resposta certa, mas, quanto mais prática, melhores serão as suas respostas. Nossas redes: Facebook, LinkedIn, Instagram Agradecimentos especiais aos nossos mentores Felipe Azank, Julia Pociotti e Gabriel Mossato que nos ajudaram no que começou como um Mini Projeto e virou Turing Talks!"
https://medium.com/turing-talks/como-avaliar-seu-modelo-de-regress%C3%A3o-c2c8d73dab96?source=collection_home---------63----------------------------,Como avaliar seu modelo de regressão,As principais métricas para avaliar seus modelos de regressão,Felipe Azank,1403,9,2020-08-03,"Imagine que você, preocupado com os casos de dengue endêmicos na sua cidade e região, decide montar um modelo que prevê o número de casos dessa doença em cada semana, de forma a auxiliar o sistema de saúde na estratégia de prevenção. Agora pense na situação em que você, como estagiário de uma startup do setor imobiliário, foi designado para criar um modelo para precificar apartamentos em São Paulo com base na área, localização, número de banheiros e proximidade de centros comerciais. Como você avaliaria se estes modelos estão bons ou ruins? Você usaria a mesma métrica para os dois modelos? O que seria pior? Termos poucos resultados muito discrepantes ou muitos resultados com poucas discrepâncias? Esse Turing Talks tem como objetivo elucidar essas dúvidas, apresentando as principais métricas, suas aplicações e benefícios. Em específico, vamos seguir a seguinte estrutura: As métricas vistas em nosso último post sobre avaliação de modelos tratam-se de ferramentas para avaliar problemas de classificação, casos em que deve-se prever a categoria de um data point desconhecido. Contudo, uma tarefa também muito recorrente na área de Ciência de Dados consiste em produzir modelos de regressão, no qual é predito uma variável numérica contínua. Veremos agora as principais métricas utilizadas para avaliar a performance de regressões. Para que isso ocorra, partimos de um exemplo em que criamos uma regressão linear multivariável bem simples para prever o preço de determinadas casas em Boston, partindo de parâmetros simples como número de quartos, criminalidade na região e distância de centros comerciais famosos. (O dataset e a descrição detalhada de suas features podem ser encontrados aqui) Muito utilizada em modelos de regressões da área de finanças, o R-Quadrado, ou Coeficiente de Determinação, é uma métrica que visa expressar a quantidade da variança dos dados que é explicada pelo modelo construído. Em outras palavras, essa medida calcula qual a porcentagem da variança que pôde ser prevista pelo modelo de regressão e, portanto, nos diz o quão “próximo” as medidas reais estão do nosso modelo. O valor do seu R-Quadrado varia de 0 a 1 e geralmente é representado em porcentagem. Por exemplo, um R² = 75% nos diz que 75% da variância de nossos dados podem ser explicados pelo modelo construído, enquanto os outros 25%, teoricamente, se tratariam de uma variância residual. Podemos ver a fórmula dessa medida abaixo, na qual ŷ representa o valor predito, y_barra representa o valor médio das amostras e y representa o valor real. Na imagem acima, podemos ver que se o modelo (reta) não se distancia muito dos dados, temos um valor de R-Quadrado alto (gráfico 1), em contraste com o gráfico 2. Essa métrica, apesar de conseguir identificar algumas relações lineares entre o modelo de regressão e os dados, apresenta uma série de desvantagens e limitações, entre elas: Mesmo sem possuir um potencial grande em avaliações de modelos de regressão, o R-Quadrado continua sendo uma métrica muito importante para análises estatísticas tanto no âmbito profissional, quanto no acadêmico. No setor financeiro, o R-Quadrado costuma ser utilizado para identificar o quão relacionada a performance de um portfólio está quando comparada com um Benchmark determinado. OBS: Apesar de relacionados, o R-Quadrado não é o valor Beta popularmente conhecido na área de finanças. Se quiser se aprofundar no assunto, veja aqui. Tendo em vista as inúmeras desvantagens acerca do R-Quadrado, foi necessário desenvolver uma alternativa mais versátil e que não trouxesse um viés em suas medidas, assim foi criado o R-Quadrado Ajustado. Partindo do mesmo princípio do R-Quadrado, essa métrica busca representar a porcentagem da variança que pode ser contemplada pelo modelo de regressão. Entretanto, esse valor não demonstra um viés devido ao acréscimo de dados ou features no modelo, como ocorria com o Coeficiente de Determinação. Isso se deve pelo fato de penalizarmos (reduzirmos) o valor caso uma feature presente não contribua significativamente para o modelo, o que pode ser entendido analisando a fórmula. Em que N representa o número de amostras, enquanto p representa o número de features (dados de entrada do modelo). Podemos perceber que, quanto mais features utilizadas sem aumentar significativamente o valor de R², menor será nosso R-Quadrado Ajustado, o que nos garante uma medida menos enviesada e sempre menor do que o R-Quadrado. Por compreender a entrada de mais variáveis, o R-Quadrado Ajustado transpõe algumas desvantagens do R-Quadrado Mesmo apresentando esse novo ajuste, as métricas até agora apresentadas costumam serem mais utilizadas para avaliar relações e modelos mais simples e, em grande maioria, lineares. Isso ocorre uma vez que essas medidas são calculadas partindo de princípios não totalmente verdadeiros (como a ideia de que o erro total entre os dados reais e os valores preditos são dados pela soma do erro total e de um erro “residual”). Métrica mais utilizada, o Erro Quadrático Médio consiste na média do erro das previsões ao quadrado. Em outras palavras, pega-se a diferença entre o valor predito pelo modelo e o valor real, eleva-se o resultado ao quadrado, faz-se a mesma coisa com todos os outros pontos, soma-os, e dividi-se pelo número de elementos preditos. Quanto maior esse número, pior o modelo. Essa métrica apresenta valor mínimo 0, sem valor máximo, e pode ser descrito pela fórmula a seguir: Uma vez que essa métrica eleva o erro ao quadrado, predições muito distantes do real aumentam o valor da medida muito facilmente, o que a torna uma métrica de avaliação excelente para problemas nos quais grandes erros não são tolerados, como é o caso de exames médicos e projeções de preços. Entretanto, um ponto negativo do uso dessa métrica é sua falta de interpretabilidade direta, uma vez que, para a predição de valores de unidade u, a unidade do MSE seria u². Tendo em vista essa diferença de unidades, o RMSE entra como uma forma de melhorar a interpretabilidade da métrica, acertando a unidade. Entretanto, essa medida, assim como o MSE, penaliza predições muito distantes da real. O Erro Absoluto Médio consiste na média das distâncias entre valores preditos e reais. Diferentemente do MSE e do RMSE, essa métrica não “pune” tão severamente os outliers do modelo. Essa medida apresenta valor mínimo 0 e não apresenta valor máximo: Pelo fato de não elevar as diferenças ao quadrado, essa medida torna-se uma opção não tão ideal para lidar com problemas delicados. Contudo, é uma métrica sólida para modelos que devem prever muitos dados ou dados sazonais, como em previsões de números de casos de doenças, nas quais prever a tendência e sazonalidade dos números é mais importante do que os valores absolutos de cada dia. Outro ponto positivo que pode ser destacado, e que também o difere do MSE, seria sua interpretação mais intuitiva, com a mesma unidade dos valores trabalhados. Em contraste com as métricas anteriores, essa medida exprime uma porcentagem, obtida através da divisão da diferença entre predito (ŷ) e real pelo valor real (y). Assim como o MSE e o MAE, quanto menor o valor, mais preciso seria o modelo de regressão. Por se tratar de uma porcentagem, essa métrica torna-se extremamente intuitiva, tanto para a interpretação do programador, quanto para a comunicação de resultados com pessoas sem conhecimento técnico. Por exemplo, ter um MAPE=12% significa que, em média, nosso modelo faz previsões que erram por 12% do valor real. Devido a sua formulação, essa métrica não lida tão bem se tratando de problemas com um grande alcance de números, como uma regressão que prevê uma variável que vai e 10 a 20.000 Essa métrica, apesar de apresentar uma fórmula um pouco mais extensa, realiza um cálculo similar ao do RMSE. Contudo, a aplicação de logaritmos se dá no objetivo de evitar a penalização de diferenças elevadas entre valor predito e real quando ambos os valores são muito grandes. Como resultado das diferenças entre o RMSLE e o RMSE, teremos os seguintes fenômenos : Tendo em vista as métricas mostradas acima, podemos extrair alguns discernimentos. Ufa! Após uma longa jornada por diversas métricas de avaliação, chegamos ao fim de mais um Turing Talks! Esperamos que vocês tenham gostado do texto! Se quiserem conhecer um pouco mais sobre o que fazemos no Grupo Turing, não deixem de seguir as nossas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Para acompanhar também um pouco de nosso projetos, acesse nosso GitHub. Até a próxima."
https://medium.com/turing-talks/como-analisar-a-efici%C3%AAncia-dos-seus-algoritmos-5730a6cb04e9?source=collection_home---------62----------------------------,Como analisar a eficiência dos seus algoritmos,As principais notações e exemplos de eficiência de algoritmos,Camila Lobianco,1500,6,2020-08-12,"Olá, humano (ou não). Bem-vindo à mais uma edição do Turing Talks! Hoje a gente vai falar sobre um assunto muito polêmico, como você pode ver pelo título. É a hora de pegar aquele seu código final, que parece resolver todos os problemas do mundo, e analisar se você está realmente fazendo a coisa do melhor jeito possível ou se tem como deixar ainda mais eficiente. Para isso, vamos começar pelo método mais clássico: analisar a eficiência temporal de algoritmos de ordenação. Esse é um tópico muito importante, uma vez que pode ser aplicado em muitos problemas, mas também é fundamental para ver que, embora todos façam a mesma coisa, não necessariamente tem a mesma eficiência. Uma observação importante antes de começar realmente o trabalho bruto, é que nem sempre um algoritmo ser mais lento é uma coisa negativa. Algumas vezes, algoritmos mais eficientes na questão do tempo demandam uma capacidade computacional (principalmente os recursivos)tão maior que nem compensa deixá-lo mais eficiente. Outro fator também é que, em algumas áreas como segurança da informação, pode ser necessário o uso de algum algoritmo menos eficiente, uma vez que normalmente eles são mais difíceis de serem “quebrados”. Outro detalhe é que eficiência temporal de analisar dados não vem sem consequências, mas logo nós vamos chegar nessa parte. Para análise de algoritmos, a gente se utiliza de uma notação com o nome de Big-O (ou, como é conhecido no Brasil, Grande-O ou ordem). Na matemática, isso basicamente serve para descrever o comportamento de funções quando seu argumento tende a um valor específico ou ao infinito. Nesse caso, para facilitar as contas, nós usamos a tendência para o infinito. Na computação, a ideia é semelhante: é uma notação para classificar o comportamento de determinados algoritmos se comportam diante mudanças no tamanho da entrada. A notação do Big-O mostra a característica das taxas de crescimento. Funções diferentes podem ter a mesma notação Big-O, basta que elas tenham o mesmo comportamento. A letra O é escolhida porque ela determina a ordem de uma função, prevendo um limite superior para a taxa de crescimento da função (o que poderia acontecer no pior dos casos). Mas, para ilustrar melhor isso, vamos dar uma olhada em alguns tipos de estruturas e ver como elas se comportam. Recomendo que, antes de qualquer coisa, você tente pensar por si mesmo o motivo dessa função ter esse comportamento. Mas antes, vamos ter uma noção das possibilidades que temos nessa análise: Agora, vamos para a parte que interessa, os cálculos: Constante (O(1)) Começaremos por uma ideia bem básica. Começaremos analisando a quantidade de operações que estão acontecendo (lembrando que a função principal sempre será chamada antes das funções descritas no código). Independente do dado que vamos colocar, seu retorno será sempre a mesma coisa: o dado de entrada. Ou seja, tendo um dado ou 10.000 dados, sempre o mesmo número de operações irão acontecer (nesse caso, serão 1+1+1 = 3 operações). Essa é a característica principal da função constante: independentemente da quantidade de dados, haverá a mesma quantidade de operações. Esse exemplo é um que não é muito recorrente no mundo da programação. Basicamente, ele aparece mais quando estamos trabalhando com determinadas estruturas como Árvore Binária ou Heap, que serão melhor analisados em uma parte futura dos posts. Mas vamos analisar melhor um algoritmo com essa complexidade. Essa é a mesma lógica do linear, que será o próximo a ser explicado. Como podemos ver, como nos anteriores, o que realmente determina o comportamento do algoritmo é o laço. O que diferencia esse caso dos outros é o fato que o número de operações estará diretamente ligado ao logarítmo na base 2. Para ilustrar isso de uma forma mais adequada, vamos contar as operações em dois casos. Então se temos o logaritmo de 8 na base 2, teremos 2(logn) + 3 operações. OBS: Vale ser ressaltado que, em função das estruturas que serão futuramente introduzidas, a base do logarítmo vai ser 2. Linear (O(n)) Vamos analisar o seguinte código: Vamos contar quantas operações isso tem. Esses +5 acontecem todos porque o vetor tem 5 elementos. Agora vamos supor que o array tem um número n de elementos. Teremos um total de operações de 1 + 1 + n + n + n + 1. Ou seja, 3n + 3 operações são feitas nesse código para uma array de tamanho qualquer. Nesse caso, podemos ver que, quanto maior a quantidade de dados, a quantidade de operações a ser feita aumenta de forma linear. Nessa notação, pegamos o comportamento da função e colocamos sua complexidade em função do seu n. Nesse caso, a notação ficaria (O(n)), pois é linear. Agora, por que ignoramos o 3 que multiplica o n e o 3 que soma? Simplesmente porque contar cada detalhe é extremamente inviável para algoritmos muito longos, então a nós resumimos ao que será mais afetado. Por exemplo, vamos supor que eu tenha um vetor de 100000000 elementos. Nesse caso, teríamos 300000003 operações. Por outro lado, com 10 dados, temos 33 operações. O 3 multiplicando e somando não faz tanta diferença, o que realmente aumenta o tempo de execução do programa é o n. Esse é, provavelmente, o modelo mais importante de ser analisado, uma vez que ele é o que aparece nos mais eficientes algoritmos de ordenação. Ele é uma alternativa para evitar algoritmos de eficiência quadrática, utilizando funções recursivas. Ele é comum em estruturas como Árvore Binária e Heap. No momento, vamos adiar um pouco da análise desse modelo para os próximos posts, uma vez que normalmente ele está ligado a estruturas um pouco mais complexas, que fazem muito mais sentido quando contextualizadas. Então temporariamente vamos pular essa e analisar o próximo modelo de algoritmo. Por mais que aqui venhamos a analisar apenas o modelo quadrático, a lógica se segue para todos os modelos polinomiais, inclusive n³, n⁴,… Diante disso, vamos analisar apenas o código a seguir. Sugiro que você tente primeiro tentar pensar sozinho e depois leia a análise conjunta. Somando, teremos um total de 20 operações. Generalizando, a fórmula geral será do tipo 2n²+2, que é exatamente o resultado encontrado quando n=3. Caso tivêssemos mais um laço encaixado, teríamos a complexidade de 2n³+2 e assim por diante. Vamos deixar esses de lado um pouco, porque eles são utilizados normalmente em algoritmos mais complexos, o que ficará para um post no futuro. Uma curiosidade sobre esses algoritmos é que eles são usados na área de segurança justamente por serem muito ineficientes (e portanto dificultar para que certos processos sejam realizados). Caso você esteja curioso para ver como isso funciona já agora, dê uma olhada em algoritmos que envolvam árvore binária ou espere pelos próximos posts de algoritmos de ordenação com esse tipo de complexidade. Depois de todo esse trabalho, agora temos uma noção básica de como calcular alguns algoritmos mais básicos. Mas essa é só a ponta do iceberg! Nos próximos posts vamos mostrar algumas aplicações mais complexas para mostrar a importância desse tema e como podemos aplicar ele nos algoritmos do dia-a-dia. Semana que vem, vamos ver mais sobre Bubble Sort, Selection Sort e Random Sort. Para mais informações, acompanhe o Grupo Turing no Facebook, LinkedIn, Instagram e nossos posts do Medium."
https://medium.com/turing-talks/introdu%C3%A7%C3%A3o-%C3%A0-vis%C3%A3o-computacional-b13698774adc?source=collection_home---------61----------------------------,Introdução à Visão Computacional,Entendendo como os computadores enxergam,Eduardo Eiras de Carvalho,1547,7,2020-08-16,"Bem vindo a mais um Turing Talks! Dessa vez iremos introduzir uma área ainda não muito abordada nos nossos últimos posts, a Visão Computacional (às vezes abreviada por CV, pelo termo em inglês Computer Vision), ou “Como as máquinas enxergam?”. A área de Visão Computacional se tornou uma das áreas mais exploradas em Computação e Inteligência Artificial, mas o que constitui essa área e por que é tão difícil de um computador extrair as mesmas informações de uma foto que um olho humano? O sistema visual Humano não tem qualquer dificuldade interpretando a variação de luminosidade, sombras ou até mesmo diferenciando o tigre acima do resto da imagem. Para o computador, no entanto, a figura acima é apenas um monte de zeros e uns, diferente da nossa capacidade de percepção tridimensional. Esta última década foi muito importante nos avanços em como fizemos computadores interpretarem imagens, e ainda falta muito para descobrir até onde conseguiremos ir. Visão computacional é a área que estuda como os computadores “veem” e entendem imagens e vídeos digitais, desde manipulação de imagens até extração de informações. As principais aplicações da área incluem: Hoje em dia até seu celular muito provavelmente possui diversos algoritmos de visão computacional para melhorar sua experiência e o jeito de capturar fotos e vídeos. Hospitais e laboratórios desenvolvem diversos estudos em reconhecimento de doenças através de imagens e um dia seu carro irá dirigir sem você nem mesmo encostar no volante, pelo menos segundo Elon Musk. No começo da área, perto de 1960, os primeiros dias de seu estudo focavam em uma tentativa de imitar a visão humana e fazer a “simples” pergunta à um computador: “O que você está vendo nessa imagem?”, na tentativa de automatizar o processo de análise. Essas tentativas iniciais foram as precursoras do reconhecimento de imagem, já que antes tudo era feito através de sensores e raio-x. Mas as máquinas enxergam diferente de nós, seres Humanos (isso se você que está lendo realmente é um Humano), a programação direta tentando imitar fielmente nossa visão não funcionava nas máquinas. Mesmo assim, em 2001, dois pesquisadores do MIT conseguiram uma técnica de reconhecer rostos em imagens através da ideia de verificar se aquela imagem possuía formas que são comuns em rostos, uma espécie de filtros passados pela imagem inteira. Apesar do avanço, não era possível, por exemplo, identificar qual a pessoa que estava na foto ou se havia mais alguma coisa, um carro, um pássaro, ou ainda responder a pergunta “o que o homem está fazendo na foto?” Mas você já deve estar pensando o que mudou tudo, com a vinda da era dos dados e o avanço do poder computacional, essa área pôde se aproveitar das mais recentes descobertas em Deep Learning. Em 2010 surgiu uma competição de visão computacional em que o objetivo era ter o menor erro em classificação de imagens em um dataset chamado ImageNet, contendo milhões de imagens de milhares de categorias, algumas imagens contendo ainda a demarcação dos objetos a serem classificados. Abaixo podemos olhar, para cada ano, o melhor resultado de cada equipe, sendo cada “bolinha” uma equipe. Você, um leitor atencioso, notou que em 2012 uma das equipes foi extremamente melhor que as outras e, ainda mais, muito melhor que o ano anterior. O que aconteceu neste ano? Em 2012, o vencedor, Alex Krizhevsky, desenvolveu a AlexNet, uma rede neural convolucional (pode ver nossos textos de redes neurais, em que começamos a explicar a rede convolucional aqui) que despertou um enorme interesse na comunidade de Deep Learning e CV. Desde então todas as equipes utilizam esta técnica em competições de CV e os resultados foram cada vez mais próximos, como se pode ver acima. Antes de entendermos melhor as técnicas atuais mais avançadas na área, ainda precisamos entender o que de fato as máquinas enxergam. Como um Humano, nós conseguimos perceber o espaço tridimensional à nossa volta, a diferença de luz, sombras e, sem esforços, diferenciar cada aspecto de objetos e suas superfícies. A intuição de como uma máquina enxerga vem de que cada pedacinho de uma imagem pode ser o quão claro ou escuro será aquele pedaço de foto. Cada pedacinho de imagem é chamado de um “pixel”, dessa forma conseguimos formar imagens monocromáticas, ou seja, preto e branco. As cores são comumente armazenadas em 8-bits, assim temos 2⁸=256 valores possíveis de intensidade para uma cor, sendo 0 sua ausência (preto), e 255 a intensidade máxima (branco). Para imagens coloridas devemos ter uma sobreposição de pixels de três cores: vermelho, verde e azul, o famoso espaço de cores RGB (red, green, blue): Existem outros espaços de cores, mas por enquanto é suficiente entender apenas este. Para formar todas as cores que enxergamos digitalmente basta atribuir um valor de 0 a 255 para vermelho, verde e azul e, sua sobreposição, formas as demais cores. Na imagem acima o branco é a intensidade máxima das três cores (pode se ver o vermelho, o verde e o azul lado a lado) e o preto é a ausência delas. O laranja é algo próximo da intensidade máxima do vermelho, um pouco de verde e nada de azul. Resumindo: Uma imagem para um computador nada mais é do que uma matriz de pixels! Simples! Um vídeo então é apenas uma sequência de imagens, ou seja, uma sequência de matrizes de pixels. Vamos visualizar, utilizando Python, a imagem de uma releitura do logo do Grupo Turing em preto e branco. Como as imagens são comumente armazenadas em três canais (acho que você já cansou de me ouvir falar de RGB), devemos levar isso em conta na hora de observar e manipular imagens, no caso abaixo iremos pegar o primeiro canal pois, como a imagem é preta e branco, os três canais são iguais. Além disso, é uma prática comum normalizarmos (dividindo por 255, que é o valor máximo de intensidade dos pixels) os valores dos pixels e passarmos os valores entre 0 e 1. Agora vamos selecionar apenas uma parte da imagem e visualizar como essa informação está sendo guardada para o computador. Se mostrarmos o array apenas da parte da direita, esperamos ver 1 onde a imagem é branca e zero onde a imagem é preta, vamos conferir: Vemos que os zeros e uns formam exatamente o pedaço de reta acima. Se quisermos fazer imagens coloridas no computador, fazemos igual o exemplo acima, a diferença é que para cada ponto da imagem (cada pixel) devemos ter três valores de 0 a 1, cada um correspondente à intensidade R, G e B. Vemos que nosso array acima tem três dimensões: a posição em Y, a posição em X e a terceira dimensão são os três valores que definem a cor. Vamos montar uma malha colorida para entender melhor essa disposição: Apesar do desenvolvimento acelerado da área, é importante sabermos seus limites, se não, por que ainda não temos nenhum carro totalmente autônomo? Um dos principais motivos para isso é que a maior parte dos últimos avanços foi na área de reconhecimento de imagem, enquanto que o próximo passo é o conhecimento visual! Consegue-se identificar muito bem rostos, pessoas, animais, objetos em imagens, mas ainda não temos o conhecimento de o que está na imagem, como por exemplo: Uma máquina consegue, atualmente, facilmente reconhecer duas pessoas na imagem e, dependendo do algoritmo, consegue ainda reconhecer um baixo e um amplificador. Mas qualquer ser Humano conseguiria descrever a foto como: “Um homem de camiseta branca, tatuado, de bigode está cantando em um palco com luzes roxas enquanto outro homem tatuado está sem camiseta, tocando baixo com o cabelo pintado de laranja”. Na realidade, já existem redes que performam razoavelmente esta tarefa de descrição de imagem, também chamada de image captioning, mas ainda estão longe de performar tão bem quanto na tarefa de classificação por exemplo. Este nível de descrição elevado ainda não está disponível nas máquinas e será uma habilidade essencial para automação de diversos processos. Certamente um carro que consegue, além de reconhecer uma pessoa na rua, dizer se ela está planejando ou não atravessá-la na faixa, será muito mais seguro de deixar fazer todo o trabalho. Não estamos lá ainda, mas com o passo de evolução que o mundo está observando na área além de evoluções na capacidade computacional e número de dados disponíveis, é bem provável que chegaremos lá. Enquanto isso, continue acompanhando nossos Turing Talks. E se quiser conhecer um pouco mais sobre o Grupo Turing, não deixe de seguir as nossas redes sociais: Facebook, Instagram e LinkedIn. Obrigado e até o próximo."
https://medium.com/turing-talks/efici%C3%AAncia-de-algoritmos-ordenando-com-bubble-sort-selection-sort-e-random-sort-382e04b2f523?source=collection_home---------60----------------------------,"Eficiência de algoritmos: ordenando com Bubble Sort, Selection Sort e Random Sort",,Camila Lobianco,1504,5,2020-08-19,"Olá, meus caros programadores. Vocês já tomaram o café do dia? Sejam bem-vindos a mais uma edição do Turing Talks! Hoje nós vamos dar continuidade ao assunto começado nesse post e testar na prática aquilo que vimos para alguns algoritmos de ordenação. Vamos começar de forma drástica. Apresento-lhes aquele que foi classificado como um dos piores algoritmos em questão de capacidade de processamento de dados na história. Sua vantagem: é o código mais simples, de fato. Sua desvantagem: demora tanto tempo para carregar que decidi analisar arrays de dimensão de, no máximo 10⁵. A seguir, temos o código para a ordenação do array. Sugiro fortemente que você tente pensar primeiro sozinho sobre a complexidade disso, antes que a gente faça o trabalho final juntos. Pensou aí? Então vamos deixar os detalhes do lado e vamos direto para o que realmente importa: a quantidade de laços do programa. Se você já teve essa sacada, percebeu que no começo, vamos percorrer o array uma vez, como foi no exemplo anterior. Porém, para cada vez que percorrermos uma casa i do array, deveremos percorrer novamente o array. Ou seja, temos que percorrer ele n vezes n vezes. Em resumo, isso é um belo de um O(n²). Vamos fazer isso ficar mais visual para facilitar então: Não é o fim do mundo, mas também não é nem de perto o ideal. Talvez seja difícil perceber como isso é ineficiente, mas depois de analisar alguns, vamos ver comparativamente como esse algoritmo pode ser um problema dos grandes se você quiser algo rápido. Mas, para dar um gostinho do desespero, segue um gráfico de como ele se comporta. Observação: randômico é um array criado com valores aleatórios, inverso é aquele que a ordem é contrária à esperada e ordem correta é aquele que, bem, a ordem está correta. Agora vamos observar mais um algoritmo. Esse é menos problemático que o Bubble Sort, e também tive que analisar vetores de até 10⁵ apenas. Mas vamos olhar primeiro para o código para ter uma ideia do “problema”: Se você pegou a ideia do de cima, já deve estar imaginando que é quase a mesma coisa do anterior. A maior diferença é que ele separa o vetor em uma parte que está ordenada e uma parte não-ordenada, fazendo um número menor de operações. A lógica é simples: colocar o menor elemento no começo da parte não-ordenada do array, como podemos ver nos gifs abaixo: Como ainda temos que ficar percorrendo esse array da forma bruta, o algoritmo ainda não é dos melhores na questão de tempo (o que, lembrando, não significa que é “pior”, tudo depende do que você precisa). No gráfico abaixo, podemos ver novamente esse comportamento de (O(n²)) que ele possui: Eu sei, é meio esquisito. Teoricamente, um array com a ordem inversa deveria ser sempre pior, não? Mas pensa aqui comigo, o selection sort de todo jeito vai ter que procurar o menor elemento e colocar ele no começo. Ou seja, de todo jeito ele deverá percorrer a parte não ordenada do array inteira. Em resumo, no começo, seja randômico ou invertido, seu comportamento não irá mudar muito. Para encerrar essas análises com a prova que a humanidade pode ter a quantidade mais absurda de ideias possíveis para resolver um problema, vamos ver o que faz o Random Sort (mais conhecido como Bogosort). No post passado, eu disse que iríamos ver um algoritmo de complexidade O(n!) em posts futuros, mas temos algo melhor: um algortimo que pode até alcançar a complexidade O(n.n!)! Mas, para facilitar, vamos utilizar um algoritmo simples apenas fatorial. Uma pequena introdução para uma ferramenta usada: shuffle é um algoritmo de embaralhamento, ou seja, ele pega um vetor e os embaralha em uma ordem aleatória. Vamos supor que o nosso array array que deve ser ordenado é [5, 2, 3, 4, 1] … É, isso pode durar literalmente por anos em arrays de tamanho maior. Caso esteja muito interessado, recomendo esse vídeo de um array de 11 valores tentando ser ordenado durante um período de quase 12 horas (e falhando miseravelmente). O motivo disso é que esse é um modelo probabilístico. Ou seja, qual a chance de que todos os elementos estejam no lugar em que deveriam estar? Supondo o array que foi dado anteriormente, 1 tem 1/5 de chance de estar no lugar correto. Depois, o 2, como o 1 já está ocupando, terá 1/4 de chance de estar no lugar correto. E assim por diante. Podemos ver que a chance final de que todos os eles estejam no lugar certo é de (1/n!). Esses são os algoritmos mais simples (em questão tanto de análise quanto de programação) que vamos ter por enquanto. Eles eram os mais utilizados antigamente (menos o Bogosort, claro) até serem criados os de ordenação recursiva que estudaremos nos próximos posts. Para mais informações, acompanhe o grupo Turing no Facebook, Linkdin, Instagram e nossos posts do Medium :)."
https://medium.com/turing-talks/implementa%C3%A7%C3%A3o-do-desfoque-gaussiano-d6e21e314920?source=collection_home---------59----------------------------,Implementação do desfoque Gaussiano,Utilizando numpy para implementar nosso próprio desfoque gaussiano,Guilherme Salustiano,1617,5,2020-08-23,"(Esse é um texto sobre Visão Computacional, veja este texto se gostaria de ler uma introdução à área) Seja bem vindo a mais um Turing Talks! Filtros de desfoque são gerados naturalmente devido a captação de imagens por algumas lentes. Tem ganhado popularidade em meio a filtros no Instagram que esboçam o fundo para destacar algo, o que já é muito usado na indústria visual. Além disso com aplicações mais pontuais é usado para tratamento de pele na pós produção de uma foto. Por se tratar de um comportamento natural da câmera também é simulado em diversos jogos como fifa 20 e nba2k20, que apresentam o conteúdo pelas “câmeras” que filmam os jogos. Mas muito além disso, são muito usados na área de visão computacional de forma a homogeneizar uma imagem. Diminuindo seu ruído e nível de detalhamento de forma a facilitar a generalização por um futuro processamento. Aqui vamos aprender de onde eles surgiram, porque são assim e como implementá-los! Para ocorrer o desfoque precisamos de uma grande quantidade de pontos de luz entrando pela lente, por isso ela só pode ser conseguida naturalmente como a câmera abaixo. Isso se deve a como a câmera captura a luz e gera a imagem. Um ponto reflete a luz em diversas direções, a lente da câmera então recupera todos esses feixes de luz e tenta junta-los em um ponto no sensor. Acontece que só é possível focar em um ponto de cada vez (o dito cujo foco da câmera), então para pontos muito longe do plano de foco os feixes de luz tendem a se interceptar atrás do sensor da câmera, o que gera no sensor um ponto espalhado, desfocado. Como mostrado pela figura com um espaço menor para a luz passar, (o obturador da câmera mais fechado) diminuímos o efeito devido aos pontos se espalharem menos. Para quem se interessar isso é chamado de profundidade de campo, estudado por fotógrafos. Como vimos, esse efeito é causado pela luz de vários pontos próximos se combinado no pixel final. Uma forma de tentar reproduzi-lo a partir de uma imagem é combinado seus pixels próximos, fazendo a media de todos dentro de uma área próxima. Aqui temos que decidir também o que fazer nos cantos, onde não se pode pegar todos os pixels. Minha escolha aqui foi por ignora-los e dividir apenas pelos que estão sob a área vermelha. Melhorando um pouco nosso algoritmo, pode ser útil em vez de uma média aritmética conseguimos adicionar pesos a cada posição, para por exemplo as posições centrais importarem mais que as mais distantes. Esses pesos são representados em uma matriz, chamada de núcleo de convolução (do inglês kernel), as vezes também chamada de máscara. Brincando com esses pesos e a quantidade de pixels pegos conseguimos diversos resultados interessantes, como diferentes tipos de blur, detecção de borda e até mesmo descobrir rostos! É essa mesma matriz de pesos que está presente nas Redes Neurais Convolucionais, elas começam com valores aleatórios e seus pesos são treinados. Abaixo está a minha implementação para a aplicação da convolução em uma imagem Agora temos que decidir como preencher nossa matriz de pesos. Nesse caso para imitar a difusão das lentes os pontos mais centrais são mais influentes mas podemos melhorar em vez de chutar potências de 2. Um dos jeitos mais comuns de preencher nosso núcleo é com o que os estatísticos amam: uma distribuição gaussiana. A equação para o núcleo gaussiana de tamanho (2k+1)×(2k+1) será: Para um núcleo de tamanho 5, função retornará os seguintes valores: E o python facilita bastante para gera-lo: Combinando os métodos conseguimos gerar nossa matriz gaussiana: Gerando o seguinte resultado: Obrigado por acompanhar até aqui! Esperamos conseguir explicar um pouco mais sobre o que é o desfoque gaussiano, além da aplicação inicial das convoluções que hoje dominam o mundo das redes neurais. Pra ficar por dentro segue a gente nas redes sociais, estamos no Facebook, LinkedIn e Instagram. Até a próxima! Se você gostou, confira também: Redes neurais convolucionais, Implementação de redes convolucionais com tensorflow e keras."
https://medium.com/turing-talks/an%C3%A1lise-de-algoritmos-de-machine-learning-ebb2b1fe3913?source=collection_home---------58----------------------------,Análise de Algoritmos de Machine Learning,,Camila Lobianco,1799,6,2020-08-30,"Seja bem-vindo de volta, caro humano. Aproveitando essa nossa fase de ficar analisando algoritmos (que começou nesse post e nesse outro), vamos para o que é realmente interessante: machine learning. Sim, chegou a hora de conhecer melhor as engrenagens dos mais famosos algoritmos de inteligência artificial. Como esses algoritmos são muito longos, não vamos escrever passo-a-passo como fizemos nos outros posts, mas qualitativamente vamos passar pelos algoritmos mais importantes. Infelizmente, é impossível analisar todos os algoritmos da área em um único post (porque, acredite ou não, eles são muitos), então vamos nos ater aos modelos mais famosos. Mas, como toda boa análise, vamos começar com uma tabela: Antes de ir para o ponto da questão, vamos fazer algumas considerações. O que pesa principalmente na eficiência dos algoritmos são sua quantidade de features e dados de treino. É aí que está a importância de escolher as melhores features e também tratar direito seus dados (o que você pode encontrar nesse nosso post aqui). Então, por mais que, matematicamente, essas complexidades estejam corretas, o Big-O sempre considera o pior caso, então dependendo do que você estiver fazendo, isso pode melhorar. Primeiro de tudo, é muito importante conhecer bem o seu modelo. Mas, para isso, nós temos esse post aqui para te ensinar o necessário. Depois que você tiver uma ideia, vamos ver como isso funciona. Já deixo claro que é importante o entendimento de árvores binárias para que a seguinte análise flua com mais facilidade. Basicamente, uma árvore de decisão faz sua avaliação levando em consideração cada divisão dos dados e vai fazer isso para cada feature em cada nó que não é o nó de uma folha. Isso acontece a medida que os níveis continuam. Supondo uma árvore binária totalmente balanceada, a complexidade será O(log(n)), porque você simplesmente deverá contar quantos níveis tem. Se tivermos 1 nó, com 2 filhos e cada um deles com 2 filhos, teremos que analisar 3 camadas (o que é, aproximadamente, log de 8 na base 2, que é 3). Porém, se tivermos uma árvore totalmente desbalanceada, com uma concentração única em um lado, teremos que andar N camadas, o que nos deixa com uma complexidade de O(n). Além disso, outra coisa que deve ser levada em consideração é o fato que nós também temos que calcular uma probabilidade em cada nó para calcular a entropia para fazer o corte. Isso adicionará uma complexidade de O(log(n)), na mesma lígica que usamos anteriormente, por causa da estrutura de árvore binária. Então vejamos, a complexidade de uma Decision Tree será de O(n * p * d * log(n)), sendo d a profundidade de camadas. Aplicando o que acabamos de descobrir, temos que a complexidade da árvore estará entre algo como O(n * p * log²(n)) e O(n² * p * log(n)) Se você não está totalmente familiarizado com esse modelo, sugiro esse nosso post. Esse algoritmo só está aqui porque ele é basicamente o que fizemos antes… Mas multiplicado pelo número de árvores, já que a Random Forest não passa de um monte de resultados comparados de um monte de Decision Trees. Então, considerando o que foi passado anteriormente, podemos ver que a eficiência desse algoritmo estará entre algo como O(n * p * log²(n) * t) e O(n² * p * log(n) * t). Agora é o momento desse nosso post brilhar. Basicamente, a Regressão Linear se resume a um conjunto de multiplicações de matrizes. A complexidade de multiplicar duas matrizes, considerando que serão dois for’s encaixados, como podemos ter uma noção a partir desse post, é de 0(n²) (mas nesse caso, a matriz a ser multiplicada é de features, não a de número de dados). Como teremos que fazer isso para cada feature, teremos uma complexidade final de O(n*p²), quando avaliando X’X (a matriz e seu inverso). Mas isso não é tudo. Quando fazemos essa multiplicação, também precisamos calcular a matriz inversa para fazer esse trabalho. Para cada feature, teremos portanto que fazer uma inversão, o que nos obriga a acrescentar a inversão dessa matriz. Para finalizar, portanto, temos uma complexidade padrão de O(n* p² + p³). Esse é o post que vai te ajudar a entender tudo sobre esse modelo. Para fazer o SVM funcionar, nós precisamos calcular o kernel de uma matriz conhecida como K, que consiste em um produto escalar entre uma função que tem como parâmetro um xi e um xj. Então, reduzindo muitas etapas do processo, podemos entender que será uma função que dependerá da multiplicação de dois valores numéricos. Até aí temos uma complexidade O(n²). Considerando que isso será feito para cada feature, essa complexidade fica como sendo O(n² * p). Porém, como no caso anterior, precisaremos em uma etapa desse processo inverter uma matriz, o que nos deixa com uma complexidade O(n * p² + p³). OBS: criar vetores suporte é algo que pode ser feito de diversas formas, o que pode fazer com que essa eficiência seja alterada. O método escolhido aqui foi apenas o mais didático, porém existem métodos bem mais eficientes que exigem mais álgebra. Vamos dar uma olhada nesse post antes de começar a análise. Esse é o método que, de longe, mais oferece abertura para diferentes tipos de eficiência temporal. Isso porque o método de funcionamento desse algoritmo funciona comparando cada elemento com uma quantidade x de elementos próximos. Dessa forma, do mesmo jeito que você pode comparar um ponto apenas com os seus dois mais próximos, você pode comparar com os seus 50 mais próximos. Nesse caso, tudo depende de como o programador decide organizar os seus dados (lembrando que existem funções para achar o número de vizinhos ideal, mas se prepare porque pode demorar um pouco para rodar). Eficiência nesse caso não é tudo. Esse é um modelo muito sensível ao parâmetro de vizinhos, então cabe a quem está programando conhecer bem seus dados e decidir se vale a pena ou não sacrificar um pouco de tempo para ter um modelo melhor ou deixar o trabalho com uma acurácia não tão boa, mas executável em um espaço de tempo razoável (o que pode nem sempre acontecer). Por fim, esse é o último post importante para ser lido antes da nossa análise. Agora vamos para a estrelinha de eficiência do mundo do Machine Learning. Naive Bayes, o modelo mais rápido e mais fácil de ser entendido, uma vez que ele só precisa de noções básicas de estatística. Basicamente, tudo que você tem que fazer é calcular a probabilidade de cada dado ser de uma determinada classe. Ou seja, teremos uma complexidade O(n) quando se trata da quantidade de dados, uma vez que eles não serão multiplicados nem nada do tipo, mas sim analisados individualmente. Como deveremos fazer isso para cada feature, basta multiplicar a complexidade pelo número de features. Disso, tiramos que a complexidade final do algoritmo é de O(n * p). Eu sei, eu sei, essa foi uma longa jornada. Mas olha só o tanto de coisa que deu para aprender nesse post! Agora se você quiser melhorar o tempo para rodar seu modelo, basta olhar essas eficiências e perceber onde seria mais adequado fazer algumas mudanças! Espero que tenham gostado, até a próxima! Para mais informações, acompanhe o grupo Turing no Facebook, Linkedin, Instagram e nossos posts do Medium :)."
https://medium.com/turing-talks/app-scraping-628e7fc514a0?source=collection_home---------57----------------------------,App Scraping,Extraindo dados de aplicativos de celular,Vinicius Cleves,1604,10,2020-09-06,"A web é uma fonte preciosa de dados. Tanto é que existe uma infinidade de ferramentas disponíveis para coletar informações de websites. Com a popularização do uso de smartphones e a crescente migração do acesso à internet dos computadores pessoais para os dispositivos móveis, certas plataformas estão sendo disponibilizadas somente no formato aplicativos móveis. Diferentemente dos websites, os aplicativos não produzem páginas HTML, então os esquemas de extração de dados utilizados em websites não funcionam. No entanto, em algum momento a informação que está no servidor precisa ser enviada para o app, esse processo é realizado através do consumo de uma API. Nesse post, vamos ver como realizar um man-in-the-middle attack em seu próprio emulador android para descobrir a estrutura da API utilizada pelo app, e como utilizar essa informação para obter os dados em python. Para observar as requisições emitidas e as respostas obtidas pelo app nós vamos precisar redirecionar o fluxo da rede através de um proxy, que vai simplesmente logar (registrar) todo esse fluxo para nós. Nos b̶o̶n̶s̶ ̶e velhos tempos de http, poderíamos redirecionar o fluxo através de um proxy sem grandes setups, mas também o poderia qualquer espertinho com um laptop conectado na mesma rede, logando e até modificando o seu tráfego sem seu conhecimento. Esse é o man-in-the-middle attack, onde um terceiro monitora e modifica o seu tráfego sem o seu conhecimento. Para resolver esse g̶i̶g̶a̶n̶t̶e̶s̶c̶o problema, é utilizado o protocolo https. Ele criptografa a comunicação entre o cliente e o servidor, impedindo que um terceiro possa entender ou modificar essas mensagens. Eu não vou entrar em detalhes de como a comunicação é estabelecida através do https, mas uma última informação é importante para entender o que vamos fazer em seguida. No estabelecimento da comunicação entre o cliente e website, o cliente precisa saber que a identidade oferecida pelo website é de fato autêntica. Isso é feito através de certificados emitidos por autoridades de certificação que já vem instalados junto ao dispositivos ou navegador acessando a internet. Esse video do Computerphile explica muito bem como funciona o man-in-the-middle attack e o que é feito para evitá-lo. Esse outro post do Joshua Davis mostra o papel dos certificados e autoridades de certificação nesse processo. É de se esperar que o aplicativo que você extrairá os dados esteja usando https, então vamos configurar um proxy com suporte a captura de tráfego em https. Os passos 2 e 3 a seguir são baseados nesse post no Blog do Ropnop e foram reproduzidos aqui para facilitar a vida do leitor. O primeiro passo é instalar o Burp Suite, que tem uma versão gratuita e será o nosso proxy. Agora vamos exportar o certificado CA emitido pelo Burp. Esse certificado será instalado no dispositivo e vai permitir ao Burp se passar por qualquer website quando redirecionarmos o tráfego por ele. Abra o Burp Suite com as configurações padrão de um projeto temporário, vá em Proxy -> Options e clique em Import / export CA certificate. Selecione Certificate in DER format e pressione Next. Insira o caminho para salvar o certificado e avance até o final para concluir. Formate o certificado de acordo com o esperado pelo android. Nós vamos instalar esse certificado no emulador que vamos criar a seguir. Copie o valor gerado pelo segundo comando e substitua por HASH no comando a seguir para renomear o arquivo de acordo com os padrões do android No meu caso o arquivo resultando ficou com a seguinte nomenclatura: 9a5ba575.0 Configure o proxy do Burp para aceitar requisições. No Burp Suite vá em Proxy -> Options e em Proxy Listeners clique em Add. Na janela que se abrir, na aba Bindings, selecione uma porta que não esteja em uso para o Bind to port, 8082, por exemplo, e All interfaces para Bind to address. Pressione OK para salvar e fechar a janela. Para finalizar, vamos desabilitar a interceptação de mensagens no proxy. Na aba Proxy -> Intercept desabilite a interceptação se ela estiver habilitada. Para descobrir a API que o app usa nós precisamos efetivamente de ter o app rodando. A maneira mais fácil e segura de fazer isso é com um emulador. Mais segura porque vamos instalar o certificado CA do burp no dispositivo com permissão de root, e é sempre bom evitar mexer nessas coisas no seu celular a menos que você realmente saiba o que está fazendo. O emulador android é muito simples de configurar e vamos ver como nos passos a seguir. O primeiro passo é fazer o download e instalação do Android Studio. Abra o Android Studio. Ele vai pedir para criar um projeto. Pode criar um projeto com as configurações padrão, nós não vamos precisar dele. Com o Android Studio aberto, pressione Ctrl + Shift + a e digite AVD Manager. O AVD Manager é o gerenciador de emuladores do Android Studio. Selecione Create Virtual Device. Você pode selecionar qualquer dispositivo, imagino, mas nesse tutorial eu escolhi o padrão, Nexus 5X. Continue com Next. Na próxima tela nós vamos escolher a versão do android do nosso emulador. Na aba x86 images selecione a versão que você preferir. A versão deve ser compatível com o Open GAPPS (sobre o qual vamos falar mais para frente). Nesse link você encontra as equivalências entre a versão do android e o número da API. Nesse tutorial eu escolhi a API nível 28, que corresponde ao Android 9. Eu não sei se o uso de uma imagem com Google APIs é necessário, mas como essa imagem é mais completa, eu optei por utilizá-la logo na primeira tentativa. Você pode ficar tentado a utilizar uma imagem com Google Play, mas essa imagem não vai servir, pois ela não te dá acesso como root. Uma vez selecionada a imagem desejada, clique em Download ao lado do nome da imagem para baixá-la. Uma vez baixada, selecione-a e pressione Next para seguir em frente. Na próxima tela faremos a configuração do emulador. A única modificação que eu fiz foi no Boot Option, mudando para Cold Boot. O Quick Boot já me deu problemas no passado, então eu sempre mudo essa configuração por precaução. Clique em Show Advanced Settings e em Emulated Performance -> Boot Options selecione Cold Boot. Pressione Finish para criar o emulador. Na janela do AVD Manager você verá o nome do emulador, guarde esse nome para mais tarde. Agora vamos instalar o Google Play no emulador que criamos. Os passos para instalação a seguir foram obtidos do post do Daishi Kato, mas nós os reproduzimos aqui para facilitar ao leitor. Primeiro vá a página do Open GAPSS e faça o download de acordo com a versão do android que você configurou no emulador. Eu usei a variante Nano. Extraia os arquivos para instalação. Substitua OPEN_GAPPS_DOWNLOAD.zip com o nome do arquivo baixado Inicie o emulador. Substitua o AVD_NAME pelo nome do seu emulador android que você criou (mantenha o @) Instale os pacotes E reinicie o sistema Nota: Pode ser que seja necessário fechar o emulador e iniciá-lo novamente. Faça login no Google Play e instale o aplicativo do qual deseja extrair os dados. Falta instalar o Certificado CA no dispositivo. Para isso vamos seguir os passos indicados no Blog do Ropnop. Para acessar o emulador como root e transferir o Certificado CA para o emulador use os comandos a seguir. Substitua CERTIFICATE.0 pelo nome do certificado CA gerado anteriormente. Agora acesse o terminal do emulador Mova o certificado para a pasta correta e corrija as permissões Saia do terminal do emulador com ctrl + d e reinicie o dispositivo Você pode verificar a instalação do certificado navegando no emulador em Settings -> Security & location -> Advanced -> Encryption & credentials -> Trusted credentials e buscando por PortSwigger Feche o emulador. Vamos executá-lo agora utilizando a rede através do nosso proxy. Pronto, agora temos o Proxy e o Emulador rodando. Vamos então ver como usá-los para descobrir a API utilizada pelo app. Agora vamos ver como é a estrutura da API que o app utiliza. Para tanto vamos deixar a aba Proxy -> HTTP history do Burp aberta e vamos abrir o nosso app. Nos logs do Burp, vamos procurar na coluna Host por um endpoint (uma url) que esteja relacionado ao domínio do app. No nosso caso foi bem fácil de achar, o endpoint era api.NOME_DO_APP.com. O app utiliza uma API REST. Em uma API desse tipo são utilizados diferentes tipos de verbos para fazer requisição, cada um com o seu papel de identificar uma intenção do cliente. Cada tipo de verbo vai acompanhar uma estrutura na requisição. Aqui vamos cobrir o básico dos dois tipos de requisição mais comuns para o nosso caso, GET e POST. As requisições do tipo GET são usadas para obter informações do servidor. Os parâmetros que você passa na requisição para identificar as informações que deseja obter geralmente vão inseridas na url, são aqueles vários alguma_variavel=algum_valor que você vê separados por um & após um ? numa pesquisa no Google. As requisições POST são utilizadas para enviar informações ao servidor. As informações geralmente vão inseridas no corpo da mensagem em formato JSON. Por fim, cabe dizer que uma API REST é organizada em um conjunto de Recursos, que para nosso propósito aqui podem ser compreendidos como um conjunto de caminhos que colocamos após o endpoint para realizar ações específicas. Esses caminhos são semelhantes a organização de pastas, com / dando inicio a um subnível. Então, para exemplificar, o /auth poderia ser utilizado para autenticar e o /calendar/users/me para obter o calendário do usuário. De volta aos logs do app no Burp, identifique as requisições realizadas pelo app. Ao selecionar uma requisição, como na imagem acima, o Burp mostrará duas abas, a primeira com as informações da requisição e a segunda com as informações da resposta. Cada aba contém sub-abas que mostram diferentes formas de enxergar as informações que contém na requisição/resposta. A primeira sub-aba (Raw) contém todas as informações da requisição/resposta em formato de texto. Você pode navegar nas outras abas para entender melhor o que cada parcela do texto na aba Raw representa. As informações que precisaremos para fazer requisições são: Para saber como solicitar alguma informação ao servidor, navegue no app de modo que ele faça essa solicitação. Então é só guardar o modelo da requisição http, identificar os elementos listados acima e aplicá-los no script em python que faremos a seguir. Agora nós vamos implementar um script para fazer requisições se passando pelo app instalado no celular. Eu presumo que você já tenha alguma experiência com python e já tenha python instalado. Se não tiver, eu sugiro começar com o Anaconda utilizando o jupyter notebook para desenvolvimento (já vem com o Anaconda). O Anaconda também já vem com a lib Requests, mas caso você esteja utilizando uma instalação mais limpa de python, você deve instalar o Requests. Requests é uma biblioteca em python para facilitar a emissão de requisições HTTP, tudo o que precisamos para o nosso objetivo. O código a seguir exemplifica uma solicitação para a API do app copiando o que observamos no log do Burp. Usamos uma sessão para fazer a persistência dos cookies e também para guardar os headers que se repetem entre as chamadas para não ter que repeti-los a todo momento no código. Nesse post mostramos como fazer o scraping de dados de um app para android. A web é uma preciosa fonte de informação, e espero que esse guia possa facilitar a sua vida quando precisar extrair dados de um aplicativo. Para mais informações sobre data science, inteligência artificial e afins, acompanhe o Grupo Turing no Facebook, Linkedin, Instagram e nossos posts do Medium :). Esses passos funcionaram para mim. Ao reproduzi-los, deve-se ter ciência do que está fazendo. A coleta de dados pode constituir uma violação dos termos de uso de certas plataformas, confira os termos de uso que se aplicam ao seu caso."
https://medium.com/turing-talks/mlops-em-google-cloud-platform-introdu%C3%A7%C3%A3o-11af43fd66e0?source=collection_home---------56----------------------------,MLOps em Google Cloud Platform - Introdução,,Abelardo Fukasawa,1535,5,2020-09-08,"Bem vindo a mais um Turing Talks! Durante os anos de 2018 e 2019, tive a oportunidade de dar aulas de ciência de dados em uma escola de tecnologia. Dentre as centenas de alunos entrando no mundo de dados, observei diversos perfis: executivos que queriam entender melhor o mundo de dados para incorporar novas estratégias às suas empresas, graduandos que viam o mercado de dados como futura carreira, profissionais de TI que queriam expandir seu leque de habilidades e até profissionais de outras áreas querendo fazer uma transição de carreira. Independente do perfil do aluno, a dúvida mais comum que recebi como professor foi: “mas como eu pego esse modelo do meu notebook e coloco em produção?”. De fato, vejo pouquíssimos cursos e materiais na internet que consigam responder essa pergunta de maneira satisfatória, ainda menos em português. É claro que você vai encontrar como fazer uma aplicação em Flask ou FastAPI, como fazer um container de um modelo para servi-lo etc, mas essas respostas não são suficientes. Um sistema baseado em Machine Learning (ML) é, antes de tudo, um sistema de software, e deve ser tratado como tal. Se no mundo de engenharia de software é comum aprender conceitos e cultura de DevOps e deployment, por que não fazê-lo no contexto de ML? Tentarei responder a pergunta dos meus alunos e de várias outras pessoas que estão entrando no mundo de dados com uma série de artigos sobre MLOps e desenvolvimento de aplicações de ML. A ideia é utilizar o ambiente do Google Cloud Platform (GCP) para construirmos um pipeline de um aplicação de ML, um passo por vez. Cobriremos deploy, monitoramento, orquestração de re-treino, gestão de features etc. Exceto essa parte introdutória, todo tutorial dessa série acompanhará um Google Colab com código aberto e conectado à nuvem, de modo de você pode replicar o desenvolvimento em sua conta GCP. Também tentarei deixar todo o código comentado e explicado, pois lidaremos com diversas ferramentas diferentes. PS: algumas partes dessa série usarão produtos fora do free-tier da GCP. Avisarei sempre que isso acontecer. Vamos dizer que você, como cientista de dados de uma empresa de marketing, foi incubido de desenvolver um produto de dados capaz de otimizar suas campanhas e canais de marketing. Você, então, pegou os dados da empresa, os limpou, fez E.D.A., feature engineering & selection, criou um modelo fantástico e ele mostrou ótima performance! Quantidade de valor gerado até o momento: zero. Não me leve a mal, mas um modelo de ML passa a gerar valor de fato quando ele está em produção, tomando decisões (com exceção de modelos puramente analíticos, claro). Ao adentrar no mundo de dados, colocamos muito energia em aprender diversos algoritmos de ML e como fazer uma modelagem de qualidade, e isso é ótimo. Porém, finalizada a modelagem, o trabalho ainda não acabou. Voltamos então à pergunta desse artigo: como colocar modelos de ML em produção? Bom, a partir do momento que saímos do Jupyter Notebook e pensamos em colocar o modelo em produção, precisamos nos preocupar em construir software capaz de responder as seguintes perguntas: Ao tentar responder essas perguntas, notamos a interseção do mundo de ML com engenharia de software. Percebemos, também, que o código da modelagem em si é, portanto, somente uma das partes de uma aplicação de ML. Eis que surge o MLOps (ML + DevOPS), uma forma de gerir o desenvolvimento e operação de um sistema baseado em ML. Essa prática passou a ganhar força a partir do paper Hidden Technical Debt in Machine Learning Systems (Sculley et al, 2015). Nessa publicação, os autores trazem questões de débito técnico específicas para o cenário de ML, como Data Dependencies e Model Entanglements, recomendo fortemente a leitura. Dessa forma, MLOps é vital para que possamos entregar sistemas baseados em ML de forma prática, segura e escalável. De uma forma resumida, temos o ciclo de um projeto de ML descrito pela imagem abaixo: A prática de desenvolvimento a partir de MLOps engloba todas as fases e áreas desse ciclo, para que tenhamos controle contínuo do desenvovimento de nosso projeto. Ok, então o que eu devo ir atrás para incluir essa prática no desenvolvimento dos meus projetos? De forma resumida, precisamos: Nos próximos posts, iremos construir nosso pipeline de MLOps, partindo de um modelo desenvolvido em jupyter notebooks. Começaremos pela etapa que gera mais valor imediato: o deploy! Veremos alguns cenários e ferramentas para fazermos nosso primeiro deploy na nuvem. Até a próxima! Para mais informações sobre data science, inteligência artificial e afins, acompanhe o Grupo Turing no Facebook, Linkedin, Instagram e nossos posts do Medium :)."
https://medium.com/turing-talks/vis%C3%A3o-computacional-o-que-%C3%A9-convolu%C3%A7%C3%A3o-ad709f7bd6b0?source=collection_home---------55----------------------------,Visão Computacional — O que é convolução?,Uma das bases da visão computacional,Rodrigo Fill Rangel,1306,8,2020-09-13,"Texto escrito por Rodrigo Fill e Paulo Sestini. Olá caro leitor dos Turing Talks, seja muito bem vindo a mais um texto, aproveitando a série de visão computacional? Espero realmente que esteja gostando. Hoje vamos apresentar mais um dos temas desta série, aprofundando o conceito de convolução, abordado nos textos anteriores desta série. Neste texto você vai aprender o que é convolução, quais tipos existem e como ela se aplica em visão computacional. Como visto nos primeiros textos as aplicações de visão computacional são das mais amplas, desde querer aplicar um filtro à uma imagem até querer que o computador seja capaz de interpretar as imagens que são fornecidas a ele. Lembrando então que as imagens são interpretadas pelo computador como uma matriz, podemos pensar que, não importa a aplicação, ao tratar os problemas citados será necessário ter uma operação matemática bem definida para lidar com as matrizes. Tal operação já existe e é chamada de convolução, que é definida tanto de forma contínua como discreta, no tempo ou no espaço. Para o computador o importante é apenas a operação discreta e especificamente para visão computacional o mais interessante é a convolução discreta espacial, mas antes vamos passar pelos outros tipos rapidamente para elevar o grau de intuição e entendimento. Convolução temporal contínua A convolução temporal contínua é geralmente utilizada em análise de sinais temporais analógicos, como a transmissão física de sinais elétricos por meio de condutores. Um significado físico para a convolução temporal é particularmente mais difícil de ser encontrado, porém, podemos pensar na convolução como uma medida de quanto dois sinais estão sobrepostos em diferentes instantes de tempo. A equação para cálculo da convolução entre duas funções contínuas, f(t) e g(t), é: Onde * denota a operação de convolução. Resolver analiticamente a integral acima pode não ser trivial, na realidade quase nunca é, mas é possível resolver a convolução de forma gráfica, como faremos a seguir. Processo de resolução gráfica Iremos realizar uma convolução entre os dois pulsos de sinal na imagem a seguir. Observe que os pulsos são funções que possuem valores diferentes de zero somente durante um certo intervalo de tempo, sendo nulas nos outros instantes. Ainda é possível que esteja um pouco difícil visualizar o que foi dito acima, então vamos utilizar o maravilhoso recurso do GIF para mostrar como fica a convolução entre dois sinais contínuos de forma gráfica, movendo um deles, mantendo o outro fixo: A imagem mostra duas funções funções, f(t) em azul e g(t-𝝉) em vermelho deslocando na imagem, repare que ela foi invertida em relação à imagem anterior que mostramos. Vemos então a vermelha sendo deslocada e sobrepondo a função azul. Em preto, por fim, está f*g, a convolução de fato. Algumas coisas para notar, a convolução tem uma parte de subida, uma constante, e uma de descida, como eu falei acima cada instante que g(t-𝝉) é deslocado gera um ‘ponto’ da função f*g, neste sentido quando a função f*g cresce, significa que a sobreposição de g(t) com f(t) está aumentando. Quando este valor se mantém constante significa que a sobreposição é a mesma. Vale lembrar então que este é um processo para funções contínuas em seu domínio, mas a operação de convolução pode ser definida em para outros tipos de domínios, como em funções discretas. Convolução temporal discreta O convolução temporal discreta é particularmente útil quando tratamos de sistemas lineares invariantes no tempo discretos, como o computador. É uma forma análoga da função que vimos anteriormente, mas por ser no tempo discreto seu cálculo analítico é consideravelmente mais simples (não inclui integrais): Convolução espacial discreta Vamos falar agora sobre a convolução mais simples, a espacial discreta, que é a de fato utilizada pelo computador quando realizamos visão computacional. A convolução espacial utiliza imagens, no caso matrizes, no lugar de funções ou sinais. Imagine a f(t) como sendo uma imagem e a g(t) como sendo outra imagem. O processo de cálculo agora da convolução não depende mais de uma integral visto que estamos em domínio discreto, intuitivamente basta sobrepor as imagens e multiplicar, pixel a pixel, seus valores, e somar tudo ao fim, isso retorna o valor da convolução naquele ponto. A quantidade de convoluções a serem realizadas vai depender do tamanho das imagens, se tiverem o mesmo tamanho apenas uma operação de convolução é feita, se tiverem tamanhos diferentes voltamos ao processo de deslocar as imagens e ir computando as operações de convolução. Vejamos o gif abaixo onde a imagem analisada está em verde, em amarelo temos a imagem que buscamos comparar, repare que a imagem amarela seria um ‘x’. Em rosa esta o resultado da convolução em cada ponto: A imagem sendo deslocada é chamada de máscara em visão computacional, seu uso pode ser dos mais variados, por exemplo: Como vimos, podemos utilizar um filtro de convolução para realizar operações em imagens, agora vamos ver alguns exemplos do que pode ser feito. Convolução para detecção Podemos utilizar filtros para encontrar para detectar padrões e objetos em imagem, um bom exemplo disso é o de detecção de bordas e contornos em imagens. Há vários tipos de filtro para isso, sendo que um dos mais famosos é o filtro de Sobel. O Filtro de Sobel possui duas componentes, uma para realizar detecção de bordas na horizontal e outra na vertical. Podemos pensar numa borda como uma mudança abrupta de intensidade na imagem, e o formato do filtro para fazer tal detecção retrata este fato, com valores variando de positivo para negativo ao longo dos eixos horizontal e vertical. Convolução para tratamento de imagens É muito comum utilizarmos editores de imagens para borrar imagens ou aumentar nitidez, e os editores fazem isto justamente utilizando convolução com diferentes filtros para cada propósito: Já escrevemos um Turing Talks especialmente para este tipo de filtro, você pode ver mais detalhes neste link: Implementação do desfoque Gaussiano Convolução em redes neurais convolucionais Com estes exemplos vimos que, dependendo dos números presentes na matriz de um filtro, uma função diferente é realizada. Porém, estes filtros são construídos manualmente, então, como uma rede neural os utiliza para a detecção de objetos? A resposta é que, ao contrário dos kernels mostrados, que são construídos manualmente para determinado propósito, as redes neurais aprendem a fazer os seus próprios sozinhas. Assim, a tarefa das redes neurais convolucionais é aprender os números certos das matrizes para que cada filtro faça a função necessária. No final, elas aprendem diversos tipos de detectores diferentes, um para cada propósito. E assim chegamos ao fim de mais um Turing Talks! Esperamos ter conseguido ajudar a compreensão desta que é uma importante operação para visão computacional. Em outros textos futuros da área vamos explorar mais a fundo as aplicações que foram citadas, então fique atento aqui no Medium para não perder os próximos conteúdos, e siga as páginas do Grupo Turing no Facebook e no Instagram, até a próxima!"
https://medium.com/turing-talks/introdu%C3%A7%C3%A3o-a-bag-of-words-e-tf-idf-43a128151ce9?source=collection_home---------54----------------------------,Introdução a Bag of Words e TF-IDF,As principais formas de preparar seu texto para o aprendizado de máquina,Camilla Fonseca,1505,10,2020-09-20,"Texto escrito por Alex Koji, Camilla Fonseca e Vitoria Rodrigues. Olá, querido leitor. Bem vindo a mais um Turing Talks! Desta vez, vamos abordar dois conceitos muito importantes para a área de Processamento de Linguagem Natural: Bag of Words e TF-IDF! O código desse artigo está armazenado no nosso GitHub, basta clicar aqui para visualizar. Antes de começarmos, é necessário já ter dado uma lida no nosso Turing Talks de Introdução ao Processamento de Linguagem Natural. Então se você ainda não viu, corre lá! Já leu? Então vamos começar: Para usarmos um modelo estatístico ou de deep learning em NLP, precisamos de features: informações mensuráveis acerca de algum fenômeno, ou seja, uma forma estruturada de armazenar informações. Porém, textos são um tipo de dado não estruturado (não organizado de uma maneira pré-definida, fixa), assim, é difícil para o computador entendê-los e analisá-los. Por isso, realizamos a chamada feature extraction, ou seja, transformamos o texto em uma informação numérica de modo que seja possível utilizá-lo para alimentar um modelo. Uma das maneiras mais populares e simples de fazer isso é com Bag of Words (BoW). BoW é uma forma de representar o texto de acordo com a ocorrência das palavras nele. Traduzindo para o português, o “saco de palavras” recebe esse nome porque não leva em conta a ordem ou a estrutura das palavras no texto, apenas se ela aparece ou a frequência com que aparece nele. Por exemplo, se a palavra TURING aparece muito num texto, ela se torna mais central e importante para a máquina. Portanto, BoW pode ser um ótimo método para determinar as palavras significativas de um texto com base no número de vezes que ela é usada. Bem simples, né? Basicamente, para gerar um modelo de bag of words precisamos realizar três passos: 1) Selecionar os dados 2) Gerar o vocabulário 3) Formar vetores a partir do documento Vamos ver cada um desses passos separadamente: Essa parte diz respeito a selecionar os dados que serão usados — no nosso caso, o texto — e prepará-lo de forma que a máquina consiga processá-lo bem. Então, primeiro, vamos falar sobre os dados com qual trabalharemos nesse artigo. Nosso texto será o poema “No Meio do Caminho” do poeta Carlos Drummond de Andrade: “No meio do caminho tinha uma pedratinha uma pedra no meio do caminhotinha uma pedrano meio do caminho tinha uma pedra. Nunca me esquecerei desse acontecimentona vida de minhas retinas tão fatigadas.Nunca me esquecerei que no meio do caminhotinha uma pedratinha uma pedra no meio do caminhono meio do caminho tinha uma pedra.” Vamos colocar nosso texto em uma variável chamada ‘texto’, dessa forma fica mais fácil trabalharmos com ele: Com nossos dados escolhidos precisamos prepará-los. Chamamos isso de pré-processamento, e se você deu uma lida no nosso artigo de Introdução a NLP já sabe que pré-processar um texto é essencial quando queremos trabalhar com ele, principalmente quando queremos aplicar um modelo de predição ou outros métodos estatísticos. São muitos os métodos de pré-processamento, mas aqui vamos realizar apenas alguns: Colocando todas as letras em minúsculos Colocar todas as letras em minúsculo é importante porque nossa máquina tende a interpretar palavras iguais mas com letras minúsculas e maiúsculas como sendo diferentes, por exemplo, para o computador ‘Turing’ e ‘turing’ não são a mesma palavra. Portanto, vamos utilizar a função .lower() do próprio python em cima da variável ‘texto’ e guardar esse novo texto na variável ‘texto_min’. Selecionando apenas letras com REGEX Esse passo é fundamental porque, para a máquina, a pontuação de um texto não importa e muitas vezes ela entende a pontuação como também sendo uma palavra. E para nossa análise com BoW, tão pouco importam os números. Nesse caso, vamos utilizar a biblioteca re e o método .findall, também vamos guardar esse texto com apenas letras na variável ‘apenas_letras’. Juntando os tokens Já que o .findall nos retorna todas as palavras como strings dentro de uma lista (tokens), vamos juntá-las com a função .join(), dessa forma nosso texto volta a ser um texto novamente. Por último, vamos guardá-lo na variável ‘novo_texto’. No fim, sua função deve ficar mais ou menos desta forma: Já pensou se você conseguisse memorizar todas as palavras que você visse? Diferente de nós que precisamos ver, rever uma, duas e provavelmente mais vezes para memorizar, o computador já consegue fazer isso de primeira. Então para gerar o vocabulário que nada mais é que a coleção de todas as palavras que ocorrem em um texto, basta passarmos todas elas uma vez só. Então a ideia para o código é basicamente: Vamos ver como fica isso na prática? Olha só! Como o vocabulário tem um número fixo de palavras, podemos usar uma representação de tamanho fixo equivalente a esse número: um vetor! Cada elemento desse vetor corresponderá a uma palavra do vocabulário. Há diversas formas de preencher nosso vetor com números para representar um documento (uma ‘amostra’ de um conjunto maior de textos), por exemplo, usar a contagem de vezes em que a palavra aparece nele. Mas a maneira mais básica de fazer isso é atribuindo um valor booleano/binário: 1 se a palavra aparece, 0 se não. Podemos pensar nesse processo como uma tabulação do documento, veja o exemplo com os versos “Nunca me esquecerei que no meio do caminho/tinha uma pedra/tinha uma pedra no meio do caminho/no meio do caminho tinha uma pedra.”: Assim, para codificar com codificação binária, o passo a passo é: Agora que você sabe como gerar um Bag of Words, vamos ao próximo assunto desse Turing Talks: TF-IDF vem para superar os problemas do Bag of Words. Trata-se de medidas estatísticas para medir o quão importante uma palavra é em um documento (texto), assim como BoW, mas com algumas diferenças. Com ele, podemos perceber a importância de uma palavra por meio de uma pontuação, o TF-IDF de uma palavra em um texto é feito multiplicando duas métricas diferentes: Em outras palavras, para o TF-IDF, quanto mais frequente uma palavra é em seu documento, mais importante ela tende a ser. Entretanto, isso depende da repetição dela ao longo de todos os documentos que estão sendo analisados. Por exemplo, suponhamos que estejamos analisando três documentos: uma revista de futebol, uma de vôlei e uma de basquete. Temos palavras que se repetem ao longo de todos esses documentos, por exemplo, a palavra “esporte” deve aparecer em todas as três revistas, certo? Então, provavelmente, não contribui muito para uma análise. Porém, palavras que se repetem muito em documentos individuais dizem mais a respeito dele, então a palavra “cesta”, por exemplo, que pode se repetir muito na revista sobre basquete, mas não nas outras, tende a se tornar mais importante para o TF-IDF. Por isso dizemos que a repetição das palavras importa com relação aos documentos que estão sendo analisados. Interessante, né? No nosso poema, consideraremos cada estrofe como um documento (mas poderíamos fazer de outras formas também, considerando cada verso como um documento, por exemplo). Para a implementação de TF-IDF precisamos seguir passos muito semelhantes ao que fizemos com BoW anteriormente: Essas duas etapas já foram feitas lá em cima, então não vamos repeti-la — Lembre-se apenas que nosso vocabulário está armazenado na lista Vocab — Portanto, vamos direto para o último passo: 3. Gerar um dicionário de frequência desses termos Então vamos para a implementação! Para criar o dicionário vamos criar uma função dicionario_de_contagem, que retorna a palavra e a quantidade de ocorrências dela. Basicamente, essa função recebe como argumento a lista Vocab, que já criamos, e o documento tokenizado, e cria um dicionário com as palavras do poema seguidas do número de ocorrências delas. Primeiro vamos tokenizar nossos documentos (estrofes): Agora nossa função dicionario_de_contagem: Por fim, vamos passar nosso vocabulário e nossos documentos para serem analisados: Como sua definição diz, esse termo é dado pela divisão da quantidade de vezes que uma palavra aparece em um documento pela quantidade de palavras desse documento, ou seja, representa a frequência que esse termo tem em um documento. Assim, vemos que o TF (frequência do termo) é basicamente a implementação de um BoW para cada documento. Então pensando no exemplo das revistas, se pegarmos a de futebol, algumas palavras que possam ter um alto índice de TF são as que têm alta relação com o assunto (futebol) como: bola, jogador, gol, entre outros. Agora, como fazer a implementação do código? A ideia é fazer como na função dicionario_de_contagem, porém passando como texto os documentos e ao invés de devolver a frequência, devolver esse valor normalizado, ou seja, dividido pela quantidade de palavras do documento. Os outputs de cada estrofe devem ficar dessa forma: Voltando ao nosso exemplo das revistas de futebol, vôlei e basquete, apenas a TF (frequência do termo) ainda vai dar maior ênfase à palavra “esporte”, que se repetirá mais vezes nas três revistas, do que a “cesta”. É aí então que entra o IDF (inverso da frequência nos documentos): uma medida, originalmente proposta pela cientista Karen Spärck Jones, para dar maior peso a palavras que ocorrem em menos documentos (no exemplo, estamos considerando cada revista como um documento). Dessa forma, esse peso será inversamente proporcional ao número de documentos em que uma palavra aparece e por isso o nome “inverso da frequência nos documentos”. Mas como calcular essa medida? Há várias formas, algumas mais convenientes dependendo da aplicação do que outras, o necessário é apenas que seja uma função inversamente proporcional à frequência nos documentos, ou seja, decrescente em relação a ela. A função mais comum é a seguinte, que já apresentamos: Note que quanto maior df(t), menor será N/df(t), por isso temos uma função decrescente. Depois, passamos essa medida para a escala logarítimica, o que reduz a sua grandeza e portanto é útil principalmente quando temos um corpus grande. A função tem a seguinte curva: Implementando o cálculo da IDF: Nossos valores de IDF devem ficar assim: Para obter a pontuação TF-IDF, basta então multiplicar essas duas medidas. Assim, quanto maior a TF e a IDF, obteremos uma pontuação maior, ou seja, as palavras consideradas mais relevantes serão aquelas que aparecem muito em um documento, mas pouco em outros. Vamos criar um DataFrame para visualizar as pontuações resultantes do poema: Note que tanto os termos que não aparecem na estrofe como os que aparecem em todas as estrofes ficam com pontuação 0, porque se o termo aparece em todas as estrofes, df(t)=N, portando seu valor de idf será log(N/N)=log1=0. Por isso, na primeira estrofe, todas as pontuações deram 0. Nesse Turing Talks abordamos uma implementação feita a mão de dois conceitos muito importantes para preparar seu texto: Bag of Words e TF-IDF. Ambos são muito importantes na área de Processamento de Linguagem Natural e podem ser melhores entendidos na prática, já que cada documento gera outputs diferentes. Além disso, a melhor abordagem depende do seu problema. Por exemplo, no caso do poema “No meio do caminho” pode ser que o TF-IDF não seja tão interessante, já que a repetição dos termos “pedra” e “caminho” ao longo da maioria dos versos é justamente o que os torna relevantes. Portanto, não tenha medo de treinar com seus próprios dados, isso ajudará muito na compreensão de cada um desses termos! Na continuação desse Turing Talks, mostramos como aplicar essas técnicas em um dataset de reviews de produtos para fazer um modelo de análise de sentimentos: Para mais textos como esse, não deixe de acompanhar o Grupo Turing no Facebook, Linkedin, Instagram e nossos posts do Medium =) Agradecimentos especiais à Julia Pociotti."
https://medium.com/turing-talks/paradoxo-da-acur%C3%A1cia-56baa75334f2?source=collection_home---------53----------------------------,Paradoxo da Acurácia,Um conceito fundamental para sua jornada em Ciência de Dados,Felipe Azank,1007,4,2020-09-23,"Quando começamos a aprender sobre alguma nova área de nosso interesse, seja ela uma disciplina da faculdade, um instrumento, um esporte, uma demanda do trabalho, sempre somos bombardeados com “conceitos fundamentais”, os quais precisamos saber em detalhes seu funcionamento e levá-los em consideração a todo momento durante nossos estudos. Hoje, conheceremos um pouco sobre um conceito fundamental para a sua jornada no mundo da Ciência de Dados, em especial quando falamos sobre o treinamento e a avaliação do seu modelo: O Paradoxo da Acurácia Em linhas gerais, o Paradoxo da Acurácia pode ser definido como a situação contraditória na qual uma acurácia elevada em seu modelo de classificação pode evidenciar uma falha do seu próprio modelo em realizar predições de fato significativas. Estranho né? Como uma acurácia alta em uma base de teste pode evidenciar uma falha no meu modelo? Essa contradição pode ser melhor explicada através de um exemplo, discutido logo a seguir. Se você ainda não sabe o que significa acurácia de um modelo de classificação, não deixe de consultar nosso Turing Talks que aborda esse assunto. Nele, também pode-se encontrar a definição de conceitos importantes como Falso positivo, Falso negativo, precision e recall. Imagine que você, como um cientista de dados, foi contratado por uma empresa para criar um modelo de Machine Learning que identifica se um e-mail é “spam” ou não a partir de seu título e assunto. Para realizar tal tarefa, você recolhe os 50 mil e-mails recebidos e mandados por todos os colaboradores da empresa, entre os quais têm-se 49 mil e-mails “verdadeiros” e mil e-mails “spam”. Além disso, você decide que usará 80% desses dados para treinar seu modelo e 20% para testar sua efetividade. Após passar por todo o processo de análise e construção de um modelo de classificação, você entrega um modelo com uma acurácia de 98%! Um resultado aparente muito bom e que deixou todos satisfeitos. Contudo, depois de algumas semanas, você começa a receber inúmeras reclamações informando que os e-mails de spam continuam aparecendo na caixa de entrada, apesar da presença de seu algoritmo com 98% de acurácia. Onde está o erro? Por que o modelo não está funcionando? O que a acurácia não está nos dizendo? Ao analisar o gráfico acima, podemos ver o quão mal distribuído estão os e-mails entre as diferentes classes (spam e não spam). Se separarmos esses dados entre teste (20%) e treino(80%) presernvando a proporção, teremos a seguinte configuração: Dados de treino Dados de teste Na tentativa de descobrir o motivo da falha no modelo, você percebe um padrão estranho e que não faz sentido, seu modelo classifica todos os textos (ou quase todos) como não spam. Isso nos dá um algoritmo que, mesmo sem utilidade prática nenhuma, apresenta uma acurácia quase perfeita. Uma vez que os dados desbalanceados garantem 9800 acertos em 10000 tentativas e, portanto, 98% de acurácia. Os pontos levantados acima nos permite concluir que, em determinadas situações e problemas, uma acurácia alta nem sempre nos dá o melhor modelo. Por exemplo, se construíssemos um modelo com 82% de acurácia, mas que apresentasse um recall elevado, ou seja, que não deixasse spams passarem com facilidade, esse modelo teria muito mais valor e de fato ajudaria na resolução do problema. Tendo isso em vista, encontramos nosso paradoxo! Um modelo com menor acurácia pode agregar muito mais valor para um problema do que um modelo com acurácia gigantesca mas indiferente aos dados. Agora que vimos o que significa e como ocorre o Paradoxo da Acurácia, a sua identificação é de igual importância. Uma forma possível de perceber se o seu modelo está apresentando uma acurácia alta, mas sem um potencial real de discriminar as classes, é utilizando e outras métricas de avaliação, como precision, recall, f1, matriz de confusão, entre outras. Todas essas muito bem explicadas no Turing Talks de métricas de avaliação em classificação. Além disso, outros métodos para escapar desse paradoxo estão relacionados com a manipulação dos dados existentes para evitar o seu desbalanceamento, ferramentas essas que serão melhor explicadas no nosso próximo Turing Talks. Muito obrigado por chegar até aqui! Espero que esse texto tenha te ajudado a entender melhor sobre o Paradoxo da Acurácia, um conceito fundamental para os seus estudos e sua formação como Cientista de Dados. Se quiserem conhecer um pouco mais sobre o que fazemos no Turing USP, não deixem de seguir as nossas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Para acompanhar também um pouco de nosso projetos, acesse nosso GitHub."
https://medium.com/turing-talks/dados-desbalanceados-o-que-s%C3%A3o-e-como-evit%C3%A1-los-43df4f49732b?source=collection_home---------52----------------------------,Dados Desbalanceados — O que são e como lidar com eles,,Felipe Azank,1277,9,2020-09-27,"Escrito por Felipe Azank e Gustavo Korzune Gurgel De acordo com os últimos levantamentos, atualmente nós humanos produzimos mais de 2,5 quintilhões de bytes de dados todos os dias. Apesar de parecer assustador, uma vez que esse número é aproximadamente 100 mil vezes o número de células de um corpo humano, esse fato levanta a possibilidade de analisarmos e estudarmos cada vez mais dados e aspectos da vida humana. Contudo, não é incomum, em nossas idas e vindas no mundo dos dados, nos deparamos com datasets que demonstram categorias muito mais frequentes que as demais e que prejudicam nossas previsões e modelos. Tendo isso em vista, hoje conheceremos em detalhe sobre esse tipo de dataset, além de sua identificação, seus problemas e como evitar que esse fenômeno prejudique o seu modelo e suas previsões. Hoje vamos falar de Dados Desbalanceados. Para isso, seguiremos a seguinte estrutura: Dados Desbalanceados podem ser definidos pela pequena incidência de uma categoria dentro de um dataset (classe minoritária) em comparação com as demais categorias (classes majoritárias). Na maioria dos casos, isso faz com que tenhamos muitas informações a respeito das categorias mais incidentes, e menos das minoritárias, o que pode, em muito casos, interferir no workflow padrão de um Cientista de Dados. O desbalanceamento nos dados mostra-se presente em diversos setores e campos do conhecimento, não sendo incomum encontrarmos eles regularmente e em contextos variados. Como em dados de diagnóstico e prognostico de câncer, em que a maioria das pessoas que realizam o experimento apresentam um resultado negativo; fraudes de pagamento, as quais costumam ocorrer em menor incidência do que não-fraudes; e até em sua caixa de spam, na qual a maioria dos e-mails são legítimos. Contudo, mesmo sendo bem presentes no cotidiano de um cientista de dados, se não lidados de alguma maneira, dados desbalanceados podem acarretar em problemas na construção de modelos e na geração de previsões. Se desenvolvermos um modelo sem considerar essa desproporcionalidade nos dados, o modelo será vítima do Paradoxo da Acurácia (explicado no último Turing Talk), em que os parâmetros do algoritmo não diferenciarão a classe minoritária das demais categorias, acreditando que estão agregando resultado devido à aparente alta acurácia. Essa falta de diferenciação pode ocasionar problemas sérios, uma vez que a identificação desses casos minoritários podem ser o cerne do desafio a ser resolvido. Vamos usar como exemplo o caso citado acima: Se nosso modelo não diferencia com sucesso os diagnósticos positivos para câncer, classificando-os como casos negativos (classe majoritária), estaríamos mandando pacientes com câncer para casa, o que apenas agregaria prejuízos. Apesar de seu potencial destrutivo na criação de modelos, os dados desbalanceados nem sempre mostram-se tão nocivos em determinadas áreas da Ciência de Dados. Segue portanto, algumas ressalvas que podemos levantar sobre os dados desbalanceados: Antes de montarmos qualquer modelo, explorar os dados e buscar insights lógicos a respeito dos dados que temos é sempre importante (quando se não, a tarefa principal). Em muitos casos, o fato de estarmos trabalhando com dados desbalanceados durante a análise nem sempre trás prejuízos aos seus insights uma vez que algumas conclusões não necessitam de uma quantidade elevada (porém suficiente) de dados. Ademais, é de se perceber que, em muitos casos, descobrir o “porquê” do desbalanceamento pode trazer informações válidas para a resolução do problema. Outra ressalva importante e destacar se relaciona com a diferença entre as categorias majoritária e minoritária: em alguns casos sua diferença é tão grande que alguns modelos conseguem distinguir as classes mesmo com uma quantidade pequena de dados. Um exemplo simples que retrata esse cenário seria na criação de um modelo que diferencia humanos e girafas com base na altura, mesmo com poucos humanos no dataset, a maioria dos modelos captará a grande diferença de altura entre as espécies. Porém, para trabalhar da melhor forma com os dados e eliminar as incertezas, lidar com os dados desbalanceados de alguma maneira sempre é a melhor escolha a se fazer. Após essa introdução e apresentação dos problemas causados pelos Dados Desbalanceados, vamos agora estudar algumas das formas mais comuns que existem para resolver esse empecilho. Uma forma de tirar o viés causado pela diferença de proporção das categorias consiste em manipular a quantidade de dados que são efetivamente utilizados pelo modelo de Machine Learning, tentando igualar o número de observações entre as classes. Esse método consiste em reduzir o número de observações da classe majoritária para diminuir a diferença entre as categorias. Há duas formas de realizar o Undersampling: A implementação básica desses métodos podem ser encontradas na biblioteca imbalanced-learn, com aplicações que podem ser vistas a seguir. É importante ressaltar que independentemente do método usado para o Undersampling a frequência dos dados acaba igual, mas a distribuição deles é diferente, e isso é o que se reflete na performance do modelo. Segue a frequência dos dados após o Undersampling. Ao contrário do Undersampling, o Oversampling consiste em criar sinteticamente novas observações da classe minoritária, com o objetivo de igualar a proporção das categorias. A maneira mais primitiva de fazer um Oversampling é por meio de cópias de dados já existentes na classe minoritária. Por exemplo, se nossos dados possuem três parâmetros (x, y e z) o Oversampling poderia ser feito escolhendo algumas entries e simplesmente duplicando o número de vezes que essas entries aparecem. A conduta descrita acima pode oferecer bons resultados quando a classe minoritária não tem uma variação quantitativa muito grande nos seus parâmetros, caso contrário o modelo pode ficar muito bom em identificar casos específicos da classe minoritária e não a categoria como um todo. Com o objetivo de resolver o problema acima, foi proposta uma técnica mais sofisticada conhecida como SMOTE. A ideia por trás dela consiste em criar observações intermediárias entre dados parecidos, ou seja, se no dataset existem 2 pessoas, uma com altura 1,80 m e pesando 78 kg, a outra com 1,82 m e pesando 79 kg; o algoritmo do SMOTE adiciona uma “pessoa” intermediária medindo 1,81 m e pesando 78,3 kg. Vale notar que o SMOTE não seleciona necessariamente a média entre ambas as informações preexistentes. Veja abaixo a implementação desses dois métodos. Concluindo, enquanto o Random Oversampling simula a existência de casos específicos e aleatório da classe minoritária, o SMOTE tenta simular a classe minoritária como um todo. Tal diferente de implementação gera um aumento na performance do modelo como ainda será demostrado neste texto. Segue o gráfico da frequência dos dados após o Oversampling: Existem diversos algoritmos de classificação, todos com suas peculiaridades, fraquezas e vantagens. Em tratar-se de Dados Desbalanceados, há algoritmos mais robustos e que agregam mais resultado no modelo do que outros. Modelos como Gradient Boosting, por exemplo, demonstram um desempenho melhor lidando com dados desbalanceados do que modelos como o KNN e SVM. Para decidir que algoritmo usar e qual é mais robusto para determinada situação, é sempre bom conhecer um pouco sobre a parte teórica de cada um deles. Por sorte, temos uma coleção de Turing Talks que tratam em detalhe sobre os principais. Se você se encontra em uma situação em que é de fato possível coletar mais dados da classe minoritária, não deixe de fazê-lo. Pode não ser o método mais fácil, mas com certeza é o mais eficaz para atingir o balanceamento. Entretanto, é de extrema importância utilizar essa técnica apenas em situações que os dados a serem adicionados são similares o suficiente com a sua base inicial. Haja vista que dados distintos podem prejudicar os parâmetros do modelo. Um exemplo simples do uso correto e incorreto desse método pode ser visto em um hipotético estudo de altura entre homens e mulheres da cidade de São Paulo (média: 162,0 cm). Se, na tentativa de aumentar a quantidade de dados referentes à mulheres, você adicionasse mais alturas de garotas da cidade de Campinas (média: 161,8 cm), esses dados apresentariam uma alta probabilidade de serem semelhantes em relação à sua distribuição. Entretanto, se, no lugar de garotas de Campinas, você adicionasse alturas referentes a mulheres da Grécia (média: 169,1 cm), talvez esses dados não ajudariam o modelo a atingir resultados melhores. Usualmente, os modelos de Machine Learning, enquanto atualizam seus parâmetros na base de treino, penalizam erros de classificação da mesma forma para todas as classes, isto é, classificar A como B ou B como A será penalizado da mesma maneira pela função de custo. Para que seja possível fazer um modelo mais resistente aos Dados Desbalanceados, adicionar uma penalidade maior para erros de classificação da categoria minoritária faz com que o algoritmo “tome mais cuidado” em errar essa classe. Dessa forma, teremos um modelo que evitará entregar resultados que ignorem as categorias com menores observações. Como padrão para a maioria dos modelos “clássicos” de Machine Learning, a métrica utilizada para treino dos parâmetros é a acurácia, contudo, com, como vimos anteriormente, problemas com Dados Desbalanceados costumam ser vítimas do Paradoxo da Acurácia. Uma maneira que visa resolver isso consiste em alterar a acurácia como métrica de treino, substituindo-a por outra mais estratégica para que seu modelo tenha parâmetros que “tome mais cuidado” em errar a classe minoritária, assim como o método anterior de mudança da função de custo. Por fim, assim como a construção de qualquer modelo de Machine Learning, verificar o desempenho de seu modelo com diferentes métricas pode te ajudar a encontrar eventuais problemas com Dados Desbalanceados, assim como verificar posteriormente se o modelo efetivamente soluciona esses problemas. Caso ainda não tenha visto, não deixe de consultar nosso Turing Talks sobre as métricas de avaliação para problemas de Classificação e Regressão. Como foi comentado no Turing Talks de métricas de avaliação, as métricas clássicas nem sempre são um meio confiável de avaliar um modelo treinado em dados desbalanceado. As práticas descritas nos itens 4,5 e 6 são muito interdependentes e quando se deseja implementá-las é possível adicionar muita substância ao modelo com algumas poucas funções da bilbioteca scikit-learn. O uso dos parâmetros corretos nas funções é de suma importante para isso. Veja a implementação abaixo, e os resultados que ela gera: O mais interessante de ser visto no código acima é que o manejo correto de dados desbalanceados pode resultar em melhorias muito expressivas na qualidade de predições. Perceba que em todos os casos os dados foram treinados com o mesmo modelo (SVM), mas após o resampling a performance do modelo nas métricas escolhidas cresceu fortemente. Dados Desbalanceados estão por toda parte e podem causar inúmeros problemas na sua jornada como Cientista de Dados se não forem levados em consideração. Portanto, saber como lidar com eles e como melhorar o seu modelo perante essa adversidade torna-se uma ferramenta muito útil. Esperamos que você tenha gostado e muito obrigado por chegar até aqui. Se quiserem conhecer um pouco mais sobre o que fazemos no Grupo Turing, não deixem de nos seguir nas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Para acompanhar também um pouco de nosso projetos, acesse nosso GitHub. Até a próxima!"
https://medium.com/turing-talks/efici%C3%AAncia-de-algoritmos-ordenando-com-merge-sort-59f91a02a667?source=collection_home---------51----------------------------,Eficiência de algoritmos: ordenando com Merge Sort,,Camila Lobianco,1301,4,2020-09-29,"E aí, colegas, como vocês estão? Hoje nós vamos utilizar um algoritmo muito famoso, o merge sort (como já diz o título), para ver como calcular a eficiência de algoritmos com chamadas recursivas. Esse post vai ser um pouco mais curtinho, mas é de extrema importância que vocês tentem refletir sozinhos no que está escrito, porque vai ser muito importante para os próximos posts. Só um pouco de história antes, esse algoritmo é muito especial porque ele trouxe novos ares para os programadores da época que só faziam ordenação com algoritmos do tipo de eficiência quadrática. Mas aí chegou o matemático John von Neumann que criou um método que ficou conhecido como dividir para conquistar. A diferença principal desse método é que, ao invés de ele tentar resolver o problema de ordenação todo de uma vez, como era feito antes com o bubble sort e o selection sort, ele iria quebrar ele em pequenos subproblemas que iriam sendo resolvidos pouco a pouco. Depois de todo esse papo, vamos para a parte interessante. Dá uma respirada, bebe aquela água fresca e se prepara porque agora as coisas começam a ficar mais divertidas. Depois que pegar a lógica, sua mente irá expandir na questão de análise de algoritmo, eu prometo. Só que, antes do desespero, se desafie a pensar sobre a complexidade desse algoritmo (dica: o segredo está na recursão). E aí, conseguiu pensar? Eu sei, esse é bem difícil de visualizar. Mas, antes de tudo, olhando para o código, perceba que cada array será percorrido uma única vez. Guarde essa informação porque ela será muito relevante mais tarde. Então, para facilitar o entendimento, vamos primeiro olhar seu comportamento: Vocês conseguem ver que, ao invés de tentar ordenar o array inteiro, ele o subdividiu em pequenos arrays e aí sim os ordenou? Isso foi algo extremamente importante para a história dos algoritmos. Esse algoritmo é conhecido por ter até ganhado um prêmio por ser o máximo de velocidade que um algoritmo de ordenação vai alcançar. Agora, vamos prestar atenção na seguinte imagem para poder analisar de uma forma mais lúdica a complexidade desse algoritmo: Essa imagem foi escolhida justamente para entender como funciona a divisão do array desordenado. Primeiro, temos um array de comprimento seis desordenado. O truque que o Merge Sort usa é que, para não ter que ficar percorrendo o vetor, ele simplesmente vai separando os dados em vetores menores e depois os ordenando pouco a pouco, como podemos ver no gif colocado acima. Assim, ele não precisa ficar o tempo todo fazendo verificações para cada elemento. Então, para ficar mais fácil, vamos analisar esse esquema por pequenos passos: Então agora vamos pensar na recursão. Podemos ver que a quantidade de recursões feitas seguirá um padrão. Vamos pensar em um array de oito elementos: primeiro, ele será dividido em dois de quatro, depois cada um dois de dois, e por fim cada um dos últimos em dois de um. Ou seja, para oito elementos, teremos 3 chamadas recursivas. Então a complexidade do algoritmo recursivo é de (O(lgn)) — a base padrão no logaritmo de complexidade é a 2. Agora vamos retornar a informação importante do começo: nesse algoritmo na parte de ordenação, temos uma complexidade linear. Portando, um algoritmo de complexidade linear que ocorre um número logarítmico de vezes, teremos uma complexidade final do algoritmo do tipo (O(n.lgn)). Para ilustrar isso, temos o seguinte gráfico: Uma coisa interessante também sobre esse algoritmo é o fato de que, no Merge Sort, o tempo de execução do caso médio (randômico) não muda muito em relação ao do pior caso (inverso). No entanto, isso não vem de graça. Por mais que, por um lado, ele tenha essa capacidade incrível de execução, ele exige uma memória e uma capacidade computacional muito grande. Mais uma vez é reforçada a ideia de que, nem sempre, uma eficiência maior de tempo significa um algoritmo melhor. Eu sei, esse foi um pouco mais trabalhoso que os anteriores, mas depois que passou não dá aquele sentimento de felicidade em perceber como isso abre portas para muitas melhorias na área de ciência de dados, por exemplo? Um exemplo interessante da aplicação desse algoritmos é para calcular a mediana de um conjunto de dados em um dataset! Espero que vocês tenham gostado! Para mais informações, acompanhe o grupo Turing no Facebook, Linkdin, Instagram e nossos posts do Medium :)."
https://medium.com/turing-talks/an%C3%A1lise-de-uma-a%C3%A7%C3%A3o-em-python-b8114bce2fc?source=collection_home---------50----------------------------,Análise de uma Ação em Python,,Thalissa Reis,1261,9,2020-10-04,"Nosso primeiro passo para Wall Street Texto escrito por Diego Lottermann, GuiCola e Thalissa Reis No Turing Talks de hoje, vamos explorar as bases das finanças quantitativas, desde a extração dos dados até o cálculo de retorno de uma ação. Você, que não sabe nada sobre finanças quantitativas ou programação, fique tranquilo, pois o propósito deste artigo é, justamente, tratar de conceitos básicos. Para que possamos começar qualquer tipo de análise, é preciso coletar os dados! É importante deixar claro que não existe um caminho único para obtenção dos dados, já que a série temporal com os preços de fechamento dos ativos analisados pode ser obtida a partir de diferentes canais, como em arquivos csv em sites de investimentos ou em API´s diversas. No presente artigo, coletaremos os dados da API do Yahoo Finance. Por quê? Simplesmente pela facilidade e gratuidade (analisamos ativos, mas ainda não somos ricos). Existem outras API´s interessantes, mas que demandam cadastro para obtenção de uma key, como o Alpha Vantage, ou são pagas, como a Morningstar e a IEX. Para obtenção dos dados no Alpha vantage seria necessário instalar a biblioteca alpha-vantage (pip install alpha-vantage) e realizar um cadastro no site da API (url). Dessa maneira, você obterá uma key, utilizada para validação da coleta de dados. Vamos explorar um exemplo de extração dos dados da Microsoft (MSFT). Tal API retorna um arsenal maior de dados, como o pagamento dos dividendos e ocorrência de split (desdobramento) do papel, por exemplo. Contudo, para os propósitos deste artigo, não serão necessárias tais informações adicionais. Por isso, é mais fácil extrair os dados (de frequência diária) da API do Yahoo Finance, através do Pandas Data Reader. Para extração de dados de uma única empresa, é necessário importar o pandas e o adicional “data” do leitor de dados do pandas, o qual chamaremos, carinhosamente, de wb em homenagem ao William Bonner. Agora, basta preenchê-lo, com as informações disponíveis do ticker desejado (no caso, MSFT — Microsoft) e a data de início da série (no caso, 01/01/2015) e a função nos retornará um dataframe! Para conferir se deu tudo certo, basta realizar uma consulta ao nosso df! Com o df em mãos, é necessário entender as features obtidas: Caso você queira apenas o preço de fechamento diretamente em seu dataframe, é possível adicionar a coluna de preenchimento exclusiva ao final do comando! A melhor parte do investimento Agora que já temos os dados do nosso ativo, vamos realizar algumas análises. Começando pelo principal fator que motiva a realização de um investimento: o retorno. Para calculá-lo, normalmente, deve-se considerar os dividendos pagos pela empresa ao longo do período analisado. Contudo, não possuímos essa informação em nosso dataset. Portanto, nesse primeiro momento, consideraremos apenas o retorno advindo do aumento (ou queda) do preço da ação. Em linhas gerais, calculamos o retorno utilizando a seguinte fórmula: Essa fórmula trata do chamado retorno simples — ou aritmético — da ação. Esse pode ser facilmente calculado usando uma função do pandas. Para exemplificar, calcularemos o retorno diário da ação da Apple ao longo do período de um ano (de 28/08/2019 a 28/08/2020). Com apenas duas linhas de código, adicionamos uma coluna com o retorno simples do ativo calculado para cada dia do período em questão. Mas, esse retorno dia a dia não nos diz muito sobre o ganho do investidor ao final do período. Para isso, podemos utilizar o retorno cumulativo simples, que é dado pela fórmula: Talvez seja contra-intuitivo tratar-se de uma multiplicação ao invés de um somatório. Mas, considerando que cada retorno é percentual, essa lógica mostra-se coerente. Por exemplo, se você comprou uma ação e ela valorizou 5% em um dia e, no dia seguinte, ela desvalorizou 2%, quanto ela valorizou (ou desvalorizou) ao todo considerando esse período? 3%? Não, pois repare que ela desvalorizou 2% considerando a valorização de 5% do dia anterior. Ou seja, ela diminuiu 2% do 105% do valor original. Portanto, ela valorizou 2,9%. E como fazemos para calcular isso em Python? Conseguimos calcular o retorno cumulativo simples por dia em poucas linhas de código! Nesse caso, percebemos que a ação da Apple deu um retorno cumulativo de 142,90% no período de um ano. E se calculássemos pela primeira forma mostrada, considerando apenas o preço de compra e venda? Como podemos perceber (e prever), obtivemos o mesmo número, mostrando que as duas formas para o cálculo do retorno simples em um dado período são eficazes. Mas sabia que não existe apenas o retorno simples? Também podemos calcular o retorno logarítmico. Então, tome um gole d’água e vamos ver como o fazemos. Cabe aqui lembrar que o logaritmo neperiano ou natural (ln) corresponde a um log de base e (número de Euler). E como podemos calculá-lo em Python? Para demonstrar, utilizaremos a mesma ação e período dos exemplos anteriores. Com o auxílio de uma função da biblioteca Numpy, criamos uma coluna com os retornos logarítmicos diários durante o período analisado. Mais uma vez, nos deparamos com o mesmo problema: os retornos diários não dizem muita coisa sobre o ganho do investidor ao final do período. Para tal, utilizaremos novamente o retorno cumulativo. Neste caso, o retorno cumulativo logarítmico apresenta a seguinte fórmula: Bem mais simples do que a fórmula do retorno simples cumulativo, né? Essa é justamente uma das razões pelas quais, em certas situações, é preferível utilizarmos o retorno logarítmico ao invés do simples. Sua implementação em Python pode ser feita de modo igualmente fácil, como mostrado abaixo: Assim, podemos ver que o investidor teve um retorno logarítmico cumulativo de 88,75%. Analogamente ao feito anteriormente, vamos substituir os valores na fórmula original de retorno logarítmico para atestar a equivalência entre as fórmulas: Conforme o esperado, obtivemos o mesmo valor ao calcularmos o cumulativo. Contudo, este está um tanto quanto discrepante do retorno simples obtido. Agora você pode estar se perguntando: qual a relação entre os dois retornos? Esta é dada pela seguinte fórmula: Substituindo os números obtidos anteriormente nesta fórmula, podemos conferir a relação entre os dois retornos. Além disso, podemos observar essa relação graficamente: Assim, está nítido que, quanto mais próximo de 0, menor é a diferença entre o valor obtido pelo retorno simples e pelo logarítmico. Isso pode ser também observado nas colunas do nosso dataset mostradas no decorrer do texto. No dia 28/8/2019, observamos um retorno simples de 0.016932 e um logarítmico de 0.016790. Nesses, a diferença entre os dois é de 0,8%. Já no dia 30/8/2019, obtivemos um retorno simples de -0.001292 e um logarítmico de 0.001293, com uma diferença de apenas 0,07% — uma diferença menor ainda — haja vista que estes estavam mais próximos de 0 do que os anteriores. Por fim, cabe mencionar que, nos cálculos de retorno apresentados acima, não consideramos o efeito da inflação. Como os preços tendem a subir (ou até descer) com o passar do tempo, o valor do dinheiro (em termos de poder aquisitivo) muda também. Por exemplo, se no período de um ano, houve uma inflação de 5% e uma ação deu um retorno de 14,5%, o retorno real do investimento foi de 9,5%. Risco, o que é e como calcular? Todos nós temos uma breve noção do que risco significa em nossas vidas: que risco estou passando ao deixar meu carro destrancado a noite em uma rua deserta? Que risco passo ao apostar na mega sena? Qual é o risco de um investimento? Fazendo uma breve consulta ao dicionário, podemos ver a seguinte definição para risco: “Probabilidade de insucesso/sucesso de determinado empreendimento, em função de acontecimento eventual, incerto, cuja ocorrência não depende exclusivamente da vontade dos interessados.” Se enxergamos nossos investimentos como empreendimentos, temos até que uma boa primeira definição para o risco dentro do mercado financeiro. Mas, diferentemente do risco de ser roubado a noite em uma rua escura, o risco do retorno de um ativo pode ser mensurado! Tome o gráfico do retorno (e não preços!) das ações da LATAM (LTM) e da Telefônica (TEF) entre o final de 2018 e 2020, por exemplo. Note que o retorno da LATAM sofreu variações muito maiores do que da Telefônica, ou seja, os retornos da Telefônica ficaram próximos da média, enquanto que o da LATAM não. Em outras palavras, a volatilidade da empresa aérea para um determinado período de tempo foi maior do que da empresa de telefonia. Como podemos metrificar essa volatilidade? Há uma série de maneiras para mensurar o risco, mas, em sua forma mais simples, o risco pode ser calculado pelo desvio padrão. Basicamente, a partir do (x_i — x_barra), vemos o quanto os retornos estão divergindo da média. Somamos todas essas diferenças e elevamos ao quadrado, para ter um valor positivo. Tiramos a média ao dividir por n e, finalmente, desfazemos o efeito do “elevar ao quadrado” ao tirar a raiz quadrada. Felizmente, não precisamos implementar essa fórmula na mão, o pandas faz isso com o método .std(): Repare que, de fato, o desvio padrão da LATAM (6,20%) foi maior que o da Telefônica (2,15%) no período. Desvio Padrão a cada período t Ainda no gráfico, vemos que alguns meses de 2020 foram aqueles com os maiores picos de retornos (positivos ou negativos), devido à crise do coronavírus, e isso interfere no cálculo do desvio padrão para anos anteriores. Para superar esse problema, podemos calcular o desvio padrão a cada t períodos de tempo, nesse caso, usaremos t = 30 (1 mês). Note que agora conseguimos isolar o efeito de períodos mais voláteis, como é o caso de abril de 2020 em diante, e focar apenas em períodos que queremos estudar. Outras medidas de risco Existem diversas outras medidas de risco que podemos usar, como o Desvio padrão exponencial móvel (EWMA), o desvio padrão estimado pelo High e Low (Parkinson Number) e o desvio padrão estimado por High, Low, Close e Open (Garman-Klass). Em um futuro não tão distante, faremos um Turing Talks apenas para medidas de risco, então, acompanhe! Mas, caso esteja muito ansioso, temos uma breve explicação desses estimadores de volatilidade em nosso Turing Talks: Construindo uma Estratégia de Investimentos Quantitativa — Time Series Momentum. Obrigada por nos acompanharem até aqui e até o próximo Turing Talks!"
https://medium.com/turing-talks/efici%C3%AAncia-de-algoritmos-ordenando-com-heap-sort-ec415ff07146?source=collection_home---------49----------------------------,Eficiência de algoritmos: ordenando com Heap Sort,,Camila Lobianco,1281,3,2020-10-06,"Oi, meus colegas. Vocês passaram bem desde o último post? Hoje nós iremos falar de mais um algoritmo que vai seguir a ideia do dividir para conquistar. Esse post também vai ser menor, mas é de fundamental importância que ele seja bem entendido para que possamos prosseguir para análises futuras. Se você passou pelo Merge, o Heap vai ser uma ideia, de certa forma, até bem semelhante na questão da análise do algoritmo. Ele se assemelha ao Selection Sort na questão de separar o vetor em uma parte ordenada e uma parte não-ordenada. Por outro lado, ele também lembra um pouco o Merge porque usa a recursão e ela deve ser levada em consideração na análise do algoritmo. Um pouco antes, vamos analisar o que é uma estrutura de Heap. Basicamente, uma estrutura de heap é uma árvore binária que segue uma das duas estruturas: o pai é sempre maior que os filhos ou o pai é sempre menor que os filhos. No primeiro caso, chamamos de max-heap, no segundo de min-heap. A imagem a seguir mostra a estrutura de um max-heap e de um min-heap: Então vamos analisar esse código: Para dar uma ideia melhor do que está acontecendo, vamos dar uma olhada nos seguintes gifs: Como podemos ver, nesse caso ainda teremos uma recursão que possibilitará que o vetor seja percorrido uma única vez, ou seja, sua complexidade, como foi analisado no Merge, pode ser considerada (O(n.lgn)). Só para ter certeza, vamos ver isso graficamente: Perceba que, nesse caso, não necessariamente a ordem correta será processada de forma mais rápida, pois o algoritmo continuará tentando que manter e verificar constantemente se a estrutura do heap é mantida. O merge e o heap são bem parecidos, mas a estrutura agora vai ser uma árvore binária. Acredite, ela vai aparecer em muitos outros lugares nos algoritmos de inteligência artificial que nós usamos, então reconhecer a eficiência de seu funcionamento é muito importante. Mas a questão que não quer calar é: Espero que vocês tenham gostado, até a próxima! Para mais informações, acompanhe o grupo Turing no Facebook, Linkdin, Instagram e nossos posts do Medium :)."
https://medium.com/turing-talks/detec%C3%A7%C3%A3o-de-bordas-e-transforma%C3%A7%C3%B5es-morfol%C3%B3gicas-em-imagens-com-opencv-8aecf8c8ba2f?source=collection_home---------48----------------------------,Detecção de Bordas e Transformações Morfológicas em Imagens com OpenCV,Extraindo algumas features clássicas de uma imagem,Luísa Mendes Heise,1100,10,2020-10-11,"Texto escrito por Luísa Mendes Heise e Guilherme Salustiano No Turing Talks de hoje nós vamos falar sobre um assunto bem famoso na área de visão computacional. O assunto é detecção de bordas e transformações morfológicas em imagens. Para que você, caro leitor, consiga acompanhar esse texto sem maiores dificuldades, precisamos que conheça o conceito de convolução em imagens e também um pouco de python. Caso sinta que não cumpre esses requisitos, relaxe, temos textos sobre esses temas aqui no Turing Talks. Detectar bordas é uma tarefa, muitas vezes nada trivial, mas é algo útil, porque nos ajuda a segmentar objetos de uma forma muito direta em algumas situações. O que é uma borda? Intuitivamente, podemos dizer que uma borda é um “lugar geométrico” que delimita um objeto qualquer. Infelizmente, essa definição é pouco precisa e pouco útil na maioria dos problemas. E por quê? Bom, porque nós queremos usar a lógica inversa: encontrar as bordas e, só depois, delimitar o objeto, ou mesmo que não queiramos delimitar o objeto, ter de fazer isso de antemão só para encontrar as bordas não seria eficiente. Então precisamos encontrar outra forma de definir uma borda para o computador. E como é essa outra forma? Resumidamente, essa outra forma é encontrar os pontos em que a intensidade luminosa muda de forma abrupta. Por exemplo, nessa foto, temos um pedaço preto que, de repente, fica branco. É justamente essa mudança na cor que nos faz identificar uma borda. Nesse caso, nós temos de novo a mudança de cores do preto para o branco. Entretanto não identificamos borda porque essa mudança não é “abrupta”. Intuitivamente, já deve ter ficado claro que a borda tem a ver com essa mudança “rápida”. Mas para definirmos alguma forma de o computador aplicar isso, precisamos ser um pouco mais precisos do que isso. Se você já estudou cálculo na vida, a palavra “mudança rápida” deve ter acendido uma luzinha na sua cabeça. Exato, vamos falar de gradiente. O gradiente é um vetor (você pode imaginar como uma setinha) que aponta para a direção que uma função tem o maior aumento. Se em um ponto esse gradiente (mais precisamente o módulo dele) é muito grande quer dizer que ali há uma taxa de variação grande no valor da função. Tá, mas nós não estamos falando de uma função aqui: é uma imagem. Como você vai utilizar a noção de gradiente em uma imagem? O que fazemos é utilizar alguns tipos específicos de kernel em uma convolução, esses kernels medem essa variações, fazendo aproximações numéricas para os gradientes. Operador de Sobel O operador de Sobel calcula separadamente os gradientes nos eixos x e y, por meio das seguintes matrizes 3x3 e ele faz isso usando simples convoluções das seguintes matrizes: A ideia aqui é fazer a diferença entre os pixels da direita e da esquerda, no caso do gradiente horizontal, assim detectar mudanças abruptas. Fica mais fácil entender com um exemplo, vamos conferir abaixo: Como você pode observar, ao encontrar uma diferença entre os pixels um lado acaba se sobressaindo ao outro gerando um maior valor na nossa convolução. Também é interessante notar que ele se dá muito bem com bordas horizontais mas não consegue diferenciar bem as verticais, por isso precisamos das duas matrizes. Combinado então esses dois resultados, por meio da distância euclidiana simples temos o seguinte resultado É interessante notar também que por termos a intensidade de cada eixo podemos plotar também o ângulo da borda usando o arco tangente. Sendo as fórmulas principais: Com o OpenCV podemos facilmente aplicar esse filtro: Os operadores de Sobel já ajudam e muito a indicar bordas, mas nem só de bordas vive um limitador de corpos. No caso dos cartoons apresentados a borda delimitam exatamente a divisão entre o rosto, o fundo e a camiseta. Em casos mais complexos, como fotos de máquinas ou pessoas, os operadores podem marcam qualquer mudança, como rugas no ferro ou pequenos amassados na camisa como bordas, que não são efetivas para separar os componentes da imagem. Para isso foi criado o algoritmo de canny, que pode ser simplificado em 4 passos: Parece bastante coisa mas vamos detalhar e entender mais um pouquinho de cada um desses passos. Passo 1: Filtro gaussiano Lembra do ruído? Pequenos amassados da camisa que são detectados como bordas? A ideia de passar um filtro gaussiano antes é “esparramar” esse ruído ao redor diminuindo assim a sua presença. No caso de bordas fortes esse filtro não será suficiente para o fazer desaparecer. Passo 2: Achar os gradientes de intensidade Como já foi detalhado acima, costumamos usar o operador de sobel para calcular o gradiente. Passo 3: Reduzir a largura de borda Como forma de simplificar o resultado final costumamos reduzir a borda pra minima possivel. Fazemos isso começando de uma borda e a partir do seu sentido, retirado dos gradientes pelo arctg, procuramos traçar seus paralelos. A partir daí teremos algo como uma distribuição normal da intensidade das bordas, então pegamos o meio disso e deixamos como a borda principal (que resume todas as outras) Passo 4: Histerese, tolerância é a chave Aqui nós vamos decidir o que é realmente borda e o que é ruído. Uma primeira abordagem é definir um limite, acima dele tudo é uma borda verdadeira e abaixo dele apagamos tudo. O problema é que fica difícil definir um limite aceitável para todos. Partimos então para a abordagem realmente utilizada, definimos dois limites, um superior onde certamente a borda é verdadeira, um inferior onde certamente é falsa, e o “talvez”, a região entre esses dois limites que apenas é considerada como borda se estiver ligada a uma borda certamente verdadeira. Juntando tudo isso temos nosso algoritmo. Você não precisa se preocupar em como vai fazer tudo isso, o OpenCV já tem um método pronto para você. Operações morfológicas são uma classe de operações não lineares que nos permitem remover ruído de uma imagem e extrair a forma/ estrutura da imagem. No geral, essas transformações são aplicadas em imagens em escala de cinza ou binárias. É muito comum aplicá-las após a extração de bordas. Na essência o que vamos fazer é uma operação lógica com os nossos pixels. Baseadas num chamado “elemento estruturante”, que vai nos dizer o que considerar nessa operação lógica. Essas operações lógicas, para o escopo desse texto, são E e OU. Também, para o escopo deste texto, vamos considerar imagens binárias, que só possuem valores de 0 e/ou 255. Elemento estruturante O elemento estruturante será nossa máscara para aplicação das operações lógicas. Esse elemento, portanto, é uma matriz, com valores (no nosso exemplo 0 ou 255). Os valores em 255(no nosso exemplo inicial) sendo os que vamos considerar para a operação lógica. Ele tem uma posição central, chamada origem, na qual vamos fazer ou não uma mudança a depender do resultado da nossa operação lógica. De forma mais clara, podemos exemplificar uma operação OU com elemento estruturante: O elemento estruturante pode possuir diferentes formas, a depender do efeito desejado na imagem. Caso você ainda não tenha entendido, fique tranquilo, caro leitor, com as explicações de dilatação e erosão, as coisas devem ficar mais fáceis. A dilatação é uma operação morfológica que utiliza o operador OU no elemento estruturante. Vamos fazer um exemplo: Vamos supor que temos essa imagem e esse elemento estruturante (em formato de cruz). Vamos começar a nossa operação no canto superior esquerdo. Podemos verificar que na primeira sobreposição, nenhum dos valores da imagem (todos 0 são diferentes de 255) bateu com os valores do elemento estruturante. Dessa forma, obtemos um falso e não fazemos nada. Andando uma posição para o lado, vemos que o valor de baixo da cruz do elemento estruturante se sobrepõe a um 255, logo a operação de OU é verdadeira, porque pelo menos uma das sobreposições foi satisfeita. Assim, mudamos o valor do pixel sobreposto à origem para 255. Seguindo mais alguns passos nessa operação: Até que, por fim, teríamos o seguinte resultado: Como podemos ver, a parte branca da imagem “dilatou”. No openCV, dilatando os valores brancos com um elemento estruturante retangular, temos o seguinte: Como vimos, a dilatação pode aumentar bordas e preencher buracos. A erosão faz o oposto disso, ela remove detalhes finos. Para fazer isso, nós vamos utilizar a seguinte regra: a cor do pixel central (sobreposto à origem do elemento estruturante) só poderá ter a cor igual à da origem se todos os pixels analisados forem um match com o elemento estruturante. Vamos partir da nossa imagem dilatada e utilizando o mesmo elemento estruturante, vamos ver o que aconteceria. Nessa primeira sobreposição, vemos que não são todos os pixels que dão match com o elemento estruturante. Assim, o elemento central não pode ser 255. Como ele já é 0, não fazemos nada. Nesse caso, novamente, os valores não batem integralmente. Como o pixel sobreposto à origem era de 255, mudamos seu valor para 0. Mais alguns passos e temos: Por fim, obtemos o seguinte resultado: No OpenCV, podemos aplicar uma erosão com um elemento estruturante retangular da seguinte forma: Opening é um nome dado ao processo de erosão seguido de dilatação (utilizando o mesmo elemento estruturante). É útil na remoção de ruído da imagem. O Closing é o contrário do opening, ou seja, é uma dilatação seguida por erosão. Ele pode ser utilizado para fechar pequenos buracos dentro dos objetos em primeiro plano ou pequenos pontos pretos na imagem. Gradiente Morfológico Essa é a diferença entre a dilatação e a erosão de uma imagem. A dilatação “engorda” as bordas, enquanto que a erosão “apaga” elas, assim a diferença dos dois serve como uma forma de aproximar o gradiente numericamente, tal como faziam os kernels visto na primeira parte deste texto. Mais alguma coisa sobre operações morfológicas… Além das apresentadas aqui, existem outras operações morfológicas. O OpenCV também nos permite criar elementos estruturantes customizados, como em formato de elipse. Parabéns por ter chegado até aqui. Esses conceitos podem ser difíceis de digerir, mas são importantes para muitas aplicações. Queremos deixar linkado aqui algumas referências muito úteis: Se você gostou do texto e acha que te ajudou, deixe os claps, siga o Turing Talks no Medium e siga as páginas do Grupo Turing no Facebook e no Instagram, até a próxima!"
https://medium.com/turing-talks/uma-an%C3%A1lise-de-dom-casmurro-com-nltk-343d72dd47a7?source=collection_home---------47----------------------------,Introdução a NLTK com Dom Casmurro,Uma análise de uma das obras mais renomadas de Machado de Assis utilizando uma das principais biblioteca de NLP,Vitoria,958,7,2020-10-18,"Olá, queridos leitores. Bem vindo a mais um Turing Talks! Desta vez, abordaremos um assunto que diz respeito a área de Processamento de Linguagem Natural, mais especificamente, vamos abordar uma breve introdução a NLTK, uma das principais bibliotecas de NLP. Para isso, vamos utilizar uma das renomadas obras de Machado de Assis: Dom Casmurro! NLTK, que significa Natural Language Toolkit, é uma biblioteca disponível na linguagem Python para realizar tarefas de NLP. É uma das principais bibliotecas de estudo para quem está ingressando nessa área, pois nela se encontram datasets e alguns algoritmos essenciais para análise e pré-processamento textual. Por meio da biblioteca NLTK é possível acessar diversos corpus, isto é, dataset de textos. Cada obra apresentada no link contém um id dentro da NLTK, esse identificador será utilizado para acessar os dados via interface python, conforme veremos a seguir: Antes da análise, precisamos preparar nosso texto. Fazemos isso porque, para a máquina, algumas palavras ou estruturas dele não importam e não fazem diferença — essa ideia ficará mais clara ao longo do nosso post, mas para se aprofundar melhor nesse assunto, recomendamos este artigo sobre Introdução ao Processamento de Linguagem Natural, em especial a sessão que trata sobre Pré-processamento. A biblioteca NLTK oferece praticamente todas as ferramentas necessárias para um bom pré-processamento, vamos vê-las a seguir: Existem palavras na construção textual chamadas de stopwords, tais palavras, dentro de uma abordagem de NLP, são irrelevantes e sua remoção colaboram com a analise textual. Alguns exemplos de stopwords comuns no português são preposições (em, na, no, etc), artigos (a, o,os, etc),conjunções (e, mas, etc), entre outras. Quando analisadas pela máquina essas palavras não possuem significância, podendo apenas atrapalhar os processos de aprendizado de máquina. Por esse motivo, geralmente, no pré-processamento removemos essas palavras. A biblioteca NLTK mais uma vez auxilia nesse processo, pois fornece as stopwords de uma língua. Assim, podemos removê-las de um texto, comparando-as com as palavras do nosso texto e mantendo somente as que não são stopwords, como veremos ao longo do texto. Assim como Stopwords, ter verbos conjugados em um texto não faz diferença quando a máquina vai processá-lo. Por isso, existem duas ferramentas chamadas Lemmatização e Stemmatização. Ambas fazem a mesma coisa: Quando passado um texto como argumento, elas reduzem todas as formas conjugadas à sua raiz. A única diferença, entretanto, é que a função que lemmatiza seu texto reduz todos os verbos a forma verdadeira da raiz — por isso quanto maior seu texto, mais tempo essa função demora para rodar no código — , enquanto a função que stemmatiza apenas “corta” as palavras no meio usando a raiz como base, o que pode gerar palavras que não existem. Por exemplo, ao lemmatizar o verbo “andando” teríamos “andar” (sua forma reduzida corretamente), mas ao stemmatizar provavelmente teríamos “anda”, já que ele cortaria o verbo apenas usando como base a raiz, e não fazendo a mudança necessária. Por fim, para entender como essas duas funções funcionam vamos pegar uma frase como exemplo: “Eu fui andando enquanto bebia um café”. Passando ela por um lemmatizador, teremos então “Eu ir andar enquanto beber um café”. Na biblioteca NLTK temos dois stemmatizadores: E um lemmatizador: Outra parte importante no pré-processamento de um texto é a tokenização. Isto é, transformar elementos do seu texto em tokens, ou seja, strings dentro de uma lista — ou, se você não tiver conhecimento de python, transformar todas as palavras do texto em elementos individuais separados por aspas. Podemos tokenizar palavras com word_tokenize, essa função recebe o texto como argumento e retorna todas as palavras do texto em forma de tokens. Já com sent_tokenize, podemos tokenizar as frases do texto, ou seja, dividir o texto em frases. Vale ressaltar que para isso, o texto precisa estar com pontuação, já que essa função utiliza os pontos finais como parâmetro de onde cortar o texto. Um outro pré-processamento necessário é selecionar apenas letras — para evitar que pontuações interfiram na análise — e fazemos isso com REGEX, uma outra biblioteca de Python e, já que o foco desse texto é NLTK, não vamos nos aprofundar nesse assunto. Por fim, nossa função de pré-processamento completa deve ficar assim: Com nosso pré-processamento feito, podemos partir para a análise! Vamos analisar algumas informações relativas ao livro, como contagem de vocabulário, bigramas, frequência de palavras e outros conceitos. Bora lá! Um tipo de análise recorrente quando estamos tratando de dados textuais é realizada por meio de contagem de palavras. Mas, para que essa análise seja bem feita, é necessário que haja o pré-processamento. Por exemplo, suponha que deixamos de remover as stopwords, nesse caso as palavras mais frequentes acabam sendo artigos, pontuação e preposições. Esse é um dos motivos pelos quais processar/preparar seu texto é tão importante! Ali em cima criamos a função de pré-processamento, aqui neste exemplo vamos aplicá-la ao nosso corpus (outra palavra para se referir ao livro Dom Casmurro) e depois plotar um gráfico mostrando quais são as palavras mais frequentes na nossa obra: Essa função serve para nos mostrar em quais contextos determinada palavra aparece, ela nos retorna o contexto que cerca o argumento. Isso é feito colando a palavra no centro das frases onde ela tende a aparecer através da função .concordance, que por sua vez recebe a palavra como argumento. Aqui vamos ver a palavra “capitu” em seus contextos: Usando a função ‘similar’ temos uma lista de palavras que tendem a ocorrer no mesmo contexto. Nesse caso, o contexto são apenas as palavras que ocorrem frequentemente de qualquer lado da palavra. Encontrar bigramas é uma parte importante para a análise de um texto. Mas primeiro, o que são bigramas? São palavras no texto que possuem um sentido único estando juntas, podendo até ocorrer separadas, mas com um sentido quando juntas. Por exemplo, nomes compostos (João Alfredo, Maria Júlia) ou nomes de cidades (São Paulo, Rio de Janeiro, Nova Iorque) são exemplos claros de bigramas, essas palavras podem ocorrer sozinhas, mas ocorrendo juntas possuem um significado. Para encontrar os bigramas de um texto com NLTK, basta chamar a função ‘collocations’: Por fim, vamos analisar uma ferramenta incrível da NLTK: O Gráfico de dispersão. Essa função serve para analisar a ocorrência das palavras ao longo de todo o texto — no nosso caso, do livro — e mostra nos riscos do eixo horizontal quando essa palavra tende a ocorrer nele! Vamos dar uma olhada como plotar esse gráfico e dar uma analisada nas ocorrências: Primeiro, temos que importar a biblioteca matplotlib — aquela famosa biblioteca de python para criar gráficos — , depois, definimos o tamanho do gráfico, neste caso será 15x10, e chamamos a função “.dispersion_plot” que leva no argumento todas as palavras que queremos analisar, aqui escolhemos “capitu”, “mãe”, “olhos”, “seminário”, “amor”, “bentinho” , “escobar” e “ezequiel”: Vamos ver como fica o gráfico? Com esse gráfico fica claro que, embora Capitu não apareça nos primeiros capítulos, uma vez que ela entra na história tende a aparecer durante todo o livro. Também podemos ver que a mãe de Bentinho está presente durante praticamente toda a obra, assim como a ocorrência da palavra “seminário”, parte importante da vida do protagonista. Já Escobar, entra na história apenas a partir do meio do livro, pois ele e Bentinho se conhecem no seminário, e Ezequiel, seu filho, aparece quase ao fim do livro, já que nasce quando Bentinho já é adulto. Nesse Turing Talks vimos algumas funções da biblioteca NLTK para ajudar no pré-processamento e na análise do seu texto, porém, essa foi apenas uma pequena fração da capacidade dessa biblioteca enorme. Recomendamos que, caso você tenha interesse, dê uma lida no NLTK Book — de onde os códigos desse artigo foram tirados — , um livro que introduz a biblioteca desde o básico, como abordado nesse texto, até o avançado, como formas de construir modelos de aprendizado de máquina! Não deixe de acompanhar o Grupo Turing no Facebook, Linkedin, Instagram e nossos posts do Medium =) Bons estudos e até a próxima! Agradecimentos especiais à Julia Pociotti."
https://medium.com/turing-talks/deep-transfer-learning-a145125b754c?source=collection_home---------46----------------------------,Deep Transfer Learning,Aprendendo a utilizar todo o poder do aprendizado,Rodrigo Fill Rangel,1095,8,2020-10-25,"Texto escrito por Rodrigo Fill e Paulo Sestini. Agradecimentos especiais para Felipe Machado. Mais um Turing Talks sobre visão computacional, hoje vamos aprender sobre o poder do Deep Transfer Learning, a tecnologia que está levando Machine Learning a um novo patamar evolutivo. Vamos começar a tratar um tema tão importante primeiro contextualizando os problemas que o Transfer Learning consegue resolver. As redes neurais em Deep Learning resolvem problemas complexos e possuem muitas camadas e, com isso, uma enorme quantidade de parâmetros treináveis. O grande problema de termos muitos destes parâmetros está no fato que quanto mais profunda a rede, maior é a demanda computacional de processamento e de quantidade de dados na base para realizar o treinamento. Em poucas palavras, uma rede profunda tem capacidade maior de aprendizado, mas em troca custa muito, é dificilmente acessível com os computadores que temos em casa atualmente e precisa de quantidades realmente grandes de dados para ser treinada. Dentro deste contexto existem redes deste tipo treinadas por gigantes corporações que detêm recursos financeiros, tecnológicos e de informação, por exemplo a Google, Microsoft e Facebook, para realizar o dispendioso processo de treinamento. Estas redes estão disponíveis online para quem quiser testá-las e utilizá-las para seus próprios projetos, por exemplo: VGG-Face, Detectron2, Openface, entre outras. Surgem a partir destas a ideia de utilizá-las como parte do aprendizado de outras redes que desempenham tarefas diferentes, mas para explicar isso melhor vamos a um exemplo: O nosso sistema nervoso sabe transferir o aprendizado entre tarefas, afinal de contas não aprendemos sempre do zero, tudo que aprendemos em nossa vida é utilizado como base para a construção de novos aprendizados. Vamos pensar em andar de bicicleta, é importante ter noções básicas de equilíbrio que nos ajudam a aprender como se manter em cima de uma, sem cair, podendo finalmente abrir mão das rodinhas. Imagine agora este mesmo indivíduo que aos 8 anos aprendeu a andar de bicicleta, aos 20 resolve tirar carta e aprender a andar de moto, o cérebro dele não irá começar do zero a aprender como se equilibrar em cima de moto, certo? Ele já sabe andar de bicicleta e, salvas devidas proporções, muito do que ele aprendeu aos 8 anos será utilizado agora para que ele aprenda a andar em uma motocicleta. Esse é o objetivo do Deep Transfer Learning, proporcionar que um projeto nosso com tarefa ou base de dados parecida com uma rede já treinada possa se aproveitar dos parâmetros já treinados desta. Apropriamos vários destes parâmetros como de nossa rede. Assim é possível realizar o treinamento em casa, sem deter grande quantidade de poder computacional ou uma base de dados enorme, simplesmente pelo fato que daquela grande quantidade de parâmetros treináveis que um modelo de deep learning possui, uma boa parte será importada desta outra rede, deixando para nosso computador e nossa base de dados treinar apenas uma pequena parte restante. Este processo, além de tornar tudo muito mais veloz, é capaz de aumentar significativamente o desempenho do modelo, mesmo com uma quantidade reduzida de dados, como veremos mais à frente. Veja o exemplo na imagem abaixo: A técnica de Transfer Learning é uma técnica geral para modelos de Machine Learning e possui diversos tipos diferentes. É possível reaproveitar dados e conhecimentos de diversas formas no treinamento de um modelo, porém, neste artigo focamos no Deep Transfer Learning, que é a técnica aplicada em redes neurais, como uma aplicação de Model-Based Transfer Learning, que é o tipo de Transfer Learning onde o conhecimento está na forma de parâmetros pré-treinados, que são transferidos entre modelos. Além de como iremos transferir o conhecimento (nesse caso, por meio de parâmetros), também devemos observar os dados utilizados e as tarefas que os modelos precisam resolver, o que permite levantar diferentes casos de aplicação da técnica. Vamos abordar alguns dos casos mais importantes e utilizados no cotidiano, mas a imagem abaixo explicita vários outros tipos existentes: Transfer Learning Indutivo: Neste primeiro caso temos que os domínios fonte e alvo são semelhantes, ou pelo menos análogos, enquanto as tarefas são diferentes. Assim o modelo tenta se aproveitar dos vieses presentes na fonte. Um exemplo pode ser de uma rede especializada em identificar cachorros em uma foto que pode ser usada para classificar cachorros a partir de sua raça. O domínio é o mesmo, ou semelhante, cachorro, mas a tarefa de classificação não é. Pela imagem é possível ver que há mais subclassificações, dependendo se o domínio possui ou não labels, multitask e self-taught learning. Transfer Learning Transdutivo: No segundo caso temos o exato oposto ao primeiro, aqui as tarefas fonte e alvo são iguais, porém os respectivos domínios são diferentes. Este problema se aplica constantemente em desafios de reconhecimento de objetos, por exemplo, imagine uma rede treinada em reconhecer cães, por outro lado nossa tarefa alvo é o reconhecimento de gatos, reconhecimento é o mesmo em ambas as tarefas, mas o domínio é diferente, já que: 𝐶ã𝑜≠𝐺𝑎𝑡𝑜, apesar que… Transfer Learning Não Supervisionado: Aqui temos um terceiro caso que difere dos outros dois, mas é mais semelhante ao primeiro que ao segundo. Este é o caso quando os labels não estão disponíveis tanto para os domínios alvo e fonte, caracterizando um problema de aprendizado não supervisionado. Mas é semelhante ao primeiro no sentido que aqui também os domínios fonte e alvo são semelhantes, enquanto as tarefas são diferentes. A ideia desta seção é mostrar algumas das abordagens mais utilizadas para treinar seus modelos a partir da transferência de aprendizado. Além disso, como estamos fazendo um texto da série de visão computacional, estas estratégias são baseadas altamente em modelos de redes neurais convolucionais, além de estarem no contexto de deep learning. Uma rede neural convolucional, CNN, geralmente tem uma estrutura padrão de montagem, claro que existem variações, mas ao abordar um novo problema é comum o desenvolvimento de uma mesma estrutura, que consiste em: Se o que eu acabei de falar está em grego para você, leitor, por favor procure dar uma lida neste artigo antes de prosseguir. Uma das estratégias consiste em cortar as últimas camadas do modelo, geralmente as fully-connected, todas ou parte delas, de forma a utilizar a arquitetura restante como parte do seu modelo alvo, e treinar novas camadas classificativas. Iremos fazer isso na parte prática mais a frente, mas por enquanto veja a ilustração a seguir: Esta é uma estratégia que faz muito sentido para o primeiro caso exposto anteriormente, ou seja, de Transfer learning indutivo, visto que se os domínios são semelhantes, as features identificadas pela rede fonte são de interesse para o modelo alvo, enquanto que como as tarefas são distintas, a forma de classificação da rede fonte não é algo interessante para a outra. A partir deste ponto podemos criar novas camadas finais e treiná-las com nossos dados para ajustar a rede à tarefa alvo. Esta estratégia é conhecida como “Off the shelf features”, pelo simples motivo que pegamos as features disponíveis na rede fonte e utilizamos elas diretamente em nossa rede alvo, um questionamento bastante pertinente seria o quão bem tal método consegue performar por si só? Bem, existem várias tarefas diferentes que podem ser analisadas e discutidas, então vamos simplesmente colocar este gráfico aqui que sumariza bastante tudo que temos para falar: Bem impressionante não é? Em muitas tarefas o desempenho é próximo, se não igual, à redes treinadas especialmente para aquela task, como em Scene Image Retrieval ou em Bird Subcategorization, em alguns casos nem foram treinadas redes específicas, sendo o transfer a única abordagem viável no momento. De cara dá pra perceber e ressaltar a importância deste método atualmente para o progresso em machine learning, estamos falando de desempenhos altíssimos com redes muito simples de serem montadas. Vamos abordar um outro tema importante, pode ser considerado uma nova estratégia ou apenas uma continuação da anterior, fica a gosto do freguês, enfim, vamos falar sobre Fine Tuning ou ajuste fino, em tradução livre. A ideia aqui é não só se aproveitar de um modelo fonte, extrair suas camadas finais, mas também treinar alguns de seus parâmetros anteriores, para que eles possam se adaptar um pouco melhor à sua tarefa alvo. Uma abordagem seria “congelar” alguns parâmetros mais genéricos, como padrões geométricos, curvas que podem ser identificadas em diversas aplicações, que são referentes às primeiras camadas convolucionais em uma CNN, e ajustar os parâmetros mais específicos, como aqueles que determinam como a rede reconhecerá objetos e formas mais úteis para a aplicação em questão, ou seja, reaproveitamos os parâmetros que reconhecem padrões mais simples comuns à várias aplicações. Sem dúvida é uma abordagem que gera um pouco mais de custo e requer um poder computacional maior, mas ainda nada próximo à treinar uma rede inteira com pesos aleatórios. Necessita de mais dados, mas se estes estão disponíveis, pode valer bastante a pena. Só uma consideração do que significa “congelar” camadas de uma rede: basicamente é atribuir learning rate igual a zero para essas camadas, de modo que seus parâmetros não serão atualizados e portanto elas não serão treinadas novamente. As camadas que não forem congeladas e possuam parâmetros pré-treinados, irão utilizá-los como parâmetros iniciais para o treinamento, dessa forma, a rede já possui bons pesos em suas conexões neurais, e o fine tuning apenas irá ajustá-los para a nova tarefa. Assim chegamos ao fim de mais um Turing Talks, esperamos que tenham realmente gostado deste tema que é central no desenvolvimento de aprendizado de máquina atualmente. Se você realmente curtiu este texto, por favor deixe seus claps aqui no Medium e não se esqueça de seguir a página do Turing Talks para não perder os próximos textos que serão postados. Siga também as páginas do Grupo Turing no Facebook e no Instagram, até mais!"
https://medium.com/turing-talks/one-shot-learning-vs-classifica%C3%A7%C3%A3o-f59ded6c20c5?source=collection_home---------45----------------------------,One-shot learning vs Classificação,Nem sempre a classificação é a melhor escolha,Luísa Mendes Heise,696,5,2020-10-28,"Bem-vindo a mais um Turing Talks! Hoje, nesse post mais curto de terça-feira, vamos falar sobre o que é one-shot learning e como se diferencia de um problema de classificação clássico. Caso você seja um leitor assíduo do Turing Talks, já deve ter visto alguns posts aqui explicando o que é classificação e alguns modelos que podem ser utilizados para fazer esse tipo de tarefa. Recapitulando um pouco disso, essa tarefa envolve atribuir certos labels a dados, ou seja, categorizar um determinado conjunto de dados em classes. Para isso, você deve ter uma base de treino em que estejam todas as categorias que você deseja prever. Ou seja, se quiser classificar imagens entre cães, gatos e patos, o seu dataset de treino deve ter imagens desses três animais. Não somente você deve ter exemplos de todas as classes, mas vários deles. Se você já treinou um classificador na vida sabe que, em geral, dois ou três exemplos de uma classe não dão conta de treinar um bom classificador, mas sim algo na ordem de centenas ou milhares. Em geral, uma das funções de custo mais utilizada para problemas de classificação é a perda de entropia cruzada (em inglês cross entropy loss). Essa função faz uso das probabilidades das classes, o que casa muito bem com o caso geral, pois o output dos modelos costuma ser um vetor de tamanho M que representa a probabilidade de cada uma das M classes. Ou seja, se eu quisesse classificar se, numa imagem, tenho um cão, um gato ou um pato eu teria, por exemplo, um vetor [0.15, 0.45, 0.4] representado a probabilidade de cada uma dessas classes. Matematicamente, essa função de custo pode ser expressa da seguinte maneira: Em que: Então, utilizando o vetor [0.15, 0.45, 0.4] para representar as probabilidades de cão, gato e pato, respectivamente, numa foto de um gato, teríamos L = -0*log(0.15) -1*log(0.45) -0*log(0.4) = 0.79. Vamos supor agora que querem instalar um sistema de reconhecimento facial no seu prédio residencial ou comercial para que a entrada no condomínio seja liberada. A abordagem por classificação seria tirar várias fotos de cada uma das pessoas que têm acesso ao prédio para montar o dataset e então treinar um modelo. Bom, não é difícil enxergar várias inconveniências nesse processo…Por exemplo: Nesses casos é melhor utilizar uma abordagem por one-shot learning. O princípio aqui é que o modelo deve aprender uma representação que nos permita extrair similaridade entre duas fotos, e não atribuir um label de forma direta a uma foto. Então, na prática, quando uma pessoa tentar acessar o prédio, nós faríamos a captura de uma imagem dela e, com a ajuda do nosso modelo, computaríamos scores de similaridade entre a foto da pessoa e as fotos de cada uma das pessoas autorizadas. Se algum desses scores fosse maior do que um certo número (99% similar, por exemplo), diríamos que a pessoa estaria liberada para o acesso. Caso contrário, ela estaria barrada. Em processamento de linguagem natural, essa mesma abordagem pode ser utilizada para computar similaridade entre perguntas, por exemplo, e assim, dizer se duas perguntas são ou não duplicatas. Nesse caso, as perguntas “Qual é a sua idade?” e “Quantos anos você tem?” devem ter uma alta similaridade, enquanto “Qual é a sua idade?” e “Qual é a sua cor favorita?” devem ter baixa similaridade. Utilizando deep learning, uma abordagem que pode ser utilizada é a de redes siamesas. O objetivo aqui é aprender representações para os dados que permitam o cálculo de métricas coerentes de (dis)similaridade, tais como distância euclidiana ou similaridade de cossenos. Em outras palavras, se fôssemos considerar o exemplo do prédio, dada uma imagem de uma pessoa, gostaríamos que o vetor gerado tivesse alta similaridade com outros vetores gerados por outras fotos dessa pessoa e baixa similaridade com vetores de fotos de outras pessoas. No caso das redes siamesas, a similaridade entre dois inputs é obtida por meio da comparação dos outputs dados pela rede neural. É importante frisar que os dois inputs passarão pela mesma rede. Bom, mas e durante o treino, como otimizamos essa rede para que ela produza vetores com alta similaridade para inputs da mesma classe e baixa similaridade para inputs de classes diferentes? Nesse caso, vamos otimizar o problema para que a função de custo triplet-loss seja minimizada. Nessa função de custo, a similaridade entre imagens de mesma classe é premiada, enquanto a similaridade entre imagens de classes diferentes é penalizada. Para isso, são utilizadas as representações para três imagens — uma âncora, uma da mesma classe da âncora e outra que não seja da mesma classe da âncora — para realizar o cálculo do custo. Assim, para calcular gradientes para otimização da rede é necessário computar o forward pass para três inputs. Então no caso em que estamos utilizando a distância euclidiana para medir a dissimilaridade (pois quanto menor mais similar) entre as nossas imagens, a fórmula da triplet-loss fica: A intuição por trás dessa fórmula é de que queremos que a parcela ||f(A) — f(N)||² ,que se relaciona à distância entre a âncora e o exemplo negativo, seja proporcionalmente maior do que a parcela ||f(A) — f(P)||², que se relaciona à distância entre a âncora e o exemplo positivo. Dessa forma, idealmente, essa diferença tende a ficar negativa. O fator alpha representa uma margem para essa diferença. Por fim, o máximo entre esse cálculo e o zero é feito, já que não queremos que a função de custo seja negativa. Nesse Turing Talks vimos as diferenças entre classificação e one-shot learning, explorando um pouco as funções de custo costumeiramente utilizadas em cada um desses casos. Não deixe de acompanhar o Grupo Turing no Facebook, Linkedin, Instagram e nossos posts do Medium =) Bons estudos e até a próxima!"
https://medium.com/turing-talks/extra%C3%A7%C3%A3o-de-paleta-de-cores-com-k-means-clustering-1c15010bbc4e?source=collection_home---------44----------------------------,Extração de paleta de cores com k-means clustering,Implementando k-means clustering para a extração de cores,Luís Henrique de Almeida Fernandes,947,5,2020-11-01,"Texto escrito por Rafael Araujo Coelho e Luís Henrique de Almeida Fernandes Sejam bem vindos a mais um Turing Talks sobre Visão Computacional! Hoje apresentaremos a extração da paleta de cores de uma imagem a partir de k-means clustering, um algoritmo de clusterização muito simples e popular. Você já deve ter visto alguma aplicação desse algoritmo mesmo sem saber. Ele é utilizado para gerar as paletas de cores que ficaram populares na internet recentemente. Ao final desse texto, teremos apresentado o funcionamento do k-means, sua implementação e a extração da paleta de cores de uma imagem. Algoritmos de clusterização visam a separar um conjunto de dados em grupos, conhecidos por clusters, de forma que os elementos de um mesmo agrupamento apresentem alguma característica semelhante. Cada um desses clusters possui um centroide, ou seja, um elemento central que representa seu respectivo grupo. No final, queremos maximizar essa diferença para que os grupos sejam o mais distintos possível. O método k-means clustering é um algoritmo de aprendizado de máquina não supervisionado bastante prático e simples de se entender. Primeiramente, a quantidade de clusters K deve ser determinada (daí que vem o ”k” em seu nome). Então, selecionamos K elementos dentro do nosso conjunto de dados aleatoriamente e estes serão os nossos centroides iniciais. O próximo passo é realizar iterações até que encontremos os centroides corretos. Estas iterações seguem a seguinte ideia: Obtidos os novos centroides, podemos fazer mais uma iteração. Esse processo deve ocorrer indefinidamente até que a variação entre os centroides antigos e os novos seja muito pequena ou até que um número específico de iterações seja atingido. Nossa implementação vai ser construída em cima de imagens com três canais de cores, RGB (veja mais sobre aqui). Isso tem algumas implicações para o funcionamento do algoritmo. Sendo C referente ao centroide, E ao píxel analisado e R, G e B, as coordenadas de cores (canal vermelho, verde e azul). Agora, sabendo o que está por trás do algoritmo e da implementação, podemos começar a escrever nosso código em python. Em primeiro lugar, vamos importar todas as bibliotecas que precisamos As principais funções criadas para esta implementação são apresentadas abaixo. Elas realizam todo o processo de iteração enunciado. O código completo se encontra em nosso Github. Com o algoritmo implementado, podemos enfim vê-lo em ação em uma imagem. Para isso, precisamos importá-la e mudar suas dimensões para rodarmos o algoritmo em menos tempo. Assim, estamos quase prontos para gerar nossa paleta. Mas antes, é interessante criarmos uma visualização que facilite nosso trabalho. Mencionamos anteriormente que as cores RGB da imagem podem ser interpretadas como coordenadas de um vetor. Se mapearmos o vetor equivalente a cada píxel no espaço, obtemos uma boa maneira de entender como o algoritmo vai funcionar. A partir dessa projeção 3D, observamos a distribuição das cores da imagem. Há uma grande concentração de cores quentes, como amarelo, laranja e vermelho. Além disso, muitos pontos roxos e também azuis. Dessa forma, podemos escolher uma quantidade de clusters condizente. Portanto, utilizaremos K = 5. Então, podemos aplicar o método em nossos dados, obtendo os centroides dos clusters, seus respectivos elementos e a quantidade de iterações necessárias até a convergência. A imagem obtida, com a paleta de cores é apresentada abaixo Percebemos que os centroides encontrados pelo algoritmo são próximos às cores que apontamos durante a visualização. Quanto às iterações, vemos os passos até a convergência com base na evolução dos centroides calculados no GIF a seguir: Nesse Turing Talks vimos que o k-means é um algoritmo bastante eficiente para essa aplicação, apesar da sua simplicidade. Pelo GIF, notamos que são necessárias aproximadamente 8 iterações para chegar a um bom resultado. Não se esqueça de seguir o Grupo Turing no Medium, Facebook, Instagram e LinkedIn. Além disso, faça parte do nosso servidor do Discord, Comunidade Turing dedicado à disseminação e ao estudo de Inteligência Artificial."
https://medium.com/turing-talks/an%C3%A1lise-de-um-portf%C3%B3lio-de-a%C3%A7%C3%B5es-em-python-1a5e0b3455fc?source=collection_home---------43----------------------------,Análise de um Portfólio de Ações em Python,“Don’t put all your eggs in one basket”,Thalissa Reis,880,10,2020-11-15,"Texto escrito por Diego Lottermann, GuiCola e Thalissa Reis No Turing Talks de hoje, vamos retomar as discussões sobre finanças quantitativas, desde a extração dos dados até o cálculo de retorno, só que, agora, de um portfólio de ativos. Recomendamos que, antes de explorar este maravilhoso conteúdo, você dê uma conferida no artigo deste link, no qual exploramos os mesmos temas só que relacionados a um ativo e não a um portfólio. Fica o mesmo aviso do último texto: você, que não sabe nada sobre finanças quantitativas ou programação, fique tranquilo, pois o propósito deste artigo é, justamente, tratar de conceitos básicos. A primeira etapa, ao trabalharmos com um portfólio, é coletar os dados! Se você leu o último artigo, você já sabe que não existe um caminho único para obtenção dos dados, pois séries temporais com preços de fechamento de ativos podem ser obtidas de várias maneiras, como em arquivos csv em sites específicos ou em API´s diversas. No presente artigo, coletaremos os dados da API do Yahoo Finance, devido ao antigo postulado BB (bom e barato — no caso, gratuito). Existem outras API´s interessantes, mas que demandam cadastro para obtenção de uma key, como o Alpha Vantage, ou são pagas, como a Morningstar e a IEX. Para obtenção dos dados no Alpha Vantage seria necessário realizar um cadastro no site da API (url). Dessa maneira, você obterá uma key, utilizada para validação da coleta de dados. Se você quiser visualizar um exemplo de extração através do Alpha Vantage, é só dar uma conferida no nosso último artigo! Já nos decidimos pelo Yahoo Finance, então vamos importar algumas bibliotecas! A primeira biblioteca (também conhecida como o primeiro cavaleiro do apocalipse) é o pandas, muito útil em diversas aplicações no universo dos dados. Além dele, devemos importar seu adicional “data”, o leitor de dados do pandas, o qual chamaremos, carinhosamente, de wb em homenagem a Warner Bros. Após as importações, é necessária a criação do seu data frame (local de armazenamento de seus dados) e de uma lista com todos os tickers das ações que você deseja adicionar ao seu portfólio. É importante salientar que o ticker escrito deve ser equivalente ao ticker utilizado pelo Yahoo Finance (não necessariamente coincide com o ticker oficial). No presente artigo, vamos fazer uso de algumas empresas aéreas: Azul (AZUL), Latam (LTM- LTMAQ no Yahoo Finance), Gol (GOL) e American Airlines (AAL). A título de curiosidade, os tickers de empresas e ETF’s do Brasil possuem sempre “.SA” no final, como BOVA11.SA por exemplo. O padrão se repete em outros países, com sufixos que diferenciam a origem do ticker. No presente artigo, por mais que o Yahoo Finance seja capaz de retornar uma série de dados, vamos fazer uso apenas do preço de fechamento das empresas X, Y, Z. Dessa forma, atribuímos todos os tickers das empresas em questão a uma variável e requisitamos os dados através desta com a limitação do Adjusted Close (Fechamento Ajustado). Além de configurar a limitação “Adj Close” à nossa coleta de dados, temos que definir a fonte de dados (data_source=’yahoo’) e o período que queremos (‘start=’mm-dd-aa’). Vamos ver se deu tudo certo? Sim! Conseguimos! Contudo, nossa missão está apenas no começo. Todos os portfólios devem possuir ativos com determinadas representatividades, chamadas de “pesos”. Você, como investidor, pode optar por possuir a mesma porcentagem de seu patrimônio em cada modalidade de investimento ou pode optar por customizá-la de acordo com suas demandas. Os processos pelos quais os pesos podem ser definidos são diversos, contudo os meios mais usuais são análises de correlação e relações risco-retorno. Tais temas serão abordados em futuros Turing Talks com maior profundidade, então, aqui, vamos nos concentrar na definição “manual” dos pesos, através da criação de uma matriz de pesos, usualmente chamada de “weights”. Para defini-los, basta atribuir o valor do peso a lista “weights” na mesma sequência em que os ativos foram colocados na lista “tickers” na coleta de dados. Em uma determinada situação, poderíamos atribuir 25% de peso a cada ativo em nosso portfólio composto por quatro ativos. Após aprender a coletar os dados e a atribuir pesos, vamos ao que interessa! Agora que já sabemos como importar preços históricos e como calcular o risco e retorno de ativos individuais, podemos usar esse conhecimento para começar a montar o nosso portfólio! Como visto no artigo anterior, a fórmula do retorno de um ativo é muito simples: E sua forma acumulada: Logo, basta calcular os retornos de cada período para termos o retorno total. Entretanto, para carteiras com mais de um ativo é necessário ponderar o retorno de cada ativo pelo seu respectivo peso. Para fazer isso, vamos exemplificar com uma carteira de 4 ações diferentes: Azul (AZUL), Latam (LTMAQ), Gol (GOL) e American Airlines (AAL) no período de dois anos (28/08/2018 a 28/08/2020). Cada ação possui um peso, ou seja, o quanto ela representa do total da carteira. Evidentemente, a soma desses pesos deve totalizar 1. Por exemplo: w = [0.3, 0.3, 0.2, 0.2], isso significa que temos 30% do nosso capital investido em AZUL, 30% em LTMAQ, 20% em GOL e 20% em AAL. Existem algumas maneiras de se calcular o retorno de um portfólio em um período t, nesse artigo vamos calculá-lo pela seguinte fórmula vetorizada: Onde 𝑟⃗ é o vetor de retornos de cada ativo da carteira no período, e 𝑤⃗ os seus respectivos pesos. O que essa fórmula nos diz? É muito simples, basicamente estamos multiplicando o retorno de cada ativo em cada período (minuto, dia, mês, etc) pelo seu peso e somando-os. Mais especificamente, o resultado da multiplicação do retorno de um ativo em uma data pelo seu peso na carteira é denominado de attribution. Logo, o retorno do portfólio em uma data é a soma dos attributions de cada ativo, ou seja, suas contribuições. Fazer essa conta por multiplicação de matrizes funciona apenas para quando os pesos dos ativos não mudam conforme o tempo passa. Caso os pesos da carteira mudem com o tempo, é necessário fazer a multiplicação “element wise”. Finalmente, note que R será um vetor de 1 coluna e t linhas, onde t é a quantidade de períodos. E como fazemos isso em Python? Veja abaixo: Agora que temos os retornos em cada período, basta fazermos a conta de retorno acumulado para termos o retorno ao final do período. Vemos, portanto, que a performance dessa carteira foi bem abaixo do Ibovespa no mesmo período. Muito provavelmente devido aos prejuízos nesses setores em decorrência da pandemia do novo coronavírus. Finalmente, vamos plotar dois cenários hipotéticos: i. Investir R$1000,00 na carteira ii. Investir R$1000,00 em Ibov Note que o Ibovespa conseguiu recuperar suas perdas, enquanto que nossa carteira não. Vale notar que as regras de retorno simples e retorno logaritmo também se aplicam, e essas foram mais bem detalhadas em nosso artigo anterior de análise de ações. Você provavelmente já ouviu que deve ter um portfólio diversificado. Essa diversificação deve ser feita nos tipos de ativos investidos — não é aconselhável investir em apenas um tipo de ativo (como ações) — alternando entre ações, títulos e fundos imobiliários, por exemplo. Analogamente, o portfólio de ações em si também deve ser devidamente diversificado. Mas você sabe porque é tão importante assim essa diversificação? A resposta curta seria: para minimizar o risco. Nesse tópico, vamos explorar mais a fundo como e porque essa minimização ocorre. Primeiro, cabe analisarmos como é feito o cálculo da volatilidade de um portfólio e como esse pode ser feito utilizando Python (numpy será um ótimo aliado para tal). Tendo visto anteriormente que, para uma ação individual, o risco é calculado através do desvio padrão, o mais intuitivo pode ser pensar que, para calcular o do portfólio, basta calcular o desvio padrão de cada ação e multiplicar pelo seu respectivo peso. Esse até poderia ser o caso se a flutuação de cada uma das ações fossem eventos independentes entre si. Mas, dificilmente, esse será o caso, tendo em vista que, normalmente, as ações oscilam conjuntamente. Dessa forma, a volatilidade de um portfólio será dada por: A matriz de covariância ( C ) é o fator que analisa a variação conjunta de preços de cada par de ativo. Cada covariância é calculada através da seguinte fórmula: Aplicando essa fórmula a cada elemento da matriz de covariância C, a diagonal principal apresentará uma característica interessante: nessa, será calculada a covariância entre a ação e ela mesma, resultando, portanto, na variância da ação. Aqui, cabe lembrar que a variância corresponde ao quadrado do desvio padrão. Assim, fica nítido o porquê de ser operada a raiz quadrada da multiplicação matricial para encontrarmos a volatilidade do portfólio. Sem mais delongas e parte teórica, vamos para a prática! Para tal, vamos considerar dois portfólios com graus diferentes de diversificação para analisarmos o impacto dessa no valor da volatilidade. Primeiro, vamos analisar um portfólio composto por: Azul (AZUL), Latam (LTM), Gol (GOL) e American Airlines (AAL) com pesos homogeneamente distribuídos (todas com 25%) e considerando o período de dois anos (de 28/08/2018 a 28/08/2020). Não precisamos nem plotar uma matriz de correlação para perceber que esse portfólio não é nem um pouco diversificado. Todas as ações pertencem a um mesmo setor e, portanto, tendem a apresentar um padrão de flutuação altamente correlato. Vamos calcular, então, o risco deste: Algumas observações importantes: Uau! Que volátil! O resultado era um tanto quanto esperado haja vista que todas as ações estão relacionadas ao mesmo setor. Vamos olhar essa relação mais de perto? Para isso, usaremos uma matriz de correlação, mas antes, cabe darmos uma relembrada na fórmula de correlação a partir da covariância: Assim, percebemos que a correlação é meramente uma normalização da covariância a partir dos desvios padrões. Ela costuma ser uma forma mais direta de visualização, pois possui limite inferior e superior (varia entre -1 e 1), enquanto a covariância pode variar entre -∞ e +∞. Vale lembrar que, quanto mais próximo de 1 for o valor absoluto, mais forte é a correlação. Vamos dar uma olhada, então, na matriz de correlação do portfólio 1: Não surpreendentemente, as ações apresentam forte correlação entre si. A menos correlata é a American Airlines, o que faz sentido considerando que, das três, ela é a única que não concentra suas operações na região da América Latina. Agora, vamos para o segundo portfólio: esse será composto pela Tesla (TSLA), Bradesco (BBD), Lockheed Martin (LMT) e pela cotação do Ouro (GC=F), com pesos distribuídos homogeneamente (25% para cada) e considerando o mesmo período do portfólio anterior. Sigamos para o cálculo do risco: Bem melhor, não? Vamos dar uma olhada agora na matriz de correlação desse portfólio: Nesse portfólio, as ações apresentam uma correlação bem menor entre si. O par mais correlato deste apresenta o mesmo valor de correlação (0,46) que o segundo par menos correlato do portfólio anterior. Assim, fica evidente como este está bem mais diversificado. Nessa análise, vemos também que a ação menos correlata é a da cotação do ouro. Aqui, cabe uma observação interessante: o ouro é comumente usado pelos investidores como um hedge (ou seja, uma estratégia de proteção para os riscos) contra a inflação e demais instabilidades do mercado. Geralmente, o ouro costuma apresentar um comportamento inverso ao dólar: quando esse cai, o ouro tende a ser mais demandado e, portanto, mais valorizado. Assim, se tudo mais no mercado estiver dando errado, pelo menos, parte do seu patrimônio estaria a salvo com o investimento no ouro. Os pesos dos portfólios exemplificados neste texto, foram escolhidos sem considerar sua forma ótima de distribuição. Mas você sabia que existem teorias e métodos que nos auxiliam na otimização da distribuição de pesos de um portfólio? Mas isso e a aplicação de IA para tal cabe a um próximo Turing Talks! Obrigada por lerem até aqui e até o próximo!"
https://medium.com/turing-talks/como-fazer-uma-an%C3%A1lise-de-sentimentos-com-vader-21bbe3f3e38d?source=collection_home---------42----------------------------,Como fazer uma análise de sentimentos com Vader,O passo a passo para performar uma análise de sentimentos com uma lib importante de NLP,Vitoria,1246,5,2020-11-22,"Para esse artigo é importante que você já possua conhecimentos básicos de Processamento de Linguagem Natural, para isso recomendamos a leitura deste texto. Essa é uma tarefa de NLP que visa classificar os sentimentos de um texto dividindo-os, por exemplo, em Positivo, Negativo ou Neutro (podendo ter mais ou menos categorias, dependendo da tarefa). Mas antes, vamos entender qual seria o benefício de utilizar uma Análise de Sentimentos: Imagine que você trabalha em uma empresa que vende um produto X e gostaria de saber o que o público está achando do seu produto: se estão gostando ou se não estão gostando, ou seja, se as opiniões estão sendo negativas ou positivas. Esse trabalho pode ser automatizado e realizado através da extração de dados sobre o produto das redes sociais (como twitter, facebook, etc) e com uma Análise de Sentimentos desses dados. Interessante, né? Quando queremos assistir um filme podemos pesquisar o que as pessoas estão falando dele na internet, ou seja, se é um filme ruim ou bom. Nesse texto é exatamente isso que ensinaremos o computador a definir: Se a review de um filme foi boa (positiva) ou ruim (negativa). Para isso, usaremos um dataset famoso do site IMDB que possui 50 mil reviews de filmes já rotuladas como positivas ou negativas. Aqui, em um primeiro momento, nós ignoraremos os labels (rótulos) e utilizaremos apenas a coluna das reviews, onde está o texto. Nosso intuito é ensinar o computador a prever esses rótulos sozinho. Depois vamos utilizar as labels reais para medir o quanto nosso computador acertou ao utilizar vader. Em resumo os passos são: Então vamos começar lendo nossos dados! Faremos isso usando a biblioteca pandas do python e lendo nosso DataFrame (nossa tabela de dados), vamos aproveitar e já importar a biblioteca BeautifulSoup para pré-processar o texto: Como podemos ver algumas das reviews estão com tags HTML, isso acontece porque nossos dados foram retirados diretamente de um site e não foram processados, ou seja, limpos. Portanto, para isso, vamos usar a biblioteca BeautifulSoup que importamos para retirar as tags com uma função que pega somente o texto: Para a análise com vader não precisamos realizar mais nenhum tipo de pré-processamento. Então vamos continuar! vaderSentiment (Valence Aware Dictionary and sEntiment Reasoner) é uma biblioteca do Python de código aberto construída para ser usada em tarefas de análise de sentimentos, principalmente aquelas que envolvem dados de mídias sociais. Ela foi construída originalmente para funcionar em inglês — tanto que nesse texto usaremos a biblioteca com dados em inglês — mas existem adaptações para o português e você pode dar uma olhada aqui neste link. A vader funciona de maneira bem simples: ela tem um léxico (uma coleção de palavras) em que cada palavra já possuí uma nota atribuída, e quando passado um documento (frase) retorna os seguintes valores em porcentagem: O compound é a métrica mais importante quando você apenas quer saber se aquela frase é positiva ou negativa, porque seu valor pode ser convertido nessas respectivas categorias e é exatamente isso que vamos aprender a fazer aqui! Para instalar a biblioteca vader podemos usar um pip install: !pip install vaderSentiment Da biblioteca vamos usar apenas a classe SentimentIntensityAnalyzer, então vamos importá-la: Com a biblioteca instalada e importada podemos começar a análise de sentimentos! Para essa análise usaremos o valor compound, como dito anteriormente. Basicamente vamos construir uma função que passa por cada review e calcula seu compound, ou seja, o quanto aquela review é positiva ou negativa. Começando pela função! Vamos criá-la e aplicá-la a coluna “review”, porém guardaremos a resposta numa nova coluna chamada “new_review”: Vamos ver como ficou nossa coluna: Podemos ver que o valor de cada review foi computado corretamente na coluna, lembrando que -1 é uma review muito negativa e +1 é uma review muito positiva, podemos ver que a primeira review (-0.9916) é negativa, enquanto a segunda (0.9670) é positiva. Entretanto, o valor do compound é numérico, e queremos aqui que as reviews estejam classificadas como categorias. Portanto, vamos criar uma função que transforma os valores > 0 (maiores que zero) em labels iguais a “positivo” e < 0 (menores que zero) em “negativo"": Função pronta e aplicada! Agora vamos printar nossa nova coluna new_sentiment: Certo. Análise feita, sentimentos preditos. E agora? O próximo passo é avaliar se a análise desses sentimentos foram boas. Para isso podemos usar uma função da biblioteca sklearn — uma biblioteca própria para tarefas de Machine Learning — que nos retorna o valor da acurácia. A acurácia é uma métrica para avaliar a quantidade de acertos em porcentagem. Aqui ela nos mostrará quantos por cento dos nossos sentimentos foram previstos corretamente. Vamos lá: No Turing Talks de hoje você aprendeu como fazer uma análise de sentimentos do zero utilizando a biblioteca vader! Vimos também uma forma de calcular a acurácia quando você já tem uma base de dados rotulada, onde, por fim, constatamos que nosso modelo conseguiu uma taxa de acerto que ficou por volta de 70%, o que não é um número perfeito, mas já é uma quantidade muito boa. Para fixar os assuntos vistos aqui recomendamos que você treine cada um desses passos com seus próprios dados! Também, se você quiser se aprofundar mais nas outras inúmeras funções da biblioteca vader, incentivamos que você dê uma olhada na documentação dela. Por fim, não deixe de acompanhar o Grupo Turing no Facebook, Linkedin, Instagram e nossos posts do Medium =) Bons estudos e até a próxima! Agradecimentos especiais à Julia Pociotti e ao Guilherme Fernandes."
https://medium.com/turing-talks/sua-primeira-ia-o-problema-dos-k-armed-bandits-cc63732567b2?source=collection_home---------41----------------------------,Sua Primeira IA: o Problema dos k-Armed Bandits,Um exemplo simples de Aprendizado por Reforço,Nelson Alves Yamashita,584,7,2020-11-29,"Para o texto de hoje você não vai precisar de conhecimento prévio na área de Aprendizado por Reforço, só de uma familiaridade com Python. Olá entusiastas de Inteligência Artificial! Sejam bem-vindos a mais um Turing Talks! Há algum tempo aqui no Medium fizemos uma série de posts introduzindo o conceito de Aprendizado por Reforço e inclusive já fizemos um te mostrando como usar essa incrível técnica para ensinar uma IA a jogar Super Mario Bros! Mas hoje decidimos voltar um pouco atrás, falaremos sobre O Problema do k-Armed Bandits (em português, o Problema das Roletas de K Alavancas), um problema que pode — de maneira intuitiva — esclarecer alguns conceitos básicos de Aprendizado Por Reforço e que você poderá implementar! Imagine que um robô vá a um cassino, e nele se depare com uma roleta com 10 alavancas. Cada alavanca tem uma certa chance de devolver uma certa quantidade de dinheiro, na média algumas alavancas devolvem mais dinheiro do que outras. O objetivo desse robô é aprender, a partir de conceitos chave do Aprendizado por Reforço, a puxar a alavanca que mais lhe devolve dinheiro. Podemos generalizar esse problema para qualquer situação em que um agente é apresentado com um número k de escolhas, e após cada escolha ele recebe uma recompensa dentro de uma distribuição probabilística estacionária. O que significa isso? Basicamente, dizemos que uma distribuição probabilística é como as probabilidades estão distribuídas ao longo de algum valor aleatório. Por exemplo, em uma escola de ensino médio, encontraremos majoritariamente alunos distribuídos entre os 15 até 18 anos, com muito menos alunos distribuídos nas outras idades. Como exemplificado neste gráfico fictício: Assim, dizemos que a probabilidade de escolhermos um aluno dentro desse intervalo (15–18) é maior. E que se escolhermos um aluno aleatoriamente, é esperado que o aluno tenha por volta dessa idade. E estacionária basicamente significa que durante toda a simulação do episódio, essa distribuição probabilística não muda. Agora que você sabe o que este termo significa, imagine que cada alavanca possui uma dessas distribuições para suas recompensas, a imagem a seguir ilustra as distribuições para um problema com 10 alavancas: No nosso problema de k-Armed Bandits dizemos que para cada k ações há uma recompensa média esperada; esse valor esperado geralmente é chamado de valor da ação. Ou seja, definimos o valor de uma ação arbitrária a, denotado de q*(a), como uma recompensa em um tempo t (Rt) dado que a ação em t (At) foi a como: (Esse 𝔼 significa o valor esperado, é como se fosse a “recompensa média” — a recompensa com maior probabilidade de acontecer dado a.) Se nosso agente soubesse todos os valores esperados, o problema seria facilmente resolvido: ele simplesmente escolheria a ação com o maior valor. O problema é justamente que ele não sabe esses valores. E para descobri-los ele necessitará realizar o que chamamos de exploração e explotação. Na exploração ele tentará conhecer melhor os valores de cada ação, para que na explotação ele já conheça uma variedade de valores diferentes e assim poderá escolher os melhores. Uma analogia com o mundo real seria o menu de um restaurante: imagine que você pediu um prato lá e acabou gostando deste prato, você poderia sempre pedi-lo quando fosse nesse restaurante e acabaria feliz, porém, se não se arriscar a pedir nenhum outro prato nunca saberá se pode haver um prato do qual você acabe gostando mais! Normalmente em Aprendizado por Reforço nós usamos letras maiúsculas para representar algo aleatório ou um valor estimado, por isso agora estamos utilizando Q maiúsculo, pois ele é uma estimativa. Como nosso agente não conhece os Valores q reais cabe a ele tentar estima-los de alguma maneira. Como estamos buscando um valor esperado — ou seja, a recompensa média — basta calcularmos a média das recompensas recebidas por nosso agente naquela ação: Porém, como na computação seria custoso executar uma somatória toda vez que gostaríamos de atualizar Q podemos fazer algumas manipulações algébricas e cair na seguinte equação: Que é a equação que utilizaremos em nosso algoritmos para estimar Q! Lembre-se que n, nesse caso, vai ser o número de vezes que aquela ação ocorreu. Então teremos um n e um Q para cada ação. Para a explotação vamos utilizar uma função bastante comum em Aprendizado por Reforço, a função argmax. O que ela faz? Ela irá receber a lista de valores Q de cada ação e devolverá o índice do maior valor, dessa maneira escolhendo a ação de maior valor estimado. Se houverem empates, por definição, ela escolhe um dos valores empatados aleatoriamente. Ué, mas se ela devolve a ação de maior valor, não era melhor só usar ela? O problema de só utilizar a função argmax é o de que se o agente não conhece os valores, ele poderá facilmente “viciar” em uma ação que na verdade não é a melhor ação, caindo assim em uma solução subótima. Para evitar esse problema vamos utilizar um método de exploração também bem comum no Aprendizado por Reforço, um algoritmo epsilon-greedy. (em português epsilon-guloso) O que ele faz? Dado um número epsilon menor que 1 e geralmente pequeno, o algoritmo escolherá um número real entre 0 e 1 aleatoriamente. Se esse número for menor que esse epsilon pequeno, o agente realizará uma ação aleatória. Caso contrário o agente escolherá uma ação baseada na função argmax. Em outras palavras, o algoritmo tem uma chance epsilon pequena de fazer uma ação aleatória. Assim, incentivando o agente a testar ações novas aleatoriamente, mas ainda explotando a ação que estimou ser melhor na maior parte do tempo. Ok, teoria é legal e tudo mais, mas agora vamos por a mão na massa! No código a baixo utilizaremos alguns conceitos de orientação à objetos, mas se você não está familiarizado com este conceito, não se preocupe! Ele não é muito relevante para o resultado, só facilita a leitura do código. Esse __init__, é uma função que é chamada junta com o objeto quando ele é criado, ela serve para passar os primeiros atributos dele. Nesse caso são as k -distribuições de probabilidade das recompensas dos Bandits. Aqui definimos o que é geralmente chamado em Aprendizado por Reforço de Enviroment (o Ambiente em português), ele é o “local” onde o agente interage com e toma suas ações. Para nosso agente utilizaremos os seguintes atributos : Os parâmetros epsilon e k_arms serão definidos pelo usuário, enquanto n_arms e Q_values são inicializados como listas de zeros. last_action é inicializado com um ação aleatória. Agora chegamos na parte mais interessante e divertida do Aprendizado por Reforço: treinar nosso agente! Aqui inicializaremos um roleta e um agente de 10 alavancas e 10 braços, com um epsilon de 0.1. Faremos 200 simulações com 1000 steps cada. Depois disso faremos um gráfico com as recompensas médias que demonstrará o quão bem o agente aprendeu a escolher a melhor alavanca. A curva de aprendizado será algo parecido com: Como podemos ver nosso agente consegue aprender razoavelmente bem qual ação lhe da uma recompensa maior! Agora, você pode experimentar outros parâmetros e ver como eles afetam o aprendizado do agente. Que tal escolher um epsilon maior, e ver se o agente aprende mais rápido? Ou escolher um epsilon menor e aumentar o número de steps em cada simulação, será que o agente consegue um resultado melhor? Deixamos esses experimentos como um exercício para você se familiarizar com o problema e o agente :D. É legal notar também que esse não é o único tipo de algoritmo que resolve esse problema, também existem algumas variações dele como o algoritmo de Limite de Confiança Superior e o algoritmo de Softmax, ambas implementações estão em nosso repositório de Aprendizado por Reforço no Github! Obrigado por ter lido até aqui! Esperamos que tenha gostado e contamos com sua presença no próximo Turing Talks! Lembre-se de nos acompanhar em nossas redes sociais! No Facebook, no Linkedin, Instagram, nossas postagens anteriores no Medium e agora nosso novo servidor no Discord! Agradecimentos especiais à Camilla Fonseca e ao Bernado Coutinho"
https://medium.com/turing-talks/extraindo-tweets-da-elei%C3%A7%C3%A3o-municipal-de-s%C3%A3o-paulo-com-python-e-tweepy-c55e63c4ccd3?source=collection_home---------40----------------------------,Extraindo tweets da eleição municipal de São Paulo com Python e tweepy,Um guia para obtenção de tweets de forma fácil e rápida,Luísa Mendes Heise,600,4,2020-12-01,"Não é novidade que o Twitter é uma excelente fonte de dados para análises envolvendo, principalmente, processamento de linguagem natural. Hoje em dia, empresas, políticos e analistas acompanham indicadores como: quanto um certo assunto é comentado na rede, avaliações da polaridade desses comentários (se são positivos, negativos ou neutros) além dos perfis que emitem tais opiniões. Uma das ferramentas que muito facilitam isso é a API do Twitter, que, ao contrário das de outras redes sociais, pode ser considerada relativamente permissiva para extrair informações da rede. A API também pode ser utilizada para criar as famosas contas de bots. No Turing Talks de hoje, vamos mostrar o passo a passo de como extrair tweets utilizando uma biblioteca do python chamada tweepy. Assim como muitas APIs, a do Twitter exige que você tenha uma conta vinculada a uma chave de acesso para a API. Isso porque o acesso tem de ser concedido a você e também existem algumas features e usos da API que são exclusivamente para desenvolvedores que estejam pagando. Para conseguir essa chave é necessário que você tenha uma conta de developer. Para consegui-la é só seguir os seguintes passos: Uma vez que você tenha a conta de developer, para obter a chave: Como já dito, para acessar a API, você vai precisar das chaves, mais especificamente dessas quatro especificadas: consumer key, consumer secret, access token e access token secret. Para utilizar essa informação no seu script, você pode criar variáveis de ambiente (mais recomendado) ou deixar hard-coded como strings no seu código (menos recomendado). Já com a biblioteca do tweepy, você utilizará um objeto que vai lidar com a sua verificação. Depois, será instanciado um objeto api com essa autorização. Nesse post não vou entrar em grandes discussões de como guardar os tweets obtidos, vou fazer da maneira mais simples: guardá-los em uma lista do Python. Entretanto, você deve estudar como proceder no seu problema em específico, uma forma recomendada é utilizar um banco de dados NoSQL, como o MongoDB por exemplo. Um dos usos que você pode querer é avaliar um assunto em específico: as eleições municipais por exemplo. Nesse caso, você pode utilizar uma query (pesquisa) dentro do twitter. Para isso, caso queira pesquisar resultados relacionados a três termos, você deve utilizar uma query com a estrutura (termo1 OR termo2 OR termo3). O código fica assim: Essa requisição nos retorna 100 tweets. Lembrando que há um limite de requisições por intervalo de tempo. Você pode fazer várias delas, se desejar fazer mais de uma requisição sobre o mesmo tópico, basta passar o parâmetro oldest_id para não obter resultados duplicados. Nesse caso, a nossa lista contém alguns jsons (interpretados já como dicionários do Python) com várias informações de cada um dos tweets. Por exemplo, printando um dos itens da lista, temos: Caso queira uma descrição detalhada de cada um desses parâmetros, basta olhar a documentação da API. As querys que você pode utilizar são inúmeras, podendo conter parâmetros como língua, localização e etc. Além disso você também pode utilizar métodos como user_timeline para pegar tweets de usuários específicos. Tweets são informações valiosas que podem ser obtidas de forma relativamente simples. Agora, você sabe como utilizar o tweepy para fazer essa tarefa. Obrigado por ter lido até aqui! Esperamos que tenha gostado e contamos com sua presença no próximo Turing Talks! Lembre-se de nos acompanhar em nossas redes sociais! No Facebook, no Linkedin, Instagram, nossas postagens anteriores no Medium e agora nosso novo servidor no Discord!"
https://medium.com/turing-talks/processando-l%C3%ADngua-falada-com-python-3b3fe31118be?source=collection_home---------39----------------------------,Processamento de língua falada com Python,"Sabemos como processar textos, mas e fala?",Vitoria,807,5,2020-12-20,"Para esse artigo é importante que você já possua conhecimentos básicos de Processamento de Linguagem Natural, para isso recomendamos a leitura deste texto. Na área de Processamento de Linguagem Natural, assim como em outras áreas da inteligência artificial, trabalhamos com dados, ou seja, informações estruturadas a respeito de alguma coisa. Em NLP, eles normalmente estão em formato de texto escrito. Mas então como podemos fazer a máquina compreender o que foi dito em um áudio? Essa tarefa é na verdade bem fácil, pois a única diferença é que você precisa transcrever o texto antes de poder trabalhar com ele e temos muitas bibliotecas feitas exclusivamente para transcrições de áudio. Vamos ver então como podemos fazer isso usando Python! Primeiro, precisamos entender que assim como um filme mostra cerca de 30 imagens por segundo, fazendo com que nosso cérebro registre como uma imagem em movimento, a taxa de amostragem de um áudio é uma medida do número de blocos de áudios por segundo, usados para representar um som no total, sendo 1 kHz = 1000 pedaços de informações por segundo. Isso nos mostra que até o mais pequeno trecho de áudio possui muitos pedaços de informação. Também precisamos entender que existem muitos formatos diferentes de áudio: mp3, m4a, wav, flac, etc. Assim sendo, a transcrição deles será melhor quando você trabalhar apenas com um tipo. Sabendo disso, vamos conhecer um pouco mais sobre a biblioteca que vamos usar! SpeechRecognition library é uma biblioteca do Python para transcrição e reconhecimento de áudios. Ela utiliza ferramentas de detecção de áudios de várias API’s diferentes, como a do Google e da IBM, e para trabalhar com elas você precisa importar e instalar as ferramentas necessárias. Entretanto, as linguagens que essa biblioteca suporta são bem poucas, sendo a principal língua o inglês. Aqui nesse texto vamos usar o recognizer do Google para transcrever um áudio em inglês. Vamos começar instalando a biblioteca: !pip install SpeechRecognition Feito isso já podemos importá-la! Vamos aproveitar e já instanciar o nosso reconhecedor de áudios e definir um threshold de 300, esse valor representa o limite do nível de energia para sons. Valores abaixo desse limite são considerados silêncio e valores acima desse limite são consideradas falas. Se você está tendo problemas porque o recognizer está tentando reconhecer palavras mesmo quando você não está falando, tente ajustar isso para um valor mais alto. Se estiver tendo problemas de não reconhecer suas palavras quando você está falando, tente ajustar isso para um valor mais baixo. Para mais informações sobre o threshold é só dar uma olhada na seção recognizer_instance.energy_threshold deste link. Vamos ver como fica: O próximo passo é ler nosso áudio. Existem muitas bibliotecas do Python próprias para isso, como por exemplo a biblioteca wave que serve para ler e mexer com áudios no formato wav. Porém, a biblioteca que estamos usando aqui já possui uma função exclusiva para esse tipo de leitura, então vamos usá-la: Nosso áudio está salvo como “audio.wav”, ele possui pouco menos de um minuto e o seguinte texto: “Hello, we are the Turing Group. We study, apply and disseminate Artificial Intelligence. One of our most important works is the articles written for Medium, our Turing Talks. I hope you like the material we wrote! Stay tuned for more news.” TRADUÇÃO: “Olá, nós somos o Grupo Turing. Estudamos, aplicamos e disseminamos Inteligência Artificial. Um dos nossos trabalhos mais importantes são os artigos escritos para o Medium, os Turing Talks. Esperamos que vocês gostem do material que escrevemos! Fiquem ligados para mais novidades.” Temos um problema para resolver antes de partirmos para a transcrição. Se printarmos o tipo (data type) do nosso áudio ele será reconhecido como “Audio File”, que é uma das formas que o Python pode ler um áudio: Entretanto, para a biblioteca transcrever, ela exige que o áudio esteja como “Audio Data”, então precisamos converter o data type antes de começarmos a transcrição! É bem fácil, vamos ver: Vamos checar para ver se realmente mudou o data type: Perfeito! Feito isso só nos resta passar o áudio para ser transcrito. Aqui vamos usar a função recognize_google, que utiliza a API do google para transcrever áudios com Python. Precisamos apenas indicar qual arquivo será transcrito no parâmetro audio_data e especificar a linguagem em language. Aqui usaremos o que está guardado na variável audio e vamos colocar a língua como “en-US” (Inglês dos EUA), já que nosso áudio está em inglês. Bora? Nossa transcrição ficou muito boa, mesmo levando em conta que o áudio foi falado por um brasileiro! Agora você pode tratar seu texto transcrito da mesma forma que trataria um texto normal, pode pré processá-lo ou obter insights mais significativos, assim como fazer uma Análise de Sentimentos, por exemplo! Note que o texto já sai com alguns pré-processamentos, como letras minúsculas e nenhuma pontuação, isso já ajuda na hora de trabalharmos com ele para outras tarefas de NLP. Nesse artigo vimos como transcrever um áudio para texto usando uma biblioteca do Python. Essa foi apenas a parte mais básica, uma vez que existem outras muitas funções nessa lib e em outras similares. Para fixar os assuntos vistos aqui recomendamos que você treine cada um desses passos com seu próprio áudio! Também, se você quiser se aprofundar mais nas outras inúmeras funções da biblioteca SpeechRecognition, assim como descobrir as línguas que ela suporta e mais sobre as API’s que ela utiliza, incentivamos que você dê uma olhada na sua documentação. Por fim, assim como disse o áudio, não deixe de acompanhar as novidades do Grupo Turing no Facebook, Linkedin, Instagram e seguir nossos posts do Medium =) Agradecimentos ao Nathan de Moura."
https://medium.com/turing-talks/word-embedding-fazendo-o-computador-entender-o-significado-das-palavras-92fe22745057?source=collection_home---------38----------------------------,Word Embedding: fazendo o computador entender o significado das palavras,Uma introdução a conceitos muito importantes em NLP: embeddings e word2vec.,Camilla Fonseca,883,10,2021-01-17,"Agradecimentos especiais a Julia Pocciotti e Luísa Mendes Heise que ajudaram a preparar os materiais desse texto. Olá, caro leitor, seja bem-vindo a mais um Turing Talks! Se você vem acompanhado nossos textos sobre Processamento de Linguagem Natural (PLN/NLP), mal deve esperar pelo de hoje, pois abordaremos um conceito importantíssimo na área: Word Embedding! Além disso, esse será o primeiro texto de uma série sobre Embeddings e Word2vec. Se você não sabe o que é ou não está muito familiarizado com NLP, recomendamos nosso Turing Talks de Introdução a NLP. Uma lida nele deve ser o suficiente para prosseguir com a leitura de hoje. Sem mais delongas, vamos para o que nos interessa… No nosso texto de Introdução a Bag of Words e TF-IDF, explicamos que para usar um modelo estatístico ou de deep learning em NLP, precisamos transformar o texto em uma informação numérica, mais especificamente um vetor. Se você não sabe o que é um vetor, para o escopo desse texto, é como se fosse uma lista de valores numéricos ou um array/matriz 1xN. Lá, apresentamos duas formas de fazer isso: Bag of Words (one-hot vectors ou frequência dos termos) e TF-IDF. No entanto, esses métodos têm algumas limitações. A primeira é que os vetores obtidos terão o mesmo tamanho do vocabulário (conjunto de todas as palavras que ocorrem em um texto), o que se torna um problema quando temos vocabulários muito grandes, e é comum lidarmos com vocabulários de milhares de palavras ou mais. A segunda é que eles não conseguem captar significado entre palavras, isto é, extraem pouca informação semântica e sintática dos textos. Bem… e de que outra forma poderíamos representar palavras? Uma ideia seria defini-las através de escalas representando alguma informação sobre elas. Por exemplo, poderíamos definir a palavra “rainha” com uma escala “Gênero” que vai de -1 a 1: quanto mais perto de -1, mais feminina a palavra, e quanto mais perto de 1, mais masculina. Todavia, apenas com informação sobre gênero não conseguimos representar bem uma palavra, então poderíamos adicionar mais uma dimensão com uma escala “Realeza”, em que a palavra tem mais a ver com monarca quanto mais perto de 1 e mais a ver com plebeu quanto mais perto de -1. Assim adicionamos cada vez mais dimensões para definir melhor a palavra, construindo um vetor em que cada dimensão serve como uma forma de capturar um tipo de informação sobre o seu significado, e é nisso que consiste Word Embedding! Essa representação vetorial que acabamos de descrever é chamada de embedding, veja exemplos para as palavras “rei” e “rainha”: Mas sair atribuindo valores para essas escalas manualmente para TODAS as palavras em um corpus (conjunto de textos) seria muito complicado, certo? Então, como obtemos esses embeddings de fato? Fazemos o computador ‘aprendê-los’, isto é, usamos algum algoritmo de machine learning para gerá-los, a partir de seu contexto. Meio doido né, mas a intuição por trás disso é que o significado de uma palavra está intimamente relacionado às palavras que em geral aparecem junto a ela. Perceba que, assim, Word Embedding supera as duas dificuldades que tínhamos citado anteriormente (tamanho dos embeddings e pouca informação semântica), pois nós que escolhemos quantas dimensões o embedding terá e cada dimensão terá informação semântica. Quanto ao tamanho ideal do vetor, escolhemos de acordo com o corpus de treinamento: quanto menor, menos dimensões; mas geralmente é um valor entre 100 e 1000. Há vários modos de ‘aprender’ esses vetores, como aplicação de algoritmos de redução de dimensionalidade (PCA, LDA, etc) na matriz de co-ocorrência das palavras, modelos probabilísticos, entre outros. Mas o modo mais popular é treinando redes neurais, como é o caso do famoso Word2vec. Word2vec é um algoritmo para obter word embeddings treinando uma rede neural rasa (com apenas uma hidden layer) com duas arquiteturas possíveis: CBOW ou Skip-Gram. Como o objetivo desse texto é introduzir o contexto de embeddings, se você não entende muito de redes neurais, não precisa se preocupar com isso agora. Falaremos mais em detalhes sobre Word2vec no próximo texto dessa série. De qualquer maneira, se quiser se aprofundar em RNs, temos textos sobre: #1, #2 e #3. Suponhamos que a frase “Fui ao supermercado e comprei abacate” esteja no nosso corpus, para exemplificar. Se escolhermos o modelo CBOW, treinaremos uma rede com a tarefa de prever uma palavra dado o contexto, ou seja, completar “fui ao ___ e comprei abacate” corretamente com “supermercado”. Se escolhermos o Skip-Gram, a tarefa da rede será dada uma palavra, prever o contexto, ou seja, a partir de “supermercado”, prever “fui”, “ao”, “e”, “comprei”, “abacate”. Se você estiver se perguntando: essas tarefas são supervisionadas ou não supervisionadas? Na verdade, dizemos que são ‘self-supervised’, ou auto-supervisionadas, já que a rede aprende por labels, porém não precisamos fazer o labelling, pois elas estão contidas no corpus base. Com a rede enfim treinada, extraímos os embeddings da matriz de pesos da hidden layer (a dimensão dessa matriz é um hiperparâmetro, nós que definimos, então é aqui que escolhemos o tamanho dos nossos embeddings, diminuindo assim a dimensão em relação ao vocabulário). Por levar em conta o contexto, esse algoritmo geralmente é capaz de gerar vetores com valor semântico, de modo que podemos então usá-los para estabelecer relações entre as palavras: o quão semelhantes elas são, por exemplo. Podemos inclusive plotar esses vetores e visualizar essas relações na prática, como na imagem abaixo: Uma das maneiras mais conhecidas de calcular a semelhança entre vetores é pela distância euclidiana, que é a forma padrão de calcular a distância entre eles. Para entender como fazer o cálculo dessa distância, vamos analisar primeiro o caso 2D: Com ajuda da imagem, vemos que a distância d(p,q) corresponde ao menor ‘trajeto’ possível entre p e q: o segmento de reta entre os dois pontos; daí, sua fórmula é nada mais nada menos que a aplicação do teorema de Pitágoras. Para calcular a distância entre vetores de tamanho arbitrário n, generalizamos então essa fórmula: Obs: p1, p2, …, pn e q1, q2, …, qn são as componentes dos vetores p e q, respectivamente. Por exemplo, p2 é o valor do vetor na linha 2. Entretanto, para dimensões maiores ou igual a 4, já não é mais possível fazer a visualização da fórmula como no caso 2. Vale lembrar que, no nosso contexto, estaremos sempre comparando vetores de mesmo tamanho (o valor escolhido para a dimensão do embedding). Dizemos, no entanto, que a distância euclidiana entre embeddings é uma medida de dissimilaridade, ou seja, quanto maior, mais dissimilar, pois esperamos que quanto mais distantes os vetores, menos semelhantes eles sejam. Para exemplificar, usaremos aqui embeddings treinados e disponibilizados pelo NILC, o Núcleo Interinstitucional de Linguística Computacional, que reúne pesquisadores de NLP de várias universidades brasileiras. Você pode baixá-los aqui. Nesse texto, estamos usando os embeddings gerados via CBOW (word2vec) de 100 dimensões. Com o arquivo baixado, usamos os embeddings por meio da biblioteca gensim. Para baixá-la, se você já tiver o pip instalado (e se não tiver, recomendo), basta rodar “pip install gensim” no terminal ou uma célula com “!pip install gensim” se estiver usando um jupyter notebook. Depois, rodamos: Vamos conferir um dos nossos vetores: Para calcular a distância euclidiana, fazemos: Obs: a função do numpy linalg.norm( ) com o argumento ord = None (default) calcula a raiz da soma dos quadrados de cada componente do vetor. Vamos comparar as distâncias d1, entre “mulher” e “pessoa”; d2, entre “mulher” e “homem”; e d3, entre “mulher” e “abacate”: Vemos que d1 é menor que d2 que é menor que d3, o que faz sentido, pois “mulher” é mais similar a “pessoa” do que a “homem”, mas é mais similar a “homem” do que a “abacate”. No entanto, se tratando de vetores de palavras, a direção deles é um componente importante para a aquisição do significado. Por isso, é mais interessante saber o ângulo entre dois vetores do que a distância entre eles. Dessa forma, a medida de similaridade mais usada de fato é a similaridade de cossenos. A similaridade de cossenos é um produto interno normalizado, para quem já estudou álgebra linear. Para quem não sabe o que é isso, basta compreender que ela permite calcular o cosseno do ângulo entre dois vetores, valor que varia entre -1 e 1, sendo 0 quando o ângulo for de 90º. A fórmula que usamos é: Quanto maior o ângulo entre dois vetores (valor entre 0º e 180º), menor o cosseno, ou seja, maior a similaridade. Usamos o seguinte código para calcular a similaridade de cossenos: Vamos calcular as mesmas relações que fizemos com a distância euclidiana: s1, a similaridade entre “mulher” e “pessoa”; s2, a similaridade entre “homem” e “mulher”; e s3, a similaridade entre “mulher” e “abacate”. Agora, s1 é maior que s2 que é maior que s3, o que faz sentido, pois a similaridade de cossenos deve ser maior à medida que as palavras são mais semelhantes. Outra relação interessante que podemos fazer é uma analogia, ou seja, ver que palavra está para palavra x, assim como a palavra y está para a w. O exemplo clássico de uma analogia é: que palavra está para “rei”, assim como “mulher” está para “homem”? Você deve ter prontamente pensado em “rainha”, mas como fazer o computador responder isso? Denominando f(x) como a função de embedding, ou seja, que leva as palavras a sua representação vetorial, uma forma de pensar em uma analogia matematicamente é: f(“mulher”) - f(“homem”) = f(p) - f(“rei”) sendo p a palavra que queremos encontrar. Daí, f(p) = f(“mulher”) - f(“homem”) + f(“rei”) Ou seja, fazemos a diferença entre os embeddings de “mulher” e “homem” e depois somamos ao de “rei”. Então, procuramos o embedding mais semelhante ao resultado (já que dificilmente o resultado vai dar algo exatamente igual a um dos embeddings). Traduzindo em código: .most_similar( ) é um método da gensim que retorna os vetores mais similares de acordo com a similaridade de cossenos. Os argumentos positive e negative recebem uma lista com vetores para considerar positivamente e negativamente respectivamente, refletindo a conta que vimos acima. Vamos conferir alguns exemplos: Conseguimos identificar corretamente que “filha” está para “filho” assim como “homem” está para “mulher”. Repare também que as demais palavras retornadas são todas femininas e tem relação com família, mostrando como o modelo conseguiu absorver significado nesse caso. Nesse exemplo, “nadando” (palavra esperada) ficou em segundo lugar, mas ainda é um resultado bom. Além disso, todas as demais palavras são verbos no infinitivo ou gerúndio. Mas… como saber se nossos embeddings são bons mesmo? Quando fazemos um modelo de machine learning, geralmente usamos métricas como acurácia e F1-score para avaliar seu desempenho, mas com embeddings isso é mais complicado. Nesse caso, temos dois tipos de avaliação: intrínseca e extrínseca. Em uma avaliação intrínseca, analisamos analogias, similaridades e plots para ver se os embeddings capturaram informações semânticas do jeito esperado, o que já começamos a fazer nas duas últimas seções. Mas, vamos dar uma olhada em mais uma analogia um pouco mais complexa: Esse resultado já não é tão bom, “rainha” nem chega a aparecer, o que mostra que ainda precisamos melhorar nossos embeddings. Para isso, poderíamos testar aumentar a dimensão dos embeddings ou outro modelo, como o Skipgram. Para visualizar os embeddings, como estaremos sempre lidando com dimensões bem maiores que 3, é preciso aplicar algoritmos de redução de dimensionalidade para plotá-los, como t-SNE. No entanto, não vamos entrar em detalhes sobre isso nesse texto. Já a avaliação extrínseca consiste em verificar se os embeddings melhoram a performance na tarefa de interesse. Por exemplo, se quiséssemos fazer uma classificador de sentimentos de reviews de um produto, poderíamos fazer um modelo sem e outro com o uso de embeddings e ver se usá-los de fato melhora as métricas do classificador. Poderíamos fazer o mesmo para comparar modelos de embedding diferentes. Ufa! Isso é uma introdução, mas foi bastante coisa né? Vimos a ideia por trás dos word embeddings e seu funcionamento básico, bem como seu potencial de agregar informação semântica a tarefas de NLP! Agora é sua vez: para absorver todos esses conceitos, recomendo baixar embeddings e brincar com similaridades, analogias e etc. No repositório do NILC tem vários modelos diferentes que você pode explorar. No próximo texto dessa série explicaremos mais a fundo o funcionamento do Word2vec, acompanhe nossas redes sociais para não perder! Siga-nos no Facebook, Linkedin, Instagram, Medium e participe do nosso servidor no Discord! E se você curte NLP, dá uma olhadinha nos nossos outros textos sobre que tem muita coisa legal!"
https://medium.com/turing-talks/ensinando-uma-rede-neural-a-jogar-flappy-bird-com-pytorch-2c219a6aecee?source=collection_home---------37----------------------------,Ensinando uma Rede Neural a jogar Flappy Bird com Pytorch,"Utilizando Aprendizado por Reforço com aprendizagem profunda e redes neurais, podemos criar uma IA que derrota o frustrante jogo Flappy Bird",Fernando Matsumoto,763,10,2021-01-24,"Texto escrito por Ariel Guerreiro, Nelson Yamashita e Fernando Matsumoto. Bem vindos entusiastas da ciência de dados e inteligência artificial a mais um Turing Talks! Dessa vez iremos continuar nossa jornada pelos conceitos da emergente área do aprendizado por reforço! Há algum tempo publicamos um texto sobre um exemplo mais simples desse tipo de aprendizado, o Problema dos K-Armed Bandits, porém, hoje iremos fazer um estudo mais aprofundado sobre o assunto. Baseando-nos no algoritmo de Q-Learning, que já explicamos neste post e neste workshop, iremos implementar e explicar um dos algoritmos mais importantes da área — responsável por resultados incríveis, como o de superar humanos em uma variedade de jogos — o Deep Q-Learning. O código desse post está disponível no GitHub e pode ser executado no Google Colab. Porém, antes de pularmos direto para o algoritmo, vamos entender um pouco mais sobre o ambiente com o qual nossa IA irá interagir (o jogo Flappy Bird) e como iremos instalá-lo para nossa simulação. Sobre o jogo: Flappy Bird é um jogo de celular lançado em 2013 pelo desenvolvedor vietnamita Dong Nguyen. O jogo alcançou um rápido sucesso por ser incrivelmente simples e frustrante: sua única ação disponível é tocar na tela e fazer o passarinho subir um pouco enquanto ele está constantemente caindo e se movendo para direita. Com isso o jogador deve desviar de canos vindo em sua direção, a cada cano desviado o jogador aumenta sua pontuação. Como o jogo é simulado no Python: Por conta de sua simplicidade e natureza Arcade o jogo demonstra um comportamento ideal para ser aprendido por uma IA. Por isso, criou-se para a biblioteca PyGame Learning Environment (PLE) uma versão do jogo que podemos utilizar como ambiente de aprendizagem para algoritmos de Aprendizado por Reforço. Como iremos interagir com o ambiente: Uma das bibliotecas mais utilizadas em Aprendizado por Reforço é o gym desenvolvido pela OpenAI. Ela fornece uma série de ambientes de rápida e simples implementação, porém já que em quase toda implementação de algoritmos de Aprendizado por Reforço acaba-se usando seus ambientes, o resultado acaba ficando meio repetitivo e pouco emocionante, por isso optamos por um ambiente mais divertido e inusitado. Assim, utilizaremos uma adaptação do PLE, o gym_ple, que adapta ambientes PLE para o gym, mantendo sua praticidade com ambientes diferentes. Sobre o ambiente: O ambiente está configurado da seguinte forma: Para isso recomendamos que seu python esteja numa versão igual ou inferior à 3.8.5. Também é necessário que você tenha git em seu computador. Caso você esteja no Windows e tenha instalado seu python pela Anaconda basta você abrir o “terminal anaconda” e executar os seguintes comandos: Em OSX e GNU/Linux é só executar esses comandos em um terminal comum. Caso você tenha encontrado algum erro ou problema na instalação, basta entrar em nosso Discord e mandar sua pergunta no canal de Aprendizado por Reforço, ficaremos muito felizes em te ajudar :). Para checar se tudo foi instalado corretamente iremos criar uma espécie de “hello world” do Aprendizado por Reforço: um agente que toma ações aleatórias. Para isso abra um Ipython Notebook e crie as seguinte célula: Provavelmente aparecerão alguns warnings mas se nenhum erro ocorrer, por enquanto está tudo certo e a biblioteca está pronta para uso! Vamos então criar nosso agente aleatório: Se tudo tiver dado certo, você verá o passarinho “se jogando” para o alto, morrendo inúmeras vezes. Isto mostra que o ambiente está sendo renderizado corretamente, e você estará pronto para o próximo passo: entender e implementar o algoritmo de Deep Q-Learning. Obs: se você estiver rodando o código no colab, é necessário comentar a linha env.render. O resto do código continua funcionando, mas não é possível visualizar o jogo. Para entender deep Q-Learning, vamos primeiramente fazer uma breve revisão de Q-Learning. Se você quiser uma explicação mais aprofundada, é só conferir as referências linkadas no começo desse post. Primeiramente, precisamos de algum lugar para armazenar os q-valores. Isso será feito numa tabela, em que cada linha corresponde a um estado, cada coluna a uma ação e cada célula guarda o valor Q(s,a). É dessa tabela que vem o adjetivo “tabular” em Q-Learning tabular. O algoritmo de Q-Learning consiste, principalmente, em dois passos: inicialmente, calcula-se o chamado “valor de bootstrap”, Qbootstrap(s,a), que é uma estimativa bem grosseira do q-valor do estado/ação atual. Uma vez que essa estimativa é feita, utilizamos o valor de bootstrap para atualizar a nossa estimativa de Q(s,a). A nossa atualização vai levar em conta tanto a nossa estimativa antiga quanto o valor de bootstrap, caso o valor não seja o mais representativo do real. Para calcular o valor de bootstrap, lembramos da relação G(t) = R(t) + γ G(t+1). O valor de bootstrap é calculado de forma análoga: onde s’, a’ são o próximo estado e ação. Como estamos interessados na política ótima, a’ é sempre a ação que produz o maior q-valor: Por fim, precisamos incorporar Qbootstrap na nossa estimativa de Q(s,a). Em Q-Learning tabular, isso é feito com a fórmula abaixo (ela não será necessária no resto do post): O Q-Learning tradicional é uma forma de gerar um agente de qualidade, mas, na prática, só pode ser usado para problemas simples. Para problemas mais complexos, se torna inviável guardar todas as combinações de estados e ações em uma tabela. Por exemplo, um videogame, com imagens preto e branco de dimensões 250 x 250 pixels, onde os pixels possuem valores entre 0 e 255 e as ações possíveis são as 4 direções, precisaria de mais de 10¹⁵⁰⁵¹⁵ entradas em uma tabela. Outro aspecto negativo do Q-Learning tabular é a falta de generalização para estados em que o agente não visitou ou teve pouco treinamento, pois o agente usa somente as informações que coletou diretamente sobre aquele estado para tomar suas decisões. Em problemas complexos, é difícil garantir uma boa exploração de todas as possibilidades de estado-ação do ambiente. Buscando solucionar estes problemas, surge a DQN. No lugar da tabela, é utilizada uma rede neural que, dado um estado, calcula os q-valores das ações possíveis. É dessa mudança que surge o nome do algoritmo. Para treinar o agente, não atualizamos os q-valores em uma tabela, mas sim os pesos da rede neural, para gerar os q-valores que melhor representam o ambiente. Assim como na versão tabular, o agente, uma vez treinado, busca tomar as ações que possuem maior valor. As mudanças mencionadas acima consistem na essência do algoritmo. No entanto, para obter uma performance aceitável, é necessário fazer mais uma alteração. O modo como as redes neurais aprendem significa que a ideia de “receber uma recompensa, calcular o Qbootstrap, treinar o agente, e repetir” não funciona muito bem. Para que a rede neural aprenda bem, vamos utilizar um replay buffer, que é basicamente uma memória com todas as transições (estado, ação, recompensa, proximo estado, done). A cada passo dado pelo agente, armazenamos uma transição nova no buffer e treinamos o agente com algumas transições aleatórias do buffer. As vantagens que o uso do replay buffer oferecem são importantes para o aprendizado do agente. Uma delas é que o agente vai ver cada transição várias vezes, reduzindo a chance do agente se “esquecer” de alguma transição antiga. Outra é de que as transições usadas são de diferentes instantes de tempo, reduzindo a influência de uma transição sobre outra, melhorando a performance da rede neural. O pseudocódigo da DQN pode ser observado na imagem a seguir: Inicialmente, vamos importar as bibliotecas necessárias para o nosso programa: A primeira parte do código que vamos ver é o Replay Buffer, que usamos para guardar as transições que o agente observa e para pegar aleatoriamente algumas transições para treinar a rede neural: Ao criar o buffer, delimitamos um tamanho máximo que ele pode ter e criamos listas para armazenar as transições (estado, ação, recompensa, proximo estado, done). O método update é utilizado para armazenar uma transição, onde as entradas de cada lista no valor do index são atualizadas para os valores fornecidos, e o index é atualizado. Por fim, o método sample pega uma quantidade de transições de forma aleatória, determinada pelo batch_size. A próxima parte do código é a rede neural. Nesta implementação utilizamos a biblioteca pytorch. Possuímos alguns textos sobre a biblioteca, que podem ser conferidos neste link A implementação da rede em si é simples: na inicialização, precisamos fornecer o tamanho da dimensão dos estados (input_dim) e o tamanho da dimensão de ações (output_dim). A rede possui uma camada de input, uma hidden layer e uma camada de saída. A ativação é feita pela função ReLU. O método forward é utilizado para calcular os q-valores de cada ação dado um estado, representando por x. Chegamos então no nosso agente em si, que possui algumas variáveis necessárias para o funcionamento do algoritmo, como o valor de gamma, que desconta as recompensas futuras, a taxa de aprendizado, a memória, que é o Replay Buffer como mencionado acima, o número de épocas que a rede deve treinar, a rede em si e seu otimizador e os valores de epsilon. O epsilon é usado para incentivar a exploração de novos estados ao forçar o agente a tomar ações aleatórias. Inicialmente, queremos que o agente tome várias ações aleatórias, para explorar o ambiente. Com o passar do treinamento, o agente já possui um conhecimento do ambiente, então não precisa tomar tantas ações aleatórias. Dessa forma, decaímos o valor de epsilon até chegar em um valor mínimo. O método act é responsável por tomar uma ação. É sorteado um número aleatório entre 0 e 1 e, caso seja inferior a epsilon, será tomada uma ação aleatória, fornecida por action_space.sample(). Caso o valor seja superior, o agente irá calcular os q-valores do estado por meio da rede neural e retornar a ação de maior valor. O método eps_decay cumpre o papel de decaimento, ou seja, diminuição do valor de epsilon, como comentado acima. O método remember é usado para guardar as transições observadas na memória do agente. O método train, por sua vez, é o responsável pelo aprendizado do agente. Nele, são selecionadas aleatoriamente transições guardadas na memória e, comparando a resposta fornecida pela rede neural e a armazenada na memória, é feito o “backpropagation” da rede neural, de forma semelhante à fórmula do Q-Learning tabular. A função train, que não faz parte do agente, é utilizada para a comunicação do agente com o ambiente. O ambiente é reiniciado e entra-se no looping de treinamento. Para um número de timesteps (transições) definido, o agente irá tomar decisões (agir) e o ambiente irá retornar as consequências das ações: a recompensa, o próximo estado e a indicação de se o episódio terminou. Estas transições são armazenadas na memória do agente e em seguida o agente treina, pegando uma amostra aleatória da memória. Se o episódio termina, é calculado o retorno do episódio e o ciclo se reinicia. Finalmente, para executarmos o treinamento, precisamos definir os valores que queremos para os parâmetros, criar o agente e treiná-lo. Aqui plotamos a média móvel dos retornos do nosso agente, podemos ver que ele conseguiu alguns resultados bem impressionantes como pontuações acima de 120: Porém, mesmo com essa alta pontuação, podemos notar uma inconsistência e certa instabilidade das recompensas. Isso se deve há uma deficiência do algoritmo quanto a estimação dos Q-valores, já que estamos criando uma estimativa com base em um estimativa… Tal problema pode ser resolvido, com uma técnica relativamente simples, criando o algoritmo de Double DQN, mas isso fica para outro post! Esperamos que tenha gostado! Se quiser explorar mais assuntos do Aprendizado por Reforço você pode checar nosso repositório no Github! Você também pode entrar em nosso Discord onde estamos constantemente realizando aulas abertas e publicando outros assuntos interessantes :). Você também pode nos acompanhar em nossas redes sociais: Facebook, Linkedin, Instagram, Medium!"
https://medium.com/turing-talks/introdu%C3%A7%C3%A3o-%C3%A0-an%C3%A1lise-de-sobreviv%C3%AAncia-kaplan-meier-plot-e-teste-de-log-rank-com-python-e-r-cdf21bc92ddf?source=collection_home---------36----------------------------,Introdução à análise de sobrevivência — Kaplan Meier Plot e teste de log-rank com Python e R,Como o médico sabe quanto tempo de vida você tem depois de um diagnóstico?,Luísa Mendes Heise,914,8,2021-01-31,"Bem-vinde a mais um Turing-Talks. Nesta semana iremos abordar um tipo de análise estatística muito utilizada na área da saúde (mas não só): a análise de sobrevivência (do inglês survival analysis ou, mais genericamente, time-to-event analysis). Aqui no Turing-Talks você já deve ter lido sobre regressão logística e regressão linear. De modo simplificado, podemos dizer que a regressão logística nos ajuda a modelar a ocorrência (ou não-ocorrência) de um evento (0 ou 1). Ou seja, se trata de uma modelagem de uma variável binária e sua relação com variáveis contínuas. Por outro lado, a regressão linear nos ajuda a modelar uma variável contínua, como altura ou tempo, e seu relacionamento com outras variáveis contínuas. Agora, vamos dizer que eu quero fazer algo que “envolve um pouco dos dois mundos”. Mais especificamente, quero avaliar o efeito de variáveis categóricas no tempo até um evento. Vamos pensar num exemplo: você quer verificar se a presença de um gene influencia no tempo que demora para uma criança começar a falar após o nascimento. Você conduz um estudo por 6 anos, acompanhando um grupo de crianças. Quando o estudo acabar, algumas dessas crianças terão falado, outras não. Algumas delas podem ter demorado 3 anos, enquanto outras 5. Além disso, você pode perder o acompanhamento de algumas (que mudaram de país, ou que os pais só não queriam mais que fossem parte do estudo). Como você poderia lidar com todos esses fatores e, por fim, responder a sua questão? Bom, é aqui que entra a análise de sobrevivência. Antes de entrarmos em maiores detalhes, é importante definir alguns termos que são parte do ‘jargão’ da análise de sobrevivência. Vamos fazer isso nos atendo ao primeiro exemplo: Vamos supor que você foi diagnosticado com uma doença terminal. Provavelmente uma das primeiras perguntas que virá a sua mente é: “quanto tempo eu provavelmente vou viver?” ou então “qual é a chance de eu viver mais de 5 anos”. O gráfico de kaplan-meier permite que ambas perguntas sejam respondidas. Ele faz isso dando a estimativa da função de sobrevivência. O cálculo da função de sobrevivência é feito com a ajuda de life-tables. Tudo começa com a preparação dos dados para que tenhamos uma tabela com a quantidade de indivíduos “vivos” (ou de casos em que o evento de interesse não foi observado) e indivíduos que “morreram” em instantes de tempo t. Partimos do pressuposto de que, no instante t=0, o evento de interesse (nesse caso morte) não foi observado o evento em nenhum indivíduo. Além disso, só é necessário acrescentar uma linha (para um tempo t) se naquele instante “alguém morreu”. O segundo passo é calcular a proporção de “sobreviventes” para cada instante de tempo t. Nesse caso, denotando a proporção de pacientes sobreviventes no instante t como prop(t), temos: E, então: Como não temos o instante 2 na tabela, é assumido: Ou, mais genericamente, para um instante de tempo t entre t’ e t’’ que não está na tabela, temos: E, de maneira genérica: Agora, vamos efetivamente calcular a função de sobrevivência: esse cálculo é feito de maneira recorrente, de modo que para calcular S(t+1) precisamos de S(t). Bom, por definição S(t=0)=100%. Daí, o cálculo de S(1), temos: De maneira genérica, temos: O plot de Kaplan-Meier é apenas a função de sobrevivência VS tempo: Bom, quase tudo claro… Mas agora precisamos entender onde entram os tais dos dados censurados. Quando um paciente é censurado no momento t, sabemos que o paciente estava vivo no momento t, mas não sabemos se o paciente morreu ou sobreviveu. Por esse motivo, os pacientes censurados não são classificados como ‘vivos’ nem como ‘mortos’ no momento t. Nós simplesmente os deduzimos do número de pacientes vivos. Daí a única diferença em relação ao caso anterior é que a fórmula de prop(t) fica um pouco diferente: E no plot de KM, nós identificamos os dados censurados com uma cruz +: Outro conceito muito importante na análise de sobrevivência é o de Hazard. Bem direto ao ponto, podemos dizer que o Hazard é o risco de morte (ou de haver o evento de interesse) em um dado tempo. A partir desse conceito, podemos definir a função de Hazard h(t) (ou de risco), que descreve a mudança desse risco em função do tempo. Por exemplo, se estamos observando o risco de morte de um paciente após contrair COVID-19, é evidente que esse risco (Hazard) muda com o passar do tempo: há um pico após alguns dias do início do quadro, mas o risco de morte decai conforme o passar do tempo, com o corpo produzindo anticorpos e vencendo a infecção. Um tipo de análise muito comum (e útil) em análise de sobrevivência é a de comparar as funções de Hazard de grupos diferentes (pacientes diabéticos e não diabéticos, por exemplo). Ao dividir uma função de Hazard pela outra, obtemos a razão de hazard (ou de risco) que pode ou não ser constante ao longo do tempo. Por exemplo: o risco de morte logo após uma dada cirurgia é 2 vezes maior para pacientes diabéticos em relação aos não diabéticos, entretanto, depois de alguns meses, essa razão muda para 1.5. Nesse caso, dizemos que os hazards não são proporcionais. Caso eles fossem, independentemente das formas das duas curvas de Hazard/risco, uma seria apenas um múltiplo da outra. Esse conceito de Hazards é importante para um tipo de análise de sobrevivência chamado modelo de riscos proporcionais de Cox ou regressão de Cox ou, apenas, modelo de Cox, que deve ficar para um Turing Talks futuro. O teste de log-rank é um teste de hipótese para comparar as distribuições de sobrevivência de duas amostras. Se você não sabe o que é um teste de hipótese/p-valores, sugiro que dê uma olhada nos materiais do workshop de estatística com R do Grupo Turing. Esse teste é utilizado para comparar as curvas de Kaplan-Meier de dois grupos e utilizamos seu p-valor para determinar se essas curvas podem ou não ser consideradas diferentes com significância estatística. A estatística de teste de log-rank compara as estimativas das funções de Hazard (ou de risco) dos dois grupos em cada tempo de evento observado. De modo que a hipótese nula do teste é de que as funções de hazard dos dois grupos comparados é igual: No caso do estimador de Kaplan-Meier, estar “sob risco” significa que aquele indivíduo observado ainda não morreu nem sofreu censura. Para computar o teste, começamos calculando uma tabela de contingência para cada instante de tempo t_j. Essa tabela se constitui de três colunas (grupo 1, grupo 2 e total) computando o número de “pacientes sob risco” e pacientes “mortos” para cada uma das colunas. Daí, para esse instante j, calculamos um chamado w_2j para o grupo 2 da seguinte maneira: Também calculamos a variância do número de falhas do grupo 2 para cada instante de tempo j, com a fórmula: Calculamos, então, uma estatística T. O p-valor do teste pode ser encontrado ao verificar o valor de T na distribuição chi-quadrado com 1 grau de liberdade. Para a nossa aplicação, vamos utilizar um dataset bem famoso do Kaggle: o Heart Failure Prediction. Esse dataset contém dados acerca do tempo de sobrevivência de indivíduos com insuficiência cardíaca associado a outras 12 features. Nesse caso, vamos estudar apenas a presença ou não de diabetes. Vemos aqui que não há diferença com significância entre a sobrevida depois de diagnóstico de insuficiência cardíaca de pacientes com e sem diabetes. Bom, foi bastante conteúdo, mas espero que tenha gostado! Se você tem interesse em ver mais conteúdo de estatística, ciência de dados, inteligência artificial, processamento de linguagem natural, finanças quantitativas e visão computacional, acompanhe o Grupo Turing em nossas redes sociais: Facebook, Linkedin, Instagram, Medium! Você também pode entrar em nosso Discord onde estamos constantemente realizando aulas abertas, divulgando vagas de estágio e efetivas e publicando outros assuntos interessantes :)."
https://medium.com/turing-talks/construindo-sua-primeira-gan-com-pytorch-a47ddcc05649?source=collection_home---------35----------------------------,Construindo sua primeira GAN com PyTorch,Crie sua primeira rede neural que gera imagens!,Wesley Almeida,720,8,2021-02-07,"Olá, seja bem-vindo a mais uma edição do Turing Talks! Nesta edição iremos dar uma introdução às Redes Adversárias Generativas, também conhecidas como GANs. As Redes Adversárias Generativas (GAN) foram introduzidas a comunidade científica em 2014 neste artigo feito por Ian Goodfellow e outros co-autores. Neste tipo de modelo temos 2 redes neurais profundas distintas competindo entre si com objetivos opostos. É interessante notar que Modelos Generativos como as GANs têm um grande potencial, pois elas podem aprender a imitar qualquer distribuição de dados, desde simples números feitos à mão até a geração de rostos super realistas como pode ser visto a seguir. Ficou interessado para entender como esse tipo de rede funciona? Gostaria de entender como você pode aumentar seu dataset? Pois então você está no post certo! Calma, calma! Antes de começarmos a construir de fato nossa GAN, é necessário decidir qual dataset e quais biblioteca serão utilizados. Como esse texto trata-se de uma introdução ao assunto, visando principalmente entender o funcionamento das Redes Adversárias Generativas, vamos utilizar um dos datasets mais conhecidos e simples na área de Visão Computacional: O MNIST. O MNIST é um banco de imagens de dígitos manuscritos composto de um conjunto de treinamento com 60 mil exemplos e um conjunto de teste com 10 mil exemplos. Além disso, as imagens possuem tamanho de 28x28 pixels e apenas um canal de cor (imagens preto e branco). Além disso, para o desenvolvimento da GAN será utilizada a biblioteca PyTorch. Assim, Os imports necessários para o download do dataset e criação das redes é apresentado a seguir, assim como a criação do Dataloader. Se o leitor não está familiarizado com o PyTorch, é possível ter uma ótima introdução à biblioteca e a redes neurais pelo seguinte Turing Talks. Com um conhecimento mínimo de ambos os temas, podemos começar a construir nossa GAN! Como já foi dito no início desse post, as GANs são constituídas de 2 redes neurais profundas distintas conhecidas como Gerador e Discriminador. É possível entender uma GAN como um jogo entre 2 jogadores: a rede Geradora é responsável por pegar um ruído (como um array com valores aleatoriamente gerados) e, a partir disso, tentar gerar uma imagem o mais próximo possível do real para enganar o Discriminador. Já o Discriminador tem como objetivo tentar distinguir se uma imagem é real ou não. Agora que já compreendemos qual o objetivo do Gerador, podemos começar a pensar em como ela gera as imagens a partir de um ruído aleatório. Para facilitar o entendimento, iremos utilizar uma rede neural simples para descrever nosso Gerador. Assim, nossa rede irá receber como entrada um ruído de tamanho arbitrário e, após aplicar transformações não-lineares por meio das camadas da rede, irá gerar um conjunto de saídas do tamanho das imagens do dataset. No nosso caso, por exemplo, as imagens tem tamanho de 28x28 pixels, dessa forma a rede Geradora terá 28x28 = 784 saídas, representando cada um dos pixels da imagem falsa gerada. Primeiro, vamos criar um bloco que representa cada camada que construirá nossa rede Geradora: Em seguida, vamos definir a estrutura do nosso Gerador que utiliza 3 blocos conforme descrito acima: Assim como no Gerador, nosso Discriminador será formado por uma rede composta de camadas totalmente conectadas (neste tipo de camada todas os nós da camada n são ligados a camada n+1). É importante notar que, como o objetivo do Discriminador é receber uma imagem e classificá-la como real ou falsa, a saída do discriminador será binária. De forma análoga ao que foi feito para o Gerador, primeiro vamos definir o bloco base que formará nossa rede: Assim, podemos construir nossa rede Discriminadora: Com isso construímos os 2 blocos principais que compõe uma GAN. Entretanto, ainda é necessário entender como as perdas de cada uma das redes é calculada para podermos treiná-la. Em uma GAN geralmente temos duas funções de perda distintas: uma para treinamento do gerador e outra para treinamento do discriminador, embora derivem de uma única fórmula como será visto a seguir. No artigo que introduziu as GANs, a rede Geradora tenta minimizar a seguinte função enquanto a Discriminadora tenta maximizá-la (daí o nome minimax): D(x): é a estimativa do Discriminador de que a imagem real x seja real; Ex: é a esperança matemática sobre todas as instâncias de dados reais; G(z): é a saída do Gerador (no nosso caso uma imagem) quando sua entrada é o ruído z; D(G(z)): é a estimativa do Discriminador de que uma imagem gerada (falsa) seja real; Ez: é o valor esperado sobre todas as entradas aleatórias para o Gerador. É possível perceber que o Gerador só tem efeito sobre o termo log(1 — D(G(z)) . A partir disso, conseguimos compreender porque o objetivo da rede Geradora é minimizar tal fórmula: D(G(z)) é a classificação do Discriminador sobre as imagens falsas (geradas). Ao treinar o Gerador estamos buscando aproximar tal valor de 1, pois isso indica que nossas imagens geradas estão parecidas com imagens reais. Dessa maneira, queremos aproximar a 1 que na formula indica a minimização delog(1-D(G(Z))) , já que quando D(G(z))tende a 1 1-D(G(Z) tende a 0 e log(0) , em termos práticos, pode ser considerado como um valor muito baixo. Já para o Discriminador precisamos nos atentar a ambos os termos. D(x) , como já foi dito, é a estimativa do discriminador da probabilidade de que a instância real de dados x seja real. Dessa forma, para imagens reais queremos que D(x) seja igual ou mais próximo possível de 1, pois isso indica que o Discriminador está classificando corretamente as imagens como reais. Por outro lado, o termo D(G(z)) para o cálculo da perda do Discriminador deve tender a 0, uma vez que nesse caso estaríamos prevendo corretamente as imagens geradas como falsas. Assim, como queremos que D(x) tenda a 1 e D(G(z)) tenda a 0, estamos buscando maximizar log(D(x)) + log(1-D(G(z))) . Por último, é interessante notar a fórmula acima deriva da entropia cruzada entre as distribuições real e gerada, o que influenciará diretamente na nossa escolha de critério utilizado na implementação. Agora que já entendemos como as perdas são calculadas, a implementação fica fácil! Antes de mais nada, vamos decidir o critério para medir a distância entre as imagens reais e geradas. Nessa caso, utilizaremos uma Binary Cross Entropy (Entropia Binária Cruzada), pois essencialmente nossa perda é baseada na entropia cruzada entre as distribuições real e gerada. Outro ponto que vale ser ressaltado é a criação de uma função para criar os ruídos que serão utilizados para geração das imagens. Veja: A implementação prática da função de perda da GAN e das atualizações do modelo é direta. Para o Gerador precisamos realizar os seguintes passos (É interessante o leitor ir comparando o passo a passo a seguir com a fórmula descrita no tópico anterior): Para o Discriminador, precisamos lembrar que ambos os termos da perda Minimax são afetados. Dessa forma, os seguintes passos são realizados (Assim como para o Gerador, vá comparando com a fórmula do tópico anterior): Antes de começarmos o loop de treinamento em si, precisamos decidir qual otimizador utilizar. Otimizadores são algoritmos ou métodos usados ​​para alterar os atributos da rede neural, como pesos e taxa de aprendizagem para reduzir as perdas. Nessa caso, para ambas as redes utilizaremos o otimizador Adam: Com isso podemos realizar o loop de treinamento. Neste caso iteramos por 100 épocas:: O leitor deve ter visto no loop de treino que utilizamos uma função chamada exibir_imgs. Tal função é utilizada para plotar imagens intermediárias do treinamento e podermos ir acompanhando o desenvolvimento da rede. Para implementar tal função pegamos nosso Tensor de saída da rede que tem tamanho 784 e o transformamos em uma matriz (1, 28, 28) que é o tamanho de uma imagem do MNIST. Além disso, para imagens com 3 canais de cores é necessário inverter os canais, pois o PyTorch utiliza como ordem BGR (blue-green-red) e o matplotlib, por sua vez, usa como padrão a ordem RGB (red-green-blue). A imagem a seguir apresenta algumas imagens geradas durante o treinamento da GAN. Ufa! O caminho foi longo, mas conseguimos construir nossa primeira GAN. Com esse post, o leitor conseguiu compreender toda a base necessária para construir Redes Adversárias Generativas e caso queira já tem totais condições de explorar redes mais complexas como uma CycleGAN ou uma DCGAN. Ficou interessado no tema? Gostaria de saber ainda mais sobre GANs e Inteligência Artificial? Então acompanhe o Grupo Turing em nossas redes sociais: Facebook, Linkedin, Instagram, Medium! Você também pode entrar em nosso Discord onde estamos constantemente realizando aulas abertas e divulgando diversos conteúdos sobre IA."
https://medium.com/turing-talks/otimiza%C3%A7%C3%A3o-de-investimentos-com-intelig%C3%AAncia-artificial-548cf34dad4d?source=collection_home---------34----------------------------,Otimização de investimentos com Inteligência Artificial,,Lucas Leme,1034,8,2021-02-14,"Utilizando paridade de risco hierárquica para montar uma carteira. Texto escrito por: Lucas Leme Santos.O código do projeto está no Google Colab Otimizar é um conceito natural do ser humano. Queremos otimizar nosso tempo, nossa produtividade, nossa vida. E por que não: Otimizar nossos investimentos? Otimização de investimentos é o processo de selecionar a melhor composição de carteira de ativos possível. Não existe carteira perfeita, mas a elaboração de estratégias que maximizem retornos a um dado risco é a prioridade dos investidores modernos. Embora essa ideia pareça simples, apenas na metade do século passado Harry Markowitz desenvolveu um algoritmo — A Fronteira Eficiente — que permite a otimização de investimentos. O maior pressuposto desse algoritmo é que investidores são avessos a risco. Então, dada uma carteira com um certo nível de retorno esperado, a combinação dos ativos com o menor risco possível será a escolhida pelo investidor. Sob essa premissa, investidores só realizarão operações financeiras de alto risco se puderem esperar altíssimos ganhos. Algoritmos de otimização como Markowitz são muito sensíveis a pequenas variações nas entradas do modelo, e esses pequenos erros podem resultar em grandes diferenças nas alocações de uma carteira ideal. Um objeto fundamental para o encontro da fronteira eficiente é a matriz de covariâncias. Com ela é possível analisar as correlações entre todos os ativos e estimar o risco da carteira. Matrizes de covariâncias são muito complexas para serem analisadas pois levam em conta toda a composição da carteira. Isso significa que erros na entrada do modelo acabam afetando o portfólio como um todo. Uma matriz de covariâncias também pode ser visualizada como um grafo totalmente conectado: Para reduzir a complexidade do grafo o ideal é eliminar conexões entre ativos pouco correlacionados e focar apenas nas relações relevantes. Para isso pode-se utilizar uma estrutura topológica conhecida como árvore. Tal estrutura além de reduzir a interferência de erros de estimativa, melhora significativamente a interpretação do grafo. Caso queira saber mais detalhes a respeito da maldição da fronteira eficiente leia o apêndice no fim do texto. O Hierarchical Risk Parity (HRP) é um algoritmo de otimização de portfólios desenvolvido por Marcos Lopez de Prado. Esse otimizador combina teoria de grafos e machine learning para construir uma carteira diversificada com soluções estáveis. O HRP pode ser dividido em 3 grandes passos: Nesta etapa, iremos aplicar uma técnica de aprendizado não supervisionado que é a clusterização hierárquica. Esse tipo de algoritmo visa construir agrupamentos (clusters) segundo um métrica de semelhança entre os dados. Para isso iremos realizar a aquisição dos preços históricos, para montar uma carteira para ser otimizada. Caso queira saber mais sobre aquisição e análise de dados financeiros, leia nosso texto: A clusterização hierárquica será realizada sobre os retornos históricos do ativos da carteira. Para efetuar essa operação temos dois principais hiperparâmetros: método e métrica. Método: Algoritmo utilizado para a clusterização, iremos utilizar ‘ward’ que computa os agrupamentos a partir de suas variâncias. Métrica: Tipo de medida que avalia a semelhança entre os dados, iremos utilizar a distância euclidiana. Apenas a partir dos retornos históricos, o algoritmo conseguiu identificar e segmentar diversos setores do mercado: No artigo original, essa técnica é mencionada como Quasi-Diagonalization. Com a utilização de aprendizado não supervisionado essa técnica pode ser simplificada em um Matrix Seriation, que é um método estatístico de reordenação de matrizes. O objetivo dessa etapa é preparar a matriz de covariâncias para a atribuição de pesos. Utilizando os clusters gerados na etapa anterior, podemos recombinar as linhas e colunas da matriz de covariância, a fim de que ativos semelhantes estejam posicionados proximos. Essa é a última e mais importante etapa do algoritmo, momento no qual os pesos serão atribuídos. A matriz de covariâncias gerada na etapa de Seriation é fundamental, pois ela será utilizada para realizar a iteração nos nós do grafo do dendograma. O algoritmo pode ser representado pelos seguintes passos: 1) Inicialização dos pesos Todos os ativos recebem peso igual a 1. 2) Iteração entre os nós da árvore Com a matriz de covariâncias, percorre-se a árvore selecionando os sub-clusters e sub-matrizes respectivos. O objetivo é realizar a diversificação de pesos entre ativos semelhantes. Olhando o dendograma do passo 1 pode-se exemplificar um hipotético par de carteiras: Carteira 1: Fundos de investimentos. Que são compostos por 2 ativos, ou seja, tem uma matriz de covariâncias V1, com dimensão 2 x 2. Carteira 2: Setor bancário. Que também é composta por 2 ativos, resultando em uma matriz de covariâncias V2. 3) Atribuição de pesos pares Com o objetivo de calcular o risco de cada cluster, iremos atribuir pesos temporários para os ativos do cluster. Dado que cada cluster tem uma matriz de covariâncias (Vi), podemos calcular os pesos da seguinte forma: 4) Determinação das volatilidades A partir da fórmula clássica de volatilidade de um portfólio: 5) Aplicação do fator de peso absoluto O fator de alocação funciona como uma fator de escala entre a sub-carteira e a carteira completa. O fatores de alocação tem um valor entre 0 e 1. O ativos presentes nas sub-carteiras, que foram inicializados com 1, terão seus valores atualizados pelos fatores de alocação. 6) Repetição dos passos 2 a 5 para todos os clusters gerados E na prática? Como essa estratégia performaria no passado? Essas são as perguntas que queremos responder com o backtesting. Vale ressaltar que rentabilidade passada, não é garantia de rentabilidade futura. Porém ao simular uma estratégia de investimento, podemos obter boas informações de como a estratégia performaria em diversos cenários. Além de analisar a performance, o backtesting é importante para comparação entre diferentes estratégias. Em nosso backtesting, iremos comparar 3 otimizadores de investimento: O princípio da estratégia aqui utilizada é simples: mensalmente rebalancear a carteira com os otimizadores citados acima. Segundo o gráfico de rentabilidades, podemos perceber que o hierarchical risk parity teve a melhor performance. Os principais destaques estão no “bull market” de 2019, e no período pós crise da COVID-19 em 2020. Tais resultados não implicam que o hierarchical risk parity é melhor que o Markowitz. Mas sim, que em períodos conturbados, o HRP consegue processar melhor as informações do mercado. Em períodos de grande volatilidade, os dados tendem a ter um menor grau de confiança. A fronteira eficiente por considerar a correlação entre todos os dados, com muito erros de estimativa, tende a convergir em uma solução não ótima. Fato que pode ser observado no período pós crise, momento no qual o Markowitz performou pior que os outras duas estratégias. Nesse texto conseguimos estudar mais um algoritmo de otimização de investimentos, que é o HRP. Ele utiliza inteligência artificial e teoria de grafos para lidar com erros de estimativas dos dados de entrada. O HRP utiliza apenas dados do passado para otimizar sua carteira: e se adicionássemos previsões de dados com machine learning? Isso fica para um próximo Turing Talks… Não deixe de acompanhar o Grupo Turing no Facebook, Linkedin, Instagram e nossos posts do Medium =) Bons estudos e até a próxima! Utilizando multiplicadores de Lagrange podemos expressar essa instabilidade: O objetivo e a restrição de uma otimização podem ser matematicamente definidos da seguinte maneira: Calculando as condições de primeira ordem com derivadas parciais chegamos na seguinte solução para a otimização: E o principal problema que podemos destacar é a necessidade da inversão da matriz de covariânças (V). Para exemplificar considere uma carteira com apenas 2 ativos: A partir do determinante da matriz podemos perceber que quanto mais correlacionados são os ativos da carteira, maior é a chance da inversa ter valores extremos. E portanto as soluções da otimização não convergem. Para solucionar esse problema o HRP estima os pesos da carteira sem a necessidade de uma otimização clássica."
https://medium.com/turing-talks/manipula%C3%A7%C3%A3o-de-s%C3%A9ries-temporais-com-pandas-db4ee39a0c1b?source=collection_home---------33----------------------------,Manipulação de Séries Temporais com Pandas,Seus primeiros passos para trabalhar com Séries Temporais em Ciência de Dados,Julia Pocciotti,840,4,2021-02-28,"Olá, seja bem-vindo a mais um Turing Talks! Hoje iremos abordar os principais conceitos básicos envolvendo Séries Temporais e como manuseá-las utilizando uma das principais bibliotecas para Data Science. Vamos lá! De maneira resumida, uma série temporal pode ser definida como um conjunto de observações ordenadas no tempo, como, por exemplo, os valores mensais de temperatura registrados na cidade de São Paulo, o salário mensal de uma pessoa durante um ano, e claro, o preço de ações em um determinado período de tempo. Antes de qualquer coisa, vamos entender como que o Pandas representa datas e horários. Nessa biblioteca, uma combinação de objetos nativos do Python são utilizadas, como o datetime e dateutil, além do numpy.datetime64. Com essas ferramentas, o Pandas nos fornece um tipo de objeto chamado Timestamp, que representa um único ponto no tempo. Vamos ver um exemplo com uma única data: Uma das maiores utilidades em usar Pandas para trabalhar com Séries Temporais é a facilidade em indexar datas. Com o Pandas, fazemos isso utilizando um objeto DatetimeIndex, que nada mais é do que um agrupamento de vários Timestamps. Com isso, podemos indexar datas em um panda Series ou em um DataFrame: Como vimos, os objetos fundamentais para trabalhar com Séries Temporais no pandas são Timestamp e DatetimeIndex. Além de trabalhar com essas classes diretamente, é muito comum utilizar a função pd.to_datetime()para manusear esse tipo de dados. Vamos ver isso em mais detalhes a seguir. pd.date_range() é um método que retorna uma frequência fixa de DatetimeIndex. Ele pode ser muito útil quando queremos criar nossa própria série temporal, basta passarmos uma data de início e outra de término, e, se necessário, podemos especificar também a frequência na qual queremos esses dados. Vamos ver alguns exemplos: Além dos exemplos que já vimos anteriormente, pd.to_datetime() pode ser usado para converter um DataFrame para uma série datetime: Para exemplo de série temporal, iremos trabalhar aqui com os preços históricos das ações da Google. Temos várias formas diferentes para importar esses dados, mas para esse exemplo vamos utilizar o pandas-datareader: Uma necessidade comum em séries temporais é reamostrar os dados em uma frequência maior ou menor. De maneira geral, existem duas formas principais de reamostragem: Essas duas técnicas podem ser utilizadas em situações nas quais os nossos dados não estão disponíveis na mesma frequência em que queremos as nossas predições. Além disso, a reamostragem pode ser utilizada como um instrumento para feature engineering, pois podemos alterar a frequência dos dados para prover insights em um problema de aprendizado supervisionado, por exemplo. Com o Pandas, isso pode ser feito utilizando o método resample() ou asfreq(). A diferença principal entre esses dois é que o resample() acaba sendo utilizado como ferramenta de data aggregation , enquanto asfreq() é fundamentalmente utilizado para data selection. Vamos pegar os preços de fechamento do Google e comparar o retorno dos dois métodos com a reamostragem dos dados no final do ano: Podemos ver a diferença entre os dois com esse gráfico. Em cada ponto, resample reporta a média do ano anterior, enquanto o asfreq reporta o valor no final do ano. No código acima usamos a string 'BA' como parâmetro dos dois métodos de reamostragem. Além desse, o Pandas oferece uma série de abreviações para diferentes tipos de frequências (ou offsets), você pode conferir todos na documentação da biblioteca clicando aqui. Outra operação comum em séries temporais é mudar os dados no tempo. Pandas possui dois métodos para isso também: shift() e tshift(). A diferença principal é que o tshift altera apenas o index, enquando o shift faz isso para os dados. Usamos o método rolling() de objetos do tipo Series e DataFrames, que retornam uma view similar a que usamos em um tipo de operação groupby. Vamos utilizar como exemplo uma rolling view com média e desvio padrão centrada nos preços das ações do Google: Nesse texto introduzimos os principais conceitos para trabalhar com Séries Temporais utilizando Pandas. Vimos como representar datas e horários e como manipular esses dados utilizando alguns dos principais métodos para Séries Temporais da biblioteca. Caso você queira se aventurar mais utilizando Pandas em Séries Temporais, você pode acessar a documentação da biblioteca aqui. Não deixe de acompanhar o Grupo Turing no Facebook, Linkedin e no Instagram! Além disso, também temos uma comunidade no Discord agora! Sinta-se livre para mandar alguma dúvida ou comentário por lá se desejar."
https://medium.com/turing-talks/ferramentas-para-processamento-de-linguagem-natural-em-portugu%C3%AAs-977c7f59c382?source=collection_home---------32----------------------------,Ferramentas para Processamento de Linguagem Natural em Português,Um guia para as principais bibliotecas para se trabalhar com NLP em português,Julia Pocciotti,988,7,2021-03-07,"Texto escrito por Julia Pocciotti e Vitoria Rodrigues Olá, leitores! Bem vindos novamente a mais um Turing Talks de Processamento de Linguagem Natural! No texto de hoje vamos conhecer um pouco mais sobre as seguintes ferramentas de NLP para a língua portuguesa, com uma breve introdução a cada uma delas: Antes de tudo, não deixem de ler alguns dos nossos outros textos de NLP, eles ajudarão muito em conceitos básicos que serão abordados aqui: Vamos lá? A biblioteca NLTK é uma das mais antigas no meio de NLP e ainda é usada para uma série de tarefas básicas, como remover stopwords, lemmatizar, tokenizar, etc. Por se tratar de uma lib grande com muitas funções, com o tempo ela foi sendo adaptada para outras línguas, entre elas o português. No nosso texto introdutório à essa biblioteca já mostramos diversas funções importantes, inclusive como importar corpus completos do Machado de Assis e fazer um pré-processamento e uma análise com eles. Porém, aqui iremos mostrar mais duas funcionalidades dessa biblioteca: A remoção de stopwords é uma passo importante no pré-processamento de um texto. Essa biblioteca oferece uma variedade de palavras do português classificadas como stopwords e facilita a remoção delas, para isso você só precisar seguir os seguintes passos: Vamos ver quais são as primeiras 20 stopwords: Para rever como fazer a função de pré-processamento que remove stopwords, basta dar olhada no texto que citamos anteriormente. A NLTK oferece diversos tipos de corpus para as mais diversas tarefas, um corpus interessante é o “MacMorpho”, que reúne milhões de palavras retiradas de textos jornalísticos da Folha de São Paulo já com suas determinadas marcações (verbo, substantivo, adjetivo, etc). Para ter acesso ao corpus basta fazer o seu download e seguir esses passos: Para conhecer mais sobre as funcionalidades da biblioteca para português é só dar uma olhada na sua documentação. spaCy é uma biblioteca para Processamento de Linguagem Natural desenvolvida principalmente para ajudar desenvolvedores a construir projetos de forma rápida e prática. Nela, podemos encontrar várias features interessantes para trabalhar com NLP, como tokenizador, POS-tagger, NER, word vectors pré-treinados, dentre outras coisas. Além disso tudo, o spaCy possui vários modelos treinados em línguas diversas, como em chinês, grego, italiano, e claro, português! Instalar o spaCy pode ser facilmente feito usando o pip: $ pip install spacy Para instalar o modelo em português, basta rodar: $ python -m spacy download pt_core_news_sm A partir disso, podemos simplesmente carregar esse modelo pré-treinado dessa forma: nlp = spacy.load(""pt_core_news_sm"") Vamos ver algumas das funcionalidades da biblioteca em português: Caso queira explorar explorar mais a biblioteca, você pode checar a documentação com todos os modelos aqui. Além disso, você também pode conferir a explicação de algumas dessas features neste notebook: A biblioteca Enelvo é muito importante para diversas tarefas de Processamento de Linguagem Natural em português porque possui a capacidade de normalizar textos, ou seja, corrige abreviações, gírias, erros ortográficos, capitaliza letras no começo das frases, de nomes próprios e acrônimos. Ela também possui uma função própria para remover pontuações e emojis. Para instalar a biblioteca podemos utilizar um pip install: $ pip install enelvo Com ela instalada, vamos ver suas funcionalidades: Essa é a principal função dessa lib, então vamos vê-la primeiro. O normaliser “arruma” abreviações (por exemplo: vc, gnt, oq, pq, etc.) e erros ortográficos. Vamos ver como preparar esse primeiro comando: Agora vamos passar uma mensagem para ser normalizada para testarmos nosso normalizador: Para fazer esses processos de capitalização, basta apenas adicionar os argumentos corretos ao Normaliser, vamos ver cada um deles: O argumento sanitizeremove pontuações e emojis, esse é um pré-processamento comum e importante na área de NLP e essa funcionalidade da Enelvo pode ajudar muito na agilidade dessa etapa. Assim como vimos acima, para usar essa ferramenta basta passar o respectivo argumento para o Normaliser: Por fim, para criar um normalizador que realiza todas essas funções, basta passar todos os argumentos juntos: Vamos testar? Veja mais sobre as funcionalidades dessa biblioteca na documentação. O NILC (Núcleo Institucional de Linguística Computacional) é um grupo originário da USP São Carlos e seus projetos e pesquisas englobam diferentes áreas de NLP, como tradução automática, sumarização de texto, criação de léxicos e dicionários, etc. Dentre esses recursos, o NILC dispõe de um repositório de embeddings treinados em português a partir de dezessete corpus, totalizando 1,395,926,282 tokens. Esses embeddings são disponibilizados em várias dimensões diferentes e seus treinamentos são feitos em algoritmos como Word2Vec, FastText, Wang2Vec e Glove. Para acessar o repositório, basta clicar aqui. Assim que você tiver feito o download dos embeddings escolhidos, para utilizá-los com o gensim, basta rodar o seguinte: Se quiser saber mais sobre word embeddings e Word2Vec, você pode conferir o nosso Turing Talks sobre o tema aqui: O Opinando foi um projeto desenvolvido por uma equipe do ICMC-USP para oferecer ferramentas para diversas tarefas de análise em Português, trata-se de um trabalho bem extenso com muitas opções de abordagens, ele é dividido em aplicações e ferramentas. As aplicações do Opinando são projetos realizados pela equipe, algumas delas são: Algumas das ferramentas disponíveis pelo Opinando: O projeto também conta com vários corpus e léxicos do Português para auxiliar nas mais determinadas tarefas, como o TweetSentBR, um corpus classificado manualmente com 15 mil tweets divididos entre positivo, negativo e neutro. Por se tratar de um trabalho extenso, não seria possível listar aqui todos os projetos, ferramentas e recursos, portanto para aprender mais sobre o Opninando, dê uma olhada no site deles, garantimos que vale a pena! Um dos grandes desafios que temos ao trabalhar com linguagem é a quantidade de dados disponíveis, pois, apesar da língua estar sempre presente ao nosso redor, acabamos dependendo de datasets anotados de forma muito específica para realizarmos algumas tarefas de NLP, como para análise de sentimentos, question answering, dentre outros. Para esse problema, uma técnica comum em Deep Learning, já muito utilizada em Visão Computacional, é o Transfer Learning, ou seja, treinamos uma rede em um problema conhecido com uma grande quantidade de dados, e depois realizamos o fine-tuning para a nossa tarefa específica. Lançado em 2018 pela equipe do Google AI, o BERT (Bidirectional Encoder Representations from Transformers) causou uma agitação em toda a comunidade de Deep Learning na época por apresentar resultados estado da arte em várias aplicações de NLP. De uma maneira resumida, O BERT aplica um treinamento bidirecional em uma arquitetura de Transformers para treinar um modelo de língua. Com isso, o modelo consegue aprender relações contextuais entre as palavras de um texto. Vamos utilizar o BERT para fazer exatamente esse processo de Transfer Learning em NLP, mas, como sempre, dependemos da língua em que o modelo foi treinado. Para usarmos o BERT pré-treinado em português, temos o BERTimbau, modelo treinado pela NeuralMind, que fornece dois modelos: o BERT Base e o BERT Large. Podemos acessá-lo através da própria biblioteca do Hugging Face dessa forma: Nesse texto abordamos algumas ferramentas para trabalhar com NLP em português. Para aprender e fixar os conteúdos recomendamos que você treine cada uma das ferramentas com a sua própria abordagem de escolha e que leia os textos e as documentações recomendadas. Conhece alguma outra ferramenta legal para trabalhar com Processamento de Linguagem Natural em português que não comentamos aqui? Agora o Grupo Turing tem uma comunidade no Discord! Compartilhe lá com a gente: Por fim, não deixe de acompanhar o Grupo Turing no Facebook, Linkedin, Instagram e, claro, nossos posts do Medium =) Bons estudos e até a próxima!"
https://medium.com/turing-talks/transformada-de-fourier-b1775e891cc5?source=collection_home---------31----------------------------,Transformada de Fourier em Visão Computacional,,Rodrigo Fill Rangel,809,8,2021-03-21,"A mais importante transformada do século XIX Texto Escrito por: Rodrigo Fill Rangel. Agradecimentos a Rodrigo Estevam pela ajuda de pesquisa. Olá caro leitor de Turing Talks, seja muito bem vindo a mais um texto da nossa querida série de textos de visão computacional. Se conhece um pouco do tema você deve estar pensando: “como transformada de Fourier pode se relacionar com visão computacional?”. Bem é para isso que este texto vai servir, mostrar as mais diversas aplicações desta que é uma das mais importantes transformações matemáticas de todos os tempos. Mas este texto é também uma introdução necessária para o entendimento do tema de um próximo texto, que abordará as transformadas Wavelets. Então sem mais delongas, vamos ao texto. A transformada de Fourier é um importante recurso de análise de sinais que consiste na decomposição de um sinal temporal em seus vários componentes espectrais de frequência, facilitando, e muito, por exemplo a análise de sinais ruidosos, ou muito confusos. O sinal pode ser analógico, como um pulso elétrico, ou digital, como uma sequência de dados acerca de uma observação, como preços de fechamento de um ativo na bolsa de valores. Para tal a transformada tenta decompor um sinal em uma somatória de senoides, estas senoides podem ser de qualquer frequência, isto o algoritmo irá buscar, cada senoide sendo representativa de apenas uma frequência. Estas senoides são chamadas de harmônicas. Observe a fórmula abaixo: Entretanto tentar entender o funcionamento apenas pela matemática do problema pode ser consideravelmente complexo, portanto vamos partir para uma exploração mais simples, utilizando representações visuais. O que tentamos fazer é representar um sinal qualquer como a soma de senos e cossenos dentro de uma transformada de Fourier. Esses senos e cossenos então são representados por sua frequência, em um novo domínio, chamado domínio da frequência, assim passamos de um sinal temporal para a somatória de senos e cossenos que representa aquele sinal no tempo, e representamos esses senos e cossenos por suas frequências, como mostra a imagem abaixo: Na figura temos um sinal em azul escuro que pode ser representados pelas duas senoides no meio da imagem, uma de frequência menor mas de amplitude maior, e outra de frequência maior e amplitude menor, que são representadas em um novo plano agora em função de suas frequências e potências, que é ligada à amplitude do sinal. Assim funciona a transformada de Fourier, levando do domínio temporal para o domínio espectral (frequência). Mas aí você, caro leitor, me pergunta para que isso serve? Qual utilidade de tamanha complexidade de transformação? Bem, temos várias aplicações da transformada de Fourier em diversas áreas do conhecimento, é fato que esta é uma transformação tão importante que o texto poderia ter uma hora de leitura sem que eu conseguisse cobrir todas as aplicações. Por isso vamos nos ater ao mais simples, mas que já é suficiente para que seja compreendido a grande relevância do tema. Para tal então vou fazer uma abordagem prática, utilizando o python. Todos códigos foram adaptados do material dos professores Steven. L. Brunton & J. Nathan Kutz, que é uma excelente referência para o tema. Primeiramente vamos considerar o seguinte código: O output deste código é: Acima nós simplesmente fabricamos um dataset, com um sinal limpo em preto, proveniente da soma de duas senoides, e juntamente a estas senoides somamos um sinal ruidoso, de grande amplitude. Vamos fazer um exercício de faz de conta, fingindo que o sinal preto não existe, não sabemos que ele está presente no sinal vermelho, e apenas temos acesso ao sinal ruidoso, mas sabemos que o sinal vermelho possuí diversos artefatos que estão complicando nossa análise. Assim vamos buscar encontrar as frequências significativas e filtrar as outras. Para tal vamos utilizar o fast fourier transform, que é uma implementação da transformada de Fourier, mas com algumas modificações que não vem ao caso neste texto. Temos: No domínio da frequência fica bastante evidente a influência de potência dominante por duas frequências no sinal, todo o resto podendo ser considerado como ruído. Portanto vamos agora filtrar estes sinais, de forma bastante simples, basta escolher um valor de threshold para a potência, se o valor for inferior ao limite, aquele espectro é zerado, caso contrário não. Pela análise do gráfico basta escolher algo como 200: Por fim podemos agora realizar a transformada inversa rápida de Fourier para conseguirmos o sinal original: Desta forma podemos compreender de forma prática apenas um pouco da importância desta ferramenta para análise de séries temporais, mas a transformada de Fourier também pode ser aplicada a imagens. É possível definir a transformada de Fourier em 2 dimensões , no próprio numpy a impĺementação é dada por np.fft.fft2. Vale dizer que não estaríamos realizando nada muito novo, apenas aplicando a transformação em todas colunas, depois em todas linhas, ou vice e versa (não importa em nada qual direção você escolher transformar primeiro). Vejamos o código a seguir: A imagem da direita acima nada mais é que uma representação dos diversos coeficientes da transformada rápida de Fourier, ou seja, em cada ponto daquela imagem temos a informação da magnitude de uma multiplicação de senos e cossenos, bem como suas fases, e o que estamos tentando fazer nada mais é que para cada pixel na imagem original encontrar uma soma de senos e cossenos de diferentes frequências que resultem no mesmo padrão observado pela imagem original, criado por suas intensidades. Agora eu sei que isso não faz o menor sentido simplesmente falando assim para vocês, até porque como a imagem da direita poderia ser uma representação da imagem na esquerda se elas não tem absolutamente nenhuma ligação visual, mas eu devo lembrar que a imagem à direita trata apenas de coeficientes, não de valores de pixels em sí, e para ficar mais claro eu quero mostrar a seguinte representação da imagem à esquerda: Perceba como é possível representar uma imagem como se ela fosse na verdade uma sequência de valores de uma serie temporal, então os vários coeficientes da transformada rápida de Fourier 2d está tentando somar senos e cossenos em cada ponto da imagem para recriar o padrão descrito pelo plot de superfície acima. Na imagem à direita temos o mesmo plot da esquerda mas em perspectiva superior, mostrando a foto da gatinha. Mas este processo por si só não é extremamente relevante, é importante entender que a partir disto podemos realizar diversas transformações na nossa imagem, sendo as mais comuns a compressão e a filtragem de ruído, mas ambas eu vou descrever somente quando passarmos por wavelets, uma vez que já me estendi demais neste texto :). Entretanto, após falar tanto, a transformada de Fourier não é perfeita, ela apresenta um grande problema que é apenas apresentar informações sobre as frequências do sinal, mas não quando esta frequência aparece no tempo. Uma forma de pensar essas transformações é em termos de resolução espectral, mas isso é tão importante que vou dedicar uma subseção apenas para isso. A resolução de uma transformada mostra o quão bem representado está o sinal em termos de algum eixo que buscamos representar. No eixo do tempo, em que nossa função estava originalmente, temos uma grande resolução temporal, mas a resolução de frequências é muito baixa, a informação de frequências naquela representação não é relevante. Ao fazer a transformação de Fourier passamos a ter uma alta resolução de frequência do sinal, mas a resolução temporal fica excessivamente baixa. Veja a imagem a seguir: Esta imagem denota como o tempo na base temporal está bem caracterizado em várias divisões, qualquer evento temporal pode ser bem posicionado em termos do tempo. Já em relação à frequência a base temporal tem baixa resolução, ou seja, não temos nenhuma informação sobre a frequência nesta base. O oposto ocorre no domínio da frequência, onde a resolução espectral é alta, assim sabemos quais faixas de frequência aparecem no sinal em análise, mas não temos informações sobre o tempo em que tais frequências ocorrem, visto que a resolução temporal é nula. Este configura um problema grave da transformada de Fourier em aplicações que necessitam tanto do conhecimento das frequências como do tempo em que cada frequência ocorre. Para tal surgiram modificações na transformada de Fourier, vou citar uma neste texto, mas a outra será explorada muito a fundo em outro texto que deve sair nas próximas semanas. A STFT nada mais é que um método de reintroduzir o tempo na análise, tentando romper com a barreira citada no final da seção anterior. Mas isso não é nada fácil, considerando que o a única forma de fazer isso implica em um trade-off entre resolução temporal e de frequências. Para ficar mais claro a STFT funciona com janelamento, considerando que o sinal analisado é composto, em cada instante, por algumas frequências dominantes, então em cada instante de tempo de janelamento, aplica-se a transformada de Fourier no sinal da janela, o que estiver fora da janela é automaticamente zerado, assim temos: A imagem acima representa bem o resultado de uma STFT, trata-se de um plot 3D com um eixo representando a mudança de frequências, outro eixo representando as mudanças temporais e o terceiro tem a magnitude ou potência dos sinais. O problema deste método é justamente seu trade-off de resoluções, neste caso quanto maior for o janelamento melhor a resolução de frequências, visto que mais mais frequências estão na janela, porém menor é a resolução temporal, ao passo que a quanto menor o janelamento melhor seria a resolução temporal, porém pior é a resolução espectral. Neste algoritmo é normal usar o janelamento igualmente dividido, como na figura abaixo: Perceba então que nem a resolução temporal nem a espectral são altas, mas ambas existem, nenhuma é nula, permitindo a análise do sinal tanto no tempo quanto na frequência. Uma aplicação interessante deste recurso pode ser visto no próprio youtube, esse método é comumente utilizado para fazer análises de músicas, identificando quais notas são tocadas em quais momentos, veja este vídeo: Assim enceramos nosso texto sobre Transformada de Fourier, espero que tenha gostado e que este texto seja útil para a compreensão da mesma. Obrigado por ter lido até aqui! Contamos com sua presença no próximo Turing Talks! Lembrando que em breve teremos outro texto sobre a transformada Wavelet. Lembre-se de nos acompanhar em nossas redes sociais! No Facebook, no Linkedin, Instagram, nossas postagens anteriores no Medium e em nosso servidor no Discord!"
https://medium.com/turing-talks/transformada-hough-detectando-formas-geom%C3%A9tricas-em-imagens-6d831be61ee8?source=collection_home---------30----------------------------,Transformada Hough — Detectando formas geométricas em imagens,,Rodrigo Estevam,617,5,2021-03-28,"Olá, amável ser! Nesta semana o Turing Talks se afasta um pouco dos tópicos de Inteligência Artificial para explorar uma técnica elementar de detecção em imagens: a Transformada Hough. Aqui você aprenderá como funciona um detector de formas geométricas parametrizáveis, além de como aplicá-lo para retas e círculos com OpenCV e Python. Antes de abrir seu editor de código e pesquisar exemplos de aplicação, como eu sei que você faz quando quer aprender uma função nova, é interessante entender o funcionamento interno da técnica que vamos utilizar. Por isso, nesse primeiro texto explicaremos a teoria por trás dela e em um próximo artigo falaremos sobre a aplicação. Então, pelo menos por agora, feche seu Vim, VSCode ou editor favorito, relaxe, e venha conosco nessa mágica viagem geométrica! Antes de partir para círculos, é interessante começar com uma forma mais simples: a reta. Retas não tem uma definição matemática clara, mas nós sabemos o que são e como representá-las. No ensino médio, ou até antes, aprendemos que podemos definir uma reta pela função y = a x + b. Porém, aqui utilizaremos uma representação um pouco diferente, mas mais adequada para nossa aplicação, a equação polar: Como é possível deduzir dos gráficos, o parâmetro r é a mínima distância da reta ao centro do plano e θ o ângulo de inclinação da normal (pontilhada) à reta. Agora que temos uma boa representação, vamos a uma provocação. Como faríamos para achar todas as retas que passam por um ponto? Na equação podemos substituir x e y pelas coordenadas do ponto e teríamos uma equação com duas incógnitas, r e θ. Mas sabemos que seno e cosseno se repetem a cada 180, assim, se passarmos por todos os thetas até 180, encontraremos os r’s correspondentes e, por consequência, todas as retas que passam por aquele ponto. Por fim, teremos um função que só depende de theta: Com o que temos, já é possível procurar retas em imagens! Sabemos que com a equação da reta nos possibilita encontrar todas as retas que passam por um ponto. Se utilizarmos a equação no ponto demarcado, teremos uma curva no espaço dos parâmetros. Se fizermos isso para mais pontos teremos mais curvas: No segundo gráfico é possível perceber que há vários pontos em que as curvas se encontram, mas dois deles saltam aos olhos. As coordenadas do ponto em que todas as curvas azuis se cruzam são r= 3 e θ = π/4, os quais representam os parâmetros da reta azul no primeiro gráfico. Podemos observar o mesmo no cruzamento das curvas laranjas, onde temos os parâmetros r=1 e θ=π/3. Encontramos as duas retas da imagem! Mas no segundo gráfico há alguns outros cruzamentos, como sabemos quais realmente representam parâmetros de retas na imagem original? Para tal, utilizaremos o conceito de acumulador. O acumulador é uma lista onde guardaremos todos os cruzamentos. Para detectar as retas escolheremos os cruzamentos com maiores recorrências no acumulador. No nosso exemplo, se escolhermos somente cruzamentos entre 3 ou mais curvas, conseguiremos encontrar as duas retas e não teremos nenhuma falsa detecção. Esse limite de recorrência é uma escolha de projeto que pode variar de imagem pra imagem, seu ajuste mal feito pode causar falsas detecções ou falhas em detectar retas existentes. O que descrevemos aqui é o funcionamento básico da Transformada Hough. Para círculos se aplica a mesma técnica utilizada para as retas, mas com parâmetros diferentes. Um círculo pode ser definido se obtivermos seu raio e as coordenadas de seu centro (xc, yc). Então, basta percorrer os pontos da imagem procurando os possíveis círculos com base nesses parâmetros. Se um ponto pertence a uma circunferência ideal, o centro desta circunferência estará na direção normal a esse ponto. Cada ponto na reta normal representa uma possivel cordenada do centro da circunferência. Quanto ao raio, basta medirmos a distância do ponto atual ao ponto na borda do círculo. Como fizemos com as retas, também mapearemos os pontos da imagem à curvas no espaço dos parâmetros. Para sabermos que aquele circulo realmente está na imagem utilizaremos o acumulador novamente. Parâmetros de circulos com muitas aparições no acumulador tem grandes chances de estarem presentes na imagem. Como temos três parâmetros dessa vez (raio e coordenadas do centro) o espaço dos parâmetros é tridimensional. Isso torna o aprofundamento da explicação neste texto inviável. Entretanto, se você quer saber mais a fundo sobre a aplicação da transformada em círculos você pode dar uma olhada no paper da aplicação oficial utilizada no OpenCV (a qual falaremos no próximo texto). Se a explicação ainda ficou muito confusa ou abstrata para você, espere a parte 2 deste texto, nela aplicaremos a Transformada Hough em imagens reais! Finalmente, espero que você tenha gostado do texto de hoje e aprendido algo novo. Para mais textos sobre Visão Computacional, IA e outros temas, continuem acompanhando o Turing Talks. E se quiser saber mais sobre o Grupo Turing siga a gente nas redes sociais: Facebook, Linkedin, Instagram, e servidor no Discord!"
https://medium.com/turing-talks/sua-primeira-an%C3%A1lise-de-sentimentos-com-scikit-learn-a47c088ea7bd?source=collection_home---------29----------------------------,Sua primeira análise de sentimentos com scikit-learn,"Aplicando TF-IDF, Bag of Words e aprendizagem de máquina para criar um classificador de reviews com uma das principais libs do Python",Camilla Fonseca,522,8,2021-04-04,"Olá e bem-vindo a mais um Turing Talks de NLP! Há algum tempo, fizemos um texto de Introdução a Bag of Words e TF-IDF, explicando como funcionam essas técnicas e mostrando como programá-las do 0. Hoje, vou mostrar como aplicar essas técnicas usando a biblioteca scikit-learn, mas não só! Vou mostrar como treinar um modelo de análise de sentimentos de reviews de produtos, desde a limpeza do dataset até a avaliação do modelo. Sugiro fortemente ler o texto acima antes de seguir, mas irei retomar os conceitos ao longo do texto de hoje conforme necessário. Além disso, o notebook com o todo o código pode ser encontrado aqui. E se você ainda não sabe o que é uma análise de sentimentos: é uma tarefa de NLP que visa classificar os sentimentos de um texto dividindo-os, por exemplo, em Positivo, Negativo ou Neutro, ou Feliz, Triste, Nervoso ou Neutro, ou várias outras formas, vai depender do seu problema. Isso tem várias aplicações interessantes, por exemplo, saber se o público está gostando ou não de um produto. Podemos automatizar esse trabalho extraindo comentários sobre o produto na internet e usando um desses modelos. E isso nos leva ao dataset de hoje… O Brazilian E-Commerce Public Dataset by Olist possui diversos dados sobre compras feitas no site Olist. Se você der uma conferida no link, verá que há vários arquivos, porém aqui estaremos usando apenas o ‘olist_order_reviews_dataset.csv’, que contém as reviews feitas pelos usuários dessas compras. Uma observação é que os dados foram anonimizados e por isso no texto aparecem nomes de casas de Game of Thrones no lugar do nome das lojas. Primeiro, vamos importar as bibliotecas necessárias: Agora, vamos ler e dar uma olhada no dataset com a bilbioteca pandas: Como nosso foco é a análise de sentimentos, não precisamos das colunas order_id, review_creation_date e review_answer_timestamp, podemos então removê-las: Agora, vamos verificar os tipos de dados das colunas e quantos dados nulos há em cada uma: Temos pouco menos de 100000 datapoints, todos possuindo um score de review, mas nem todos possuem review escrita. Vamos então verificar se há dados duplicados baseado no id e removê-los: Como estamos interessados justamente no texto, vamos juntar as colunas review_comment_title e review_comment_message em uma só (coluna nova review) e tirar entradas que não possuem texto. Após isso, ficamos com 43152 datapoints. Na coluna review_scores, há 5 valores de score diferentes (indo de 1-pior até 5-melhor), porém, para facilitar nossa tarefa, vamos classificar as reviews apenas como positiva ou negativa. Se o score for menor ou igual a 3, consideraremos negativa (0) e caso contrário, positiva (1). Essas serão as labels que usaremos para treinar o modelo e vamos armazená-las em uma nova coluna: É sempre bom conferir o quão balanceada é a distribuição dos dados nas labels: Vemos que há bem mais reviews positivas do que negativas, mas não é uma disparidade tão grande a ponto de precisarmos tratá-la em um primeiro momento. Os pré-processamentos são etapas de preparação do texto com o objetivo de reduzir o tamanho do vocabulário (conjunto de palavras que ocorrem em todos os textos do dataset) e simplificar algumas formas lexicais para que o nosso algoritmo consiga extrair informações relevantes e tenha um desempenho melhor. No entanto, não vou explicar a fundo cada etapa aqui. Para isso, você pode conferir o seguinte Turing Talk: Os pré-processamentos que irei fazer são: Para remoção de stopwords, vamos usar a lista disponível na biblioteca nltk. Nessa lista constam as palvras ‘não’ e ‘nem’, que se removidas podem mudar a interpretação do sentimento de uma frase e por isso resolvi remover da lista de stopwords: Além disso, usaremos o lematizador da biblioteca spacy. Para isso, precisamos fazer o download de um modelo de português da lib. Se você estiver usando o Jupyter ou o Google Colab, rode a seguinte célula: Se estiver usando um Kaggle Notebook, precisará habilitar internet no kernel (vá em Settings -> Internet e selecione Internet connected) e então rode a seguinte célula: Após o download, podemos importar o modelo: Agora, estamos prontos para criar uma função que realiza os pré-processamentos e aplicá-la no dataset! Como tinham reviews com apenas números ou emojis, ainda há dados faltantes na coluna review, vamos removê-los: Obs: Também poderíamos ter usado a spacy para tirar as stopwords, o que faria mais sentido já que iríamos usar o lematizador dela depois de qualquer forma. No entanto, nesse caso, usar a lista da nltk deu melhores resultados. Um possível motivo é que a lista de stopwords da spacy é maior e pode ter tirado palavras importantes para a análise de sentimentos. Essa etapa consiste em transformar o texto em uma informação numérica, mais precisamente um vetor, para que seja possível utilizá-lo para alimentar um modelo. Aqui, vou mostrar como usar a scikit-learn (a.k.a. sklearn) para realizar as duas técnicas de feature extraction citadas no início: Bag of Words e TF-IDF. Essa primeira representa o texto como um vetor em que cada elemento corresponde a uma palavra do vocabulário e seu valor é quantidade de vezes em que tal palavra aparece nesse texto, ou então simplesmente 0 se a palavra não aparece nele e 1 se não. A função CountVectorizer da sklearn faz isso pra gente! O parâmetro binary indica se será usada a contagem total das palavras ou apenas 0 e 1 (eu usei essa última opção). Já o parâmetro max_features define o tamanho do vocabulário, assim max_features = 5000 constrói o vocabulário com as 5000 palavras que mais ocorrem no total dos textos. Já essa técnica vetoriza o texto atribuindo para cada palavra uma pontuação que mede a importância dela nesse texto. Também há uma função na sklearn que já faz isso para a gente: TfidfVectorizer! Lembrando que se quiser uma explicação mais a fundo das duas técnicas e suas vantagens e desvantagens, basta ler esse texto. Aqui, vou comparar o desempenho do modelo com os dados vetorizados tanto com BoW como com Tf-idf. Mas primeiro, é preciso dividir os dados em base de treino (70%) e teste (30%): Vamos também importar as métricas que serão usadas para avaliação de cada modelo e definir uma função para mostrá-las: Se quiser entender melhor alguma das métricas usadas acima, pode conferir no Turing Talk abaixo: O modelo que mostrarei aqui é uma regressão logística, que apresentou as melhores métricas, porém também foram testados outros, se quiser ver é só checar no notebook do kaggle! Vamos agora ver as métricas: Analisando as métricas, vemos que a diferença entre usar Bag of Words ou TF-IDF foi bem pequena, apesar de o modelo treinado com os dados com TF-IDF ter ido melhor, com acurácia e F1 de 90% e AUROC de 89% (arredondando). Vamos agora ̶b̶r̶i̶n̶c̶a̶r̶ testar com novas predições! O principal objetivo desse texto era mostrar as etapas básicas para o treinamento de um modelo de análise de sentimentos, o que espero que tenha sido atingido com sucesso! É claro que ainda há espaço para testar outras coisas e quem sabe melhorar o modelo! Poderíamos fazer pré-processamentos diferentes, por exemplo: já que havia emojis em algumas reviews e esse é um dos principais meios das pessoas expressarem seus sentimentos, talvez fosse interessantes incluí-los na análise. Poderíamos também explorar outras ferramentas de NLP, como as apresentadas no texto abaixo: Esses testes é algo que vou deixar como lição de casa para você e se achar algo legal, pode contar aí nos comentários ;) Por fim, para mais textos como esse, continue acompanhando o Turing Talks! E se quiser saber mais sobre o Grupo Turing siga a gente nas redes sociais: Facebook, Linkedin, Instagram e servidor no Discord!"
https://medium.com/turing-talks/teoria-moderna-do-portf%C3%B3lio-em-python-e8bad41ebb8b?source=collection_home---------28----------------------------,Teoria Moderna do Portfólio em Python,,Julia Pocciotti,911,6,2021-04-11,"Otimize a sua carteira de investimentos com a teoria de Harry Markowitz Texto escrito por Julia Pocciotti Quando falamos sobre ""Otimização de Portfólios"", estamos comentando sobre o processo utilizado para selecionarmos a melhor distribuição de ativos em uma carteira a fim de atingirmos um objetivo comum. Esse objetivo pode variar de acordo com o perfil de cada tipo de investidor, pode ser que para um seja preferível maximizar o retorno esperado de seu portfólio, enquanto, para outro, o objetivo seria investir em uma carteira com o menor risco possível, por exemplo. Hoje, vamos falar sobre todos esses conceitos e como otimizar um portfólio utilizando a chamada Teoria Moderna do Portfólio, do laureado com o prêmio Nobel da Economia, Harry Markowitz. A essência da Teoria Moderna do Portfólio está ligada ao trade off existente entre o Retorno e Risco. Segundo Markowitz, os investidores seriam avessos ao risco, e, por isso, só estariam dispostos a aumentar o risco de sua carteira a fim de alcançarem um retorno esperado mais alto. Faz sentido, certo? Então a pergunta que iremos trabalhar aqui vai ser a seguinte: ""Considerando um certo nível de risco, qual carteira me dará o maior retorno?"". Para isso, vamos antes entender melhor como calcular o Retorno e o Risco de um portfólio. Para calcularmos os retornos simples dos nossos ativos em python, podemos simplesmente utilizar a função pct_change() da biblioteca pandas. Aqui, para exemplificar o nosso portfólio, estaremos usando uma carteira composta por ações da Amazon, Google, Microsoft e Netflix: Porém, como não estamos lidando as ações unicamente, precisamos calcular o retorno do portfólio como um todo. Para isso, vamos distribuir a porcentagem que cada ativo receberá na nossa carteira criando um vetor de pesos (w) e multiplicando-o pelos retornos simples dos ativos: Existem várias formas diferentes para se calcular o risco de um ativo, mas no geral, essas métricas estarão relacionadas a quão volátil é uma ação. A ideia principal é que ativos muitos voláteis, ou seja, que variam muito, não são muito seguros. Sendo assim, a forma mais comum de se obter esse tipo de métrica para uma ação é através do cálculo de seu desvio padrão: Intuitivamente, poderíamos pensar que para calcular o risco de uma carteira, bastaria multiplicar o risco de cada uma de suas ações pelo vetor de pesos de seu portfólio, mas é aí que entra a grande sacada de Markowitz. Em um portfólio, o seu risco não será igual à média do risco de cada um de seus ativos, pois nesse caso precisamos levar em conta como que todas as ações estão se movimentando em conjunto. Para isso, precisamos antes calcular a matriz de covariância das médias dos retornos: Tendo calculado a matriz de covariância ( C ), e com os nossos vetores de pesos (w), em seguida, vamos utilizar esta fórmula para calcular o risco do portfólio: Sua implementação em Python fica assim: Todo investimento possui uma certa taxa de risco associada a ele, mas é certo que alguns investimentos são mais seguros do que outros, certo? O Sharpe Ratio é uma métrica utilizada para nos ajudar a analisar se estamos sendo devidamente recompensados por uma taxa adicional de risco assumido. Para isso, ele compara o retorno de um investimento que possui um risco associado com um outro de ""risco livre"". No caso de uma carteira de ações na bolsa brasileira, ele poderia comparar o retorno dela com um o retorno de um investimento em renda fixa, por exemplo. Para calcular o sharpe ratio subtraimos o taxa de risco livre do retorno do portfólio e dividimos esse valor pelo risco do portfólio: Quanto maior for o valor do Sharpe Ratio, mais atraente será esse investimento dado o risco associado. Porém, um valor negativo significa que a taxa livre de risco é maior que o retorno do portfólio. De maneira geral, um sharpe ratio maior que 1 é considerado bom. Agora que já entendemos como calcular o retorno e o risco de um portfólio, vamos simular várias carteiras com pesos diferentes e observar qual delas irá nos retornar o portfólio com o maior Sharpe Ratio. Com o código acima estamos gerando várias carteiras com vetores de pesos diferentes. Nessa função, ela retorna um dicionário com todos os pesos, retornos, riscos e sharpe ratios de todos os portfólios. Para descobrirmos qual carteira obteve o melhor Sharpe Ratio, utilizamos a seguinte função: Fazendo uma simulação com as funções que descrevemos acima, a carteira que nos retornaria o maior Sharpe Ratio seria a com a seguinte distribuição de pesos: Lembre-se que a simulação dos pesos que estamos utilizando é aleatória! Então se você tentar reproduzir esse código, os resultados dos pesos serão diferentes! Se pegarmos todos os portfólios que foram gerados e plotarmos em um gráfico, tendo como referência os retornos no eixo y, e os riscos no eixo x, temos um esquema que é conhecido como a ""Fronteira Eficiente"". Antes de entrarmos a fundo na sua interpretação, vamos implementá-lo em Python utilizando o Matplotlib: Utilizando a função com a carteira que estamos usando de exemplo até agora, temos o seguinte gráfico: Mas afinal, o que é a fronteira eficiente? A fronteira eficiente nada mais é do que a linha que representa os retornos máximos que podemos obter com um portfólio dado um certo nível de risco. Portfólios ótimos sempre devem estar na linha da fronteira, pois, para qualquer portfólio abaixo da linha, há uma carteira disponível com o mesmo risco associado, porém com um retorno maior. Qualquer portfólio acima da fronteira é impossível. Para calcular o nosso melhor portfólio, utilizamos como métrica o maior Sharpe Ratio, porém, analisando o que acabamos de ver sobre a fronteira eficiente, você pode estar se perguntando o que constituiria um bom portfólio no final das contas, e a resposta é: depende! Um bom portfólio vai depender do tipo de investidor e o que ele espera ganhar com os seus investimentos. Para uma pessoa que está próxima da aposentadoria, talvez seja preferível um portfólio que oferece um risco menor, já para uma pessoa mais jovem, talvez aumentar seus retornos com um portfólio um pouco mais arriscado seja uma opção mais atraente. Independente do nível de risco que o investidor estará disponível a aceitar, o importante é estar na linha da fronteira. Tendo isso em mente, podemos modificar o nosso código para adicionar métodos para retornar a carteira com o menor risco e o maior retorno: Hoje você deu seus primeiros passos com otimização de investimentos conhecendo um pouco mais sobre a teoria de Harry Markowitz. A Teoria Moderna do Portfólio abriu espaço depois para que outras teses de otimização de portfólios também fossem desenvolvidas. Também temos um Turing Talks que comenta uma outra estratégia para tal, chamada Paridade de Risco Hierárquica, você pode conferi-lo aqui: Caso tenha perdido alguma parte do código, a implementação está toda disponível na biblioteca turingquant: Por fim, não deixe de acompanhar o Grupo Turing no Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Bons estudos e até a próxima!"
https://medium.com/turing-talks/transformadas-wavelet-em-vis%C3%A3o-computacional-94c72d57e049?source=collection_home---------27----------------------------,Transformadas Wavelet em Visão Computacional,A transformada que revolucionou a Pixar e o mundo,Rodrigo Fill Rangel,448,10,2021-04-18,"Texto Escrito por: Rodrigo Fill Rangel e Rodrigo Estevam Olá, caro leitor de Turing Talks, seja muito bem vindo a mais um texto da nossa querida série de textos de visão computacional. Se você leu nosso texto de algumas semanas atrás sobre Transformada de Fourier deve estar esperando ansiosamente por este aqui, que é uma continuação espiritual daquele, da mesma forma que a transformada wavelet é uma sucessora da transformada de Fourier de alguma maneira. Pra entender tudo que eu vou falar aqui é importante que você tenha acompanhado o texto passado, que está bem aqui: Transformada de Fourier em Visão Computacional. Vai lá, da uma lida, deixa os claps e depois volta aqui pra aproveitar tudinho. Eu terminei o texto passado falando sobre as resoluções, tanto da transformada de Fourier quanto da transformada de Fourier de tempo curto. Agora vamos direto falar sobre a resolução espectral da transformada wavelet, já pra mostrar para vocês como ela apresenta uma melhoria em relação às propostas anteriores. O grid de resolução da transformada wavelet é genericamente assim: Numa primeira olhada ele parece devidamente estranho, né? Mas existem ótimas justificativas para esta divisão. Vejamos, primeiramente, que em baixas frequências a resolução temporal é baixa, correto? Sim, ela é baixa pois uma onda de baixa frequência muda lentamente no tempo, ou seja, não precisamos ter uma grande resolução para medir coisas que mudam de forma lenta, mas conforme a frequência aumenta é necessário aumentar a resolução temporal, visto que agora as coisas mudam mais rapidamente no tempo, o período de oscilação é menor, assim para obter mais informação, temos de aumentar a resolução temporal e reduzir a resolução espectral do sinal, já que geralmente as frequências altas são menos significativas no sistema. E assim temos a estrutura em grid mostrada. Vale dizer que esta estrutura pode ser definida por você para satisfazer as suas necessidades. Para conseguir este padrão a transformada wavelet vai funcionar de forma similar à transformada de Fourier, ela toma a função de análise e projeta a mesma sobre uma base ortogonal, a grande diferença neste caso é que a base ortogonal utilizada neste caso não será meramente uma junção de senos e cossenos, mas uma hierarquia de funções ortogonais, que vão modificadas, ajustadas, para justamente se encaixarem nestes menores retângulos de resolução na imagem acima. Essas funções descritas são chamadas de wavelets, elas devem ser funções ortogonais, ou seja, integral nula no tempo, e podem ter diferentes formatos, por isso que existem diversas transformadas wavelets, porque a função wavelet utilizada pode variar, alguns tipos são: Todas estas funções serão então descritas como sendo uma Ψ(𝑡). Esta função será a “wavelet mãe” que dará origem a outras variações de wavelets usadas para serem adaptadas a cada uma das resoluções. A função genérica de uma “wavelet filha” em função da wavelet mãe é: É possível notar que os fatores a e b definem deslocamento e o escalonamento da função, então o valor de b desloca a função no tempo, escolhendo para qual retângulo da esquerda para a direita aquela variação de wavelet corresponde, enquanto que o parâmetro a aumenta ou encolhe a função no tempo, e isso essencialmente corresponde à aumentar ou diminuir a frequência da onda, escolhendo verticalmente qual seria o retângulo correspondente. Para entender melhor veja a imagem abaixo: A curva em azul em todos os casos mostra a wavelet mãe, enquanto que para cada quadro temos uma variação possível em termos dos parâmetros a e b. Veja agora como cada wavelet “filha” preenche o retângulo de resolução: Em se tratando de Wavelets já cobrimos quase tudo que tinha para cobrir, resta apenas agora definir a fórmula da transformada em si, mas não é nada muito obscuro, segue a mesma estrutura da definição em Fourier: Que consiste simplesmente no produto interno entre a wavelet em questão e a função a ser transformada, lembrando que no espaço de funções contínuas o produto interno corresponde àquela integral mencionada no texto anterior, mas não precisamos nos focar nos aspectos matemáticos neste momento. Agora que já cobrimos quase tudo do básico da teoria de wavelets, vamos tentar aplicar esta transformação em uma imagem, claro, bem como a FFT, a transformada wavelet pode ser definida espacialmente em uma versão 2d. Vejamos um exemplo de implementação. Assim como em Fourier, a transformada espacial da wavelet é bastante diferente da temporal e tentar entender o processo espacial pode ser mais eficiente se feito de forma mais visual. Vamos usar uma biblioteca em python que já cuida de toda aplicação espacial, e temporal se essa for a aplicação. Essa biblioteca é chamada PyWavelets e pode ser encontrada aqui. Os códigos aqui apresentados são baseados no material dos professores Steven. L. Brunton & J. Nathan Kutz, que é uma excelente referência para o tema. A princípio vamos apenas escolher uma imagem e aplicar a transformada para ver o resultado direto: O primeiro passo está abaixo, em que definimos o nível de decomposição, no caso n = 1 mesmo, e o tipo de wavelet, no caso a daubacchies 1 como mostrada na imagem de wavelets acima. Com um pré-processamento podemos chegar no resultado abaixo: O resultado é curioso, temos uma imagem com a metade do tamanho da original, além de outras 3 imagens que são uma espécie de detalhes da imagem original. Para explicar exatamente o que está acontecendo na imagem acima eu vou necessitar da ajuda de uma das maiores pesquisadoras da área hoje, Ingrid Daubechies, com base nos slides desta aula. O que está acontecendo neste slide acima é o seguinte: Temos uma imagem na parte superior e ampliamos a imagem até vermos os pixels dela, bem como seus valores. Estes pixels estão na primeira linha de números. Repare como os números são valores muito próximos, eles tem uma diferença brusca em um ponto mas em sua grande parte os valores são próximos, então vamos fazer o seguinte, vamos tentar unir eles., tirando a média entre dois pixels adjacentes. O resultado é a segunda linha. Ao fazer isso perdemos informação sobre os pixels anteriores, correto? Eram dois agora temos um só que tenta condensar a informação contida nos dois anteriores. Não queremos perder totalmente esta informação então vamos fazer o seguinte, vamos guardar a diferença entre o segundo pixel e o primeiro. Assim podemos reconstruir a primeira linha a partir da segunda. A terceira linha, primeira em vermelho, contem a informação da diferença. Podemos repetir o processo, o número de vezes que fazemos isso corresponde ao nível de decomposição da wavelet, o parâmetro n naquele código acima. Repare em uma coisa, quase todas diferenças registram valores pequenos, indicando que os pixels eram praticamente idênticos e que perdemos pouca informação ao fazer a média entre os dois. Há, entretanto, os casos destacados por um circulo verde que ressalta que naquele ponto houve uma grande perda de informação ao realizar a média. Entender isso é vital então releia e observe a imagem se tiver alguma dúvida. Isso cobre a wavelet espacial em uma dimensão, mas e em duas? Bem temos que fazer o mesmo processo mas nas duas dimensões, assim, temos: A imagem acima tem uma parte de uma imagem, representada pela matriz 4x4, vamos aplicar o procedimento que vimos primeiro horizontalmente, chegando nas imagens matrizes 4x2. A imagem em preto representa as médias e a em vermelho as diferenças. Agora vamos realizar o procedimento verticalmente sobre essas imagens resultastes da operação horizontal, média e diferenças da imagem preta e média e diferenças da imagem vermelha, gerando 4 imagens ao total, todas 2x2. A primeira sofreu um processo de médias horizontais e verticais, representando uma suavização, redução de detalhes em relação à imagem original. Esta imagem é a apresentada como a imagem superior esquerda na figura resultante do código, mostrando a gatinha acima. A segunda matriz 2x2 passou por um processo de médias verticais e diferenças horizontais, portanto ela contem informação dos detalhes horizontais da imagem, que foram perdidos na suavização. Já a terceira matriz 2x2 passou por um processo de diferenças verticais e médias horizontais, armazenando os detalhes verticais da imagem original. Por fim a ultima matriz passou por diferenças horizontais e verticais, armazenando informações de detalhes inclinados. Com toda essa informação agora, volte à imagem da gatinha acima e analise como as outras três figuras estranhas armazenam majoritariamente detalhes em algum sentido como falado agora. Por que quase toda imagem é cinza? Isso ocorre porque essas imagens passaram por algum processo de diferenças então seus pixels tem valores não de 0 a 255 mas de -255 a 255, sendo o zero o cinza. Veja então que grande parte das imagens de detalhes são praticamente zero, quase tudo na imagem é cinza. Esse é o pulo do gato para entender melhor grande parte das aplicações de wavelets em imagens, vamos falar mais sobre elas. Uma das principais aplicações, tanto da tansformada de Fourier quanto das Wavelets, é a compressão de sinais e imagens, descantando-se a segunda. A relevância da compressão de imagens se dá em diversos contextos, quase todas imagens disponíveis para nosso acesso são comprimidas por fatores grandes, para serem acessíveis pela internet, não ocuparem espaços proibitivos nas unidades de armazenamento, entre outros fatos. Por exemplo na compressão antiga JPEG, que utilizava a FFT para compressão, o arquivo armazena uma série de módulos e fases de senos e cossenos que serão inversamente transformados quando houver a necessidade de exibir a imagem da forma convensional que estamos acostumados. Neste processo o Fast da FFT se torna muito importante, para não tornar proibitivo a conversão e exibição das imagens. Como visto na explicação acima, grande parte das imagens é referente apenas à pequenas variações entre os seus píxels, a parte realmente interessante das imagens, notoriamente as bordas, ou features, não aparecem com tanta frequência na imagem, consequentemente podemos filtrar essa grande parte de informação “pouco relevante”, mantendo ainda a grande parte das features na imagem e ao mesmo tempo reduzindo expressivamente seus tamanhos, veja a aplicação em código abaixo: No código acima transformamos a imagem, aplicamos uma máscara para considerar apenas as diferenças mais importantes, com valores maiores, e reconstruirmos a imagem, o resultado é o seguinte: Grande parte da imagem é composta por informações pouco relevantes, então a perda de qualidade só se torna visível para altos níveis de compressão. Algo importante de ser notado neste tipo de compressão é que elementos de alta frequência, texturas, grama, pelo, qualquer feature que oscile muito, são mais difíceis de serem comprimidas consequentemente apresentam maior dificuldade de aplicação dos métodos de compressão. Outro fator de alta importância para a eficiência da compressão de arquivos é a resolução inicial da imagem, quanto menor a imagem mais complicado é de ser comprimida. Para entender a questão das features de alta frequência, veja a imagem abaixo, note que ambas imagens utilizadas tem o mesmo tamanho, justamente para que isto não seja um fator de influência no resultado: Essa grande dificuldade de compressão de features de alta frequência, como pelos de animais ou grama e vegetação, é um dos motivos da Pixar, em 1997, quando lançaram sua primeira animação, Toy Story, terem feito a pele dos brinquedos todos de plástico, para evitar justamente altas frequências, que tornariam proibitivas as compressões para o cinema. Entretanto em 2004 eles lançaram o Monstros S.A que trazia em tela o Sullivan, ostentando seus pelos, e desde então esse tipo de modelagem não seria mais um problema para a Pixar e outras animadoras. Entre Toy Story e Monstros S.A o que ocorreu? O desenvolvimento das tecnicas de compressão utilizando wavelets. Claro que eu não sei exatamente o que a Pixar usa, é uma empresa fechada, mas é no mínimo curioso. Neste mesmo período a compressão dos formatos JPEG passou a ser atualizada para compressão via wavelets e não Fourier. Para ter uma ideia de como era proibitivo, aqui está a compressão do Sullivan com Fourier: Fourier tem uma perda grande de informação por não conseguir lidar bem com as features de alta frequência. Outra aplicação bastante comum tanto da transformada wavelet como de Fourier é a filtragem de ruídos, tanto em sinais 1D quanto em imagens. Veja o exemplo abaixo: Assim enceramos nosso texto sobre Transformadas Wavelet, espero que tenha gostado e que este texto seja útil para a compreensão da mesma. Obrigado por ter lido até aqui! Contamos com sua presença no próximo Turing Talks! Lembre-se de nos acompanhar em nossas redes sociais! No Facebook, no Linkedin, Instagram, nossas postagens anteriores no Medium e em nosso servidor no Discord!"
https://medium.com/turing-talks/aprendizado-por-refor%C3%A7o-aplica%C3%A7%C3%B5es-na-%C3%A1rea-de-ci%C3%AAncias-biol%C3%B3gicas-e1131373fc9a?source=collection_home---------26----------------------------,Aprendizado Por Reforço: aplicações na área de ciências biológicas,"Veja as aplicações de Aprendizado por Reforço na Bioinformática, Robótica e Medicina",Nelson Alves Yamashita,448,6,2021-04-25,"Olá entusiastas de inteligência artificial, bem vindos a mais um Turing Talks de Aprendizado por Reforço! Se você não se lembra ou ainda não conhece, o aprendizado por reforço é uma técnica de aprendizado de máquina que envolve principalmente a interação de um agente com um ambiente, no qual esse agente tenta aprender as melhores ações para uma situação com base em um sistema de recompensas. Se você se interessar, possuímos uma série de textos com os básicos dessa técnica aqui no nosso Medium, checa lá :). Recentemente, mostramos como aplicar diversos algoritmos de Aprendizado por Reforço em diversos jogos, como por exemplo uma Rede Neural que Aprende a Jogar Flappy Bird, Uma IA que Aprende a Jogar Pong e até uma que Aprende a Jogar Super Mario. Como podemos ver, o Aprendizado por Reforço possui um potencial muito grande, então por que só o usamos em jogos? Existem uma série de motivos para isso: Embora jogos sejam importantes ambos para a sociedade e para a computação, um algoritmo que apenas serve para resolvê-los não parece ser a coisa mais útil do mundo… Por isso preparamos este texto com algumas das aplicações mais recentes de RL em diversas áreas do conhecimento, como Bioinformática, Robótica e Medicina. O problema da montagem de sequência de DNAs é um problema conhecido tanto na ciência da computação quanto na bioinformática por sua grande complexidade. Basicamente o que se tenta realizar nesse problema é a reconstrução de uma molécula de DNA original com base em uma grande quantidade de fragmentos de sequências menores vindas dessa molécula. Recentemente o Aprendizado por Reforço vem sendo aplicado nesse problema. Seu objetivo é treinar um agente que consiga achar o caminho durante a montagem dos fragmentos do seu estado inicial até um estado de alinhamento final, maximizando uma função de performance, a qual soma todas as pontuações de sobreposição dos fragmentos adjacentes. O modelo de aprendizado por reforço conseguiu resultados melhores do que outras técnicas, como aprendizado supervisionado e algoritmos genéticos [1]. O desenvolvimento de moléculas do zero, normalmente chamada de montagem de novo (do latim, do novo), recentemente se tornou uma área de pesquisa ativa graças aos, dentre outras coisas, recentes avanços em aprendizado por reforço. Nela tentamos desenvolver moléculas específicas que consigam ativar proteínas particulares, sendo extremamente útil no estudo de bioativos, que por sua vez são de grande importância na descoberta de novas drogas. Um dos modelos treinados para isso é o REINVENT, nele o agente já treinado com uma estrutura de molécula específicas, gera um caractere de uma molécula em formato de texto por episódio, recebendo recompensas com base na molécula formada. Ele utiliza uma modificação de um famoso algoritmo de RL, no qual foca na alteração da política, o REINFORCE . Em um teste para criar compostos ativos com receptores de dopamina, o modelo conseguiu criar moléculas quimicamente válidas, ambas novas e até já conhecidas por serem ativas com o receptor [2]. Uma área onde é bem comum utilizar IA é na área de robótica, com o objetivo de melhorar performance ou otimizar tarefas desejadas. Um exemplo interessante dessa área que utiliza RL é a Sutura Colaborativa, ou seja, um agente auxiliando o profissional na hora de realizar sutura (pontos cirúrgicos). Essa tecnologia seria aplicada em cirurgias assistidas por robôs. Abaixo, uma imagem representando as 4 etapas do processo de sutura: Uma das possíveis dificuldades que o modelo poderia encontrar seria a diferença entre os estilos de sutura dos profissionais. O algoritmo utilizado para essa tarefa foi o Q-Learning, um algoritmo bem simples e poderoso, e o resultado foi que o agente conseguiu aprender a realizar essa tarefa de uma forma semelhante a de um profissional, sendo robusto a pequenas variações de movimento. Isso mostra que esse tipo de tecnologia pode auxiliar e muito os operadores desses equipamentos, mantendo a precisão [3]. Uma aplicação de RL que vem ganhando espaço na medicina é na melhoria de tratamentos. Um exemplo na literatura é com Câncer: há aplicações que buscam encontrar qual a melhor dose para o tratamento com quimioterapia. Essa tarefa é difícil, visto que é necessária uma dose suficiente para atacar as células cancerígenas enquanto não se deve causar problemas ao paciente. Em [4], os autores utilizam o algoritmo Q-Learning como base para tentar resolver esse problema, alcançando resultados satisfatórios. Um outro exemplo é Ventilação Mecânica, visto que o uso extensivo é custoso para o hospital e pode causar complicações para o paciente na UTI. A utilização de RL nessa tarefa conseguiu trazer resultados interessantes utilizando dados históricos de UTI, sendo necessário mais estudos antes da aplicação clínica [5]. Um grande desafio na área da saúde é a inferência diagnóstica clínica, isto é, descobrir qual o diagnóstico do paciente para saber qual tratamento é o mais indicado. Para tentar resolver esse problema, foi aplicado um algoritmo de RL, usando alguns conjuntos de dados disponíveis, divididos em dois grupos: O modelo, a partir da descrição do paciente e sintomas, utiliza os dados Externos para fazer uma predição de qual é o diagnóstico clínico. O resultado obtido demonstra que essa modelagem pode auxiliar no diagnóstico dos pacientes, embora seja necessário ainda outros estudos [6]. RL é uma das grandes áreas do Aprendizado de Máquina, com suas características únicas. No texto de hoje, tivemos a oportunidade de mostrar que RL não é aplicado somente em jogos; suas aplicações vão em diversas áreas e ajudam a resolver problemas extremamente complexos, como tratamento de câncer e auxílio na descoberta de novas drogas. Lembre-se de nos acompanhar em nossas redes sociais! No Facebook, no Linkedin, Instagram, nossas postagens anteriores no Medium e agora nosso servidor no Discord! Se quiser se aprofundar em algum dos temas descritos, considere ler seus artigos originais:"
https://medium.com/turing-talks/houghcircles-detec%C3%A7%C3%A3o-de-c%C3%ADrculos-em-imagens-com-opencv-e-python-2d229ad9d43b?source=collection_home---------25----------------------------,HoughCircles — Detecção de círculos em imagens com OpenCV e Python,,Rodrigo Estevam,521,6,2021-05-09,"Bem vindo, caro leitor! Este é o segundo, e ultimo, texto de uma minissérie sobre Transformada Hough aqui no Turing Talks. Se você não leu o primeiro, você deveria, já que lá explicamos toda a teoria por trás da transformada e aqui focaremos em sua aplicação. Conceitos já explicados serão retomados e usados sem serem explorados novamente, então dê uma pausa aqui e antes leia o artigo: Transformada Hough — Detectando formas geométricas em imagens. O objetivo deste texto é explorar os usos da função cv2.HoughCircles(). Entenderemos como utilizá-la de forma eficiente em diversos tipos de imagens. Todo os exemplos serão em Python, mas como o OpenCV também está disponível em outras linguagens, a “tradução” deve ser simples. Antes de qualquer outra coisa é legal entender qual o funcionamento interno do detector. Inicialmente vamos listar os argumentos da função e entender o uso de cada um deles. Você sempre pode olhar na documentação oficial, mas tramemos aqui uma descrição traduzida e simplificada: image: a imagem em grayscale na qual se deseja detectar os círculos. method: técnica a ser aplicada para detecção. Por enquanto o OpenCV somente tem o método cv2.HOUGH_GRADIANT. Para mais informações sobre esse método e um benchmark com alguns outros baseados em Transformada Hough, leia este paper. dp: razão inversa da resolução do acumulador em relação a imagem. Esse é o parametro mais confuso de entender, mas normalmente você deixará ele entre 1 e 2 (pelo menos foi o que experenciei pessoalmente). O que é importante você gravar é que quanto maior esse valor, mais sensivel o detector é, podendo causar falsos positivos. Já se ele for muito baixo, alguns circulos podem não ser detectados. minDist: a distância mínima entre dois círculos. Infelizmente essa função não consegue detectar círculos concêntricos. Se este argumento não for corretamente ajustado é possível que falsos círculos sejam detectados, por isso é importante entender qual a distância ideal no caso da sua imagem. Um exemplo básico de que esse parâmetro está muito pequeno é quando uma circunferência está sendo detectada mais de uma vez. param1: segundo parâmetro do Canny interno que a função aplica antes do método em si, o primeiro parâmetro é param1/2. Se você não sabe o que é Canny, você deveria ler nosso excelente artigo sobre. Basicamente este é um filtro de bordas. Como vimos no texto de Transformada Hough, a técnica se baseia em percorrer a imagem em busca de pontos que possam pertencer a borda de uma circunferência, então esse filtro é indispensável para preparar a imagem para essa tarefa. param2: limite do acumulador. Como dito no texto passado, os círculos são guardados no acumulador e aqueles com “maior confiabilidade” tem maior “score” na lista. Este argumento age como pontuação mínima que um círculo tem que ter para ser detectado. Se param2 for muito baixo podem haver falsas detecções. Já se ele for muito alto, podem ocorrer falhas em círculos “verdadeiros”. minRadius e maxRadius: limites para o tamanho dos círculos a serem detectados. Esses parâmetros não são obrigatórios, mas se você tiver uma estimativa dos tamanhos do círculos que quer encontrar eles podem prevenir falsas detecções fora desta faixa. Ao final, a função retorna circles: uma lista com (x,y,raio) dos círculos encontrados. Para melhor performance do nosso detector é aconselhável aplicar algum processamento na imagem para facilitar a tarefa dele. Quanto mais as circunferências estiverem em destaque melhor, pois sua detecção será mais fácil. Além disso, quanto menos detalhes não relevantes na imagem menos processamento é necessário (o que aumenta a velocidade de execução) e maior acurácia é atingida. Imagine agora que você precisa contar o número de automóveis que passam por algum local, por exemplo um pedágio, como faria? Uma opção é treinar um modelo de Rede Neural para detectar veículos (como uma YOLO) e contá-los com ela. Entretanto, treinar um modelo não é uma tarefa fácil e com certeza não é algo rápido. Adicionalmente, você teria que ter um computador (ou controlador) que aguentasse rodar a rede a um framerate (quadros por segundo) decente. Outra opção é contar as rodas dos veículos, deste modo saberemos quantos veículos passaram por lá. Essa técnica não precisa de treinamento, como uma Rede Neural, e é muito mais leve, o que a torna mais viável para vídeos. Então, será a segunda abordagem que teremos aqui! Esse exemplo está disponível no meu Github, caso você queira ter acesso ao código (e me doar uma estrelinha). Mas faremos o passo a passo aqui também. De inicio, precisamos saber o tipo de imagem que iremos analisar. Por isso escolhi a foto de uma moto para ajustar o nosso detector: Como rodas normalmente são pretas, não há muito tratamento de cores a ser feito. Partiremos então direto para o greyscale: A imagem ainda possui muitos detalhes que são irrelevantes para a nossa detecção. Por isso teremos uma etapa de pré-processamento que diminuirá a complexidade da mesma: Aplicamos um Blur Gaussiano, que borra a imagem, para eliminar pequenos detalhes que não nos servirão. Em seguida fizemos duas Transformações Morfológicas, essas são filtros que, novamente, ajustam a forma do nosso objeto para simplificá-lo. Você pode ver mais sobre elas no tutorial do OpenCV, eu pessoalmente sempre o consulto quando preciso de uma transformação dessas. Como última etapa do pré-processamento iremos apagar tudo que não é preto da imagem. Como as rodas geralmente são pretas não há a necessidade de analisar outras cores (ou tom de cinza) Esta ultima imagem já está pronta para ser usada na função mas vamos fazer um passo extra: aplicar o Canny nela para entendermos como a imagem vai ser recebida pela Transformada Hough. Isso também nos ajuda a ajustar o param1, que vai no Canny interno da função. É importante que as rodas não tenham muitos detalhes internos, visto que isso pode “confundir”o detector. Mas, como visto na imagem, as rodas estão suficientemente destacadas e há poucos elementos para gerar incerteza à função. Já ajustamos o param1 quando experimentamos com o Canny acima e os parâmetros minDist, minRadius e maxRadius podem ser obtidos intuitivamente olhado para a imagem, já que as rodas normalmente tem tamanhos parecidos e uma distância mínima entre elas. Falta ajustar o parâmetro dp e param2. Esses dois podem ser encontrados por tentativa e erro, já que sabemos mais ou menos o comportamento deles. Após alguma experimentação o melhor ajuste de parâmetros foi: que encontrou os seguintes círculos: Para saber se esses são realmente os círculos que correspondem as rodas podemos desenhá-los na imagem original: Yeah, temos o nosso detector! Chamando novamente a função com esses parâmetros em outras imagens parecidas também deve gerar um resultado satisfatório. Só não esqueça de submeter as novas imagens ao mesmo pré-processamento. Espero que esse artigo e pequeno exemplo te ajude pra quando for precisar aplicar a cv.HoughCircles(). Novamente, você pode baixar um JNotebook com o código lá no meu repositório do GitHub. Para mais textos sobre Visão Computacional e IA em geral, continue ligado no Turing Talks. E para mais informações sobre o Turing USP siga a gente nas redes sociais: Facebook, Linkedin, Instagram, e servidor no Discord!"
https://medium.com/turing-talks/transforma%C3%A7%C3%A3o-watershed-com-opencv-68a5bd8196a0?source=collection_home---------24----------------------------,Transformação Watershed com OpenCV,,Luís Henrique de Almeida Fernandes,452,5,2021-05-30,"Seja bem vindo a mais um Turing Talks de Visão Computacional! Como já vimos em outras publicações nossas, uma tarefa muito comum em visão computacional é a segmentação de imagens. Basicamente, esse é o trabalho de dividir a imagem em várias seções que esperamos que sejam úteis (cada seção pode representar um tomate, por exemplo). Existem muitos algoritmos que resolvem esse problema, mas hoje vamos nos aprofundar na transformação Watershed. Vamos começar pelo básico, o que é uma watershed? Traduzindo para o português, watershed seria a linha que delimita uma bacia hidrográfica, a mesma bacia que estudamos em geografia no ensino médio. Para refrescar a memória, uma bacia hidrográfica é uma área onde toda a água (seja da chuva ou de degelo) flui para um único afluente (um rio ou um lago). A diferença é que, nesse texto, vamos trabalhar com imagens, olhando para a intensidade de cada píxel como se fosse uma altitude. Essa consideração nos permite interpretar qualquer imagem, independente de seu conteúdo, como um mapa topográfico. Esse é a premissa na base da transformação Watershed. Agora, tendo nossa imagem mapeada como um mapa de altitudes, a sacada da Watershed é “inundar” a imagem. Vamos escolher diversos pontos na imagem, que chamaremos de marcadores, de onde vamos simular o nível da água subindo gradativamente. Em algum momento, as “poças” de água crescendo dos diversos marcadores vão se encontrar, e é nesse ponto de encontro que vamos delimitar nossas bacias hidrográficas. Encontramos nossa watershed! A razão pela qual esse processo nos gera uma boa segmentação da imagem é que os objetos contidos nela devem formar “vales” ou “lagos” que serão delimitados por toda sua volta pela linha de watershed (o encontro dos “lagos”). Não parece tão complicado, né? Mas a implementação desse algoritmo não é tão simples. Sem otimizações ou marcadores bem localizados, nossa imagem pode facilmente sofrer de oversegmentation, o que não nos dá muita informação sobre os objetos na imagem. Por isso, na aplicação desse texto, vamos focar no tratamento da imagem para obter bons marcadores, e vamos usar a implementação de Watershed oferecida pela biblioteca OpenCV. Como disse anteriormente, queremos achar bons marcadores para nossa imagem. O ideal seria conseguirmos marcar o centro de cada objeto que desejamos segmentar, mas isso seria um trabalho exaustivo e, a esse ponto, poderíamos ter delimitado nossos objetos manualmente. Para contornar esse problema, vamos tentar separar nossa imagem em foreground (primeiro plano) e background (plano de fundo). Esperamos que nossos objetos estejam em primeiro plano, por isso, vamos colocar os marcadores no foreground. Antes de qualquer coisa, importamos o OpenCV para manipulação das imagens, matplotlib para fazer sua visualização e numpy para algumas outras operações no meio do caminho. A primeira tarefa na nossa lista, é fazer o thresholding da imagem. Esse processo “binariza” nossa imagem, assim como queremos (primeiro e segundo plano), nos dando uma ideia primitiva de qual plano cada parte da imagem pertence. Agora temos nosso primeiro plano (branco) e plano de fundo (preto). Porém, podemos melhorar ainda mais o tratamento da nossa imagem. Para isso, vamos lançar mão de transformações morfológicas. Em termos simples, uma transformação morfológica é qualquer transformação que mude o formato do que está dentro da imagem. Veja alguns exemplos a seguir: Os exemplos mostrados na figura são os que vamos utilizar no processamento. Dilation ou dilatação expande os objetos da imagem, o que nos vai dar mais certeza de que o plano de fundo restante realmente faz parte do plano de fundo. Já a erosion ou erosão faz justamente o contrário, diminuindo os objetos, nos dando a mesma certeza para o primeiro plano restante. Por fim, o opening ou abertura é simplesmente a erosão seguida de dilatação, o que remove ruído da imagem. Enfim, podemos aplicar tudo isso à nossa imagem, utilizando opening para remover ruido e dilation para obter sure_bg (nosso plano de fundo de que temos mais certeza). No caso de sure_fg, vamos utilizar uma função de distância de cada ponto no foreground ao background. Essa função nos permite, diferente da erosão, diferenciar objetos que podem estar em contato, enquanto sem ela, esses objetos seriam considerados um só. Ainda adicionamos mais um thresholding para retirar um pouco de ruído do nosso foreground e definir píxels não conhecidos como a subtração do foreground e background. Quase lá! Agora só temos que criar nossos marcadores utilizando a função connectedComponents, que vai atribuir um inteiro positivo diferente para cada elemento do foreground e 0 para o background. Nosso último problema é de compatibilidade. A implementação de Watershed da OpenCV considera tudo que temos certeza (primeiro ou segundo plano) como um inteiro positivo, e os desconhecidos como zero. Mas não é isso que temos, e para arrumar essa situação, somamos 1 a todos os markers. Assim, background fica em um único grupo (grupo 1) e o foreground como qualquer inteiro positivo. Dai, só zeramos o valor dos desconhecidos. Horaaaaaay! Chegamos no final! Agora basta aplicarmos a Watershed e marcar as linhas na nossa imagem original. Vemos que os resultados não são perfeitos para imagens no mundo real, mas mesmo assim a segmentação nos informa bem a posição de cada tomate. Em casos mais controlados, a segmentação por watershed pode ser uma ferramenta poderosa que não envolve aprendizado profundo, só uma sacada um pouco diferente. Você pode encontrar o código deste texto no nosso GitHub. Parabéns e obrigado por chegar até aqui! Este é o fim de mais um Turing Talks. Para mais textos, veja as sugestões abaixo e visite nossas redes sociais para não perder nenhuma novidade! Facebook, Instagram, Linkedin e nosso servidor no Discord"
https://medium.com/turing-talks/alan-turing-o-homem-que-mudou-a-hist%C3%B3ria-d0547822d264?source=collection_home---------23----------------------------,Alan Turing: O homem que mudou a história,"Vida, história e desafios",Vitoria,477,8,2021-06-20,"Alan Mathison Turing nasceu em 23 de junho de 1912 e foi um matemático, cientista da computação, lógico, criptoanalista, filósofo e biólogo teórico britânico. Turing foi extremamente importante para o desenvolvimento do conceito de computação e algoritmos como conhecemos hoje. Por esse motivo, ele é conhecido por muitos como o pai da ciência da computação. Alan nasceu em Londres, no distrito de Maida Vale. Seus pais o matricularam na escola St. Michael’s, em Charles Road, aos seis anos de idade. Desde cedo, Turing já demonstrava sinais de uma grande inteligência, que seria melhor desenvolvida durante seus anos escolares, já que era notada tanto por sua família quanto por seus professores e diretores. Alan também transitou pelas escolas Hazelhurst Preparatory School, de 1922 a 1926, e na Sherborne School, em 1926, que frequentou dos seus 13 anos a diante. Embora fosse reconhecido por certos docentes, alguns insistiam em levar em conta apenas seu desempenho nas matérias de humanas, enquanto sua verdadeira aptidão estava na área de exatas, como Matemática e Física. Por esse motivo, ele começou a resolver problemas extremamente avançados enquanto ainda tinha apenas 15 anos, não tendo sequer tido matérias mais avançadas, como cálculo. Foi na última escola em que estudou que Turing conheceu Christopher Collan Morcom, seu grande amigo pessoal que é descrito por historiadores como sendo “seu primeiro amor” — fato comprovado apenas por cartas que Turing escrevia. A amizade entre ambos foi um ponto de extrema importância na vida de Alan, tendo o inspirado em diversos momentos da sua vida. Entretanto, a amizade teve um triste fim, com a morte de Christopher por complicações de tuberculose em 1930. Sobre o amigo, Alan Turing escreveu: “Tenho certeza de que não poderia ter encontrado em lugar nenhum outro companheiro tão brilhante e, ao mesmo tempo, tão charmoso e despretensioso. Eu considerava meu interesse em meu trabalho, e em coisas como astronomia (que ele me apresentou), como algo a ser compartilhado com ele e acho que ele sentia o mesmo por mim… Eu sei que devo colocar muita energia e interesse pelo meu trabalho como se ele estivesse vivo, porque é isso que ele gostaria que eu fizesse.” Alguns historiadores teorizam que a morte do amigo foi o que guiou Alan para algumas crenças, como o ateísmo. Entretanto, são apenas teorias baseadas nas cartas que ele escrevia para a mãe de Christopher depois da morte do amigo. Após sair da escola, Turing estudou e se formou na King’s College, uma faculdade constituinte da Universidade de Cambridge. Lá, ele se graduou com honras em sua turma no curso de matemática. Logo depois de formado, Turing começou a publicar e a trabalhar em diversos artigos, muitos de extrema importância, como o seu trabalho provando o teorema central do limite. Dentre esses trabalhos, um dos mais importantes foi o artigo “Sobre números computáveis, com uma Aplicação ao Entscheidungsproblem”, que continha uma noção de uma “máquina universal” que poderia executar tarefas de quaisquer outras máquinas de computação, como cálculos. Foi reconhecido, anos depois, que a noção central do computador moderno se deve ao artigo de Turing. Também depois de formado, enquanto continuava sua carreira acadêmica através do PhD, ele se dedicou ao estudo de criptologia. Por fim, em 1938 ele conseguiu concluir a pós e obter seu título de doutor com o artigo Sistemas de Lógica Baseada em Ordinais. Em 1950 ele desenvolveu o que hoje em dia ficaria conhecido como “Teste de Turing”, cujo o objetivo era definir um padrão através do qual uma máquina poderia ser considerada inteligente. No caso proposto, a partir do momento em que um humano não conseguisse distinguir se estava falando com um humano ou uma máquina, aquela máquina poderia ser considerada inteligente. Para saber detalhadamente sobre uma das teorias mais importantes criadas por Turing, leia nosso artigo sobre o assunto aqui: Como já mencionamos, o trabalho de Turing foi crucial para a concepção da computação moderna com as Máquinas de Turing. As ideias derivadas deste trabalho ficaram conhecidas como a tese de Church-Turing, em homenagem aos seus autores Alonzo Church e, é claro, Alan Turing, que chegaram, sozinhos e em momentos distintos, a conceitos equivalentes de computabilidade. O trabalho teórico logo se demonstrou muito útil. Com o início da Segunda Guerra Mundial, as forças armadas alemãs começaram a utilizar as máquinas Enigma para comunicação. Estas máquinas tinham o trabalho de “embaralhar” ou criptografar (de forma que se acreditava ser indecifrável) as mensagens alemãs transmitidas por rádio, dando enorme vantagem tática dos contra os aliados. É aí que entra Turing. Logo que a Grã-Bretanha entrou na guerra, Alan começou a trabalhar na Government Code and Cypher School, divisão de Bletchley Park, onde tentava decifrar o código gerado pela Enigma. Durante esse período, Turing tomou inspiração no seu próprio trabalho em computabilidade e nas máquina Bomba (criada pelos poloneses para quebrar o Enigma) e projetou, junto com seus colegas em Bletchley Park, a máquina Bombe. Esse mecanismo, mais eficiente que sua versão polonesa, era capaz de testar, automaticamente, configurações dos códigos Enigma para diminuir as configurações que teriam que ser checadas manualmente. Com esse avanço, os aliados começaram a conseguir decifrar as mensagens alemãs frequentemente, evitando ataques em navios cargueiros e militares no oceano Atlântico. Se estima que o trabalho de Turing na Segunda Guerra salvou mais de 14 milhões de vidas, e encurtou a guerra em dois anos. Após a Guerra, Alan Turing continuou trabalhando em diversas áreas, desde formação de padrões por reações químicas na biologia até contribuições para os primeiros computadores eletrônicos, ideias que antecederam a arquitetura de Von Neumann e trabalhos em estatística, probabilidade e criptografia que se mostram relevantes até hoje. Em 1941, Turing pediu sua colega Joan Clarke, do Hut 8, em casamento. O noivado, entretanto, foi curto. Depois de pouco tempo, Turing admitiu a ela sua homossexualidade, culminando no fim do relacionamento. Sua orientação sexual permaneceu um segredo durante os seguintes 11 anos. Entretanto, em 1952, quando tinha 39 anos, Turing iniciou um relacionamento secreto com Arnold Murray, um jovem de 19 anos. Nesse período, a casa de Alan foi roubada, e durante as investigações policiais, ele admitiu em seus testemunhos um relacionamento com Murray. Nessa época, práticas homossexuais eram consideradas crime na Inglaterra— como em diversas outras partes do mundo — , por esse motivo Turing foi acusado por “indecência” de acordo com a Seção 11 da Lei de Emenda à Lei Criminal de 1885 e condenado a julgamento. O caso foi levado ao tribunal, e ainda em 1952 Alan se declarou culpado. Em sua sentença, ele pôde escolher entre prisão ou liberdade condicional ligada à tratamentos com hormônios (castração química), optando pela liberdade. As injeções continham o chamado estilbestrol (agora conhecido como dietilestilbestrol ou DES), um estrogênio sintético, e duraram cerca de 1 ano, deixando Turing com sequelas irreversíveis. Sua condenação culminou em sua proibição de entrar nos EUA e levou a sua demissão de seu emprego no governo, onde ainda servia como consultor de criptografia. Em 1954, dois anos após sua condenação, Turing foi encontrado morto em sua residência. Na autópsia, foi revelado que o motivo fora intoxicação por cianeto. Seu corpo foi cremado e sua cinzas espalhadas no jardim do crematório, assim como as de seu pai. Até os dias atuais, muitas teorias são levantadas a respeito de sua morte, sendo principalmente três: Quando seu corpo foi descoberto, uma maçã comida pela metade estava próxima do local, e muito se especulou que Alan tivesse envenenado a fruta com uma dose letal de cianeto e depois ingerido, culminando na sua morte. Inclusive, um inquérito foi aberto sobre as causas da morte, determinando que se tratou de um suicídio, apesar da maçã nunca ter sido testada para a substância. Após a conclusão do inquérito, o professor Jack Copeland questionou a conclusão da polícia, dando uma teoria alternativa para a morte de Turing: inalação acidental de cianeto ao longo dos anos, vindo de um aparelho para galvanizar ouro em colheres, que utilizava cianeto em sua composição. Essa teoria é muito relevante e acreditada pelo fato de que Alan tinha um aparelho desses em sua casa e porque a quantidade da substância encontrada em seu corpo se assemelhava com doses pequenas através de inalação. Nesse caso, a maçã teria apenas sido uma coincidência. Essa é provavelmente a teoria menos falada a respeito de sua morte. De acordo com alguns pesquisadores e teóricos da conspiração, os serviços secretos temiam que os comunistas pudessem sequestrar ex membros afastados do governo para conseguir informações confidenciais. Como Turing fora banido dos EUA, mas não dos países europeus, mesmo depois de sua condenação ele teria realizado algumas viagens pelo continente, inclusive em países próximos a cortina de ferro, o que preocupou o governo. Por fim, o serviço secreto o considerou uma ameaça muito grande para o país caso fosse capturado, optando por assassiná-lo. Muitos anos depois da morte de Alan Turing, apenas em 1967, a Inglaterra descriminalizou a prática homossexual no país — sendo descriminalizada em 1980 na Escócia e 1982 na Irlanda do Norte. Entretanto, somente 42 anos depois, através de uma petição com 30 mil assinaturas, o primeiro ministro britânico Gordon Brown pediu desculpas em nome do governo pelo tratamento recebido por Turing na época: “Ainda que Turing tenha sido tratado sob as leis da época e nós não possamos voltar no tempo, seu tratamento foi evidentemente injusto e eu tenho o prazer de ter a chance de pedir profundas desculpas por todos pelo que lhe aconteceu. Então, em nome do governo britânico e de todos que vivem em liberdade graças ao trabalho de Alan eu orgulhosamente digo: Perdão, você merecia algo muito melhor” De 2005 a 2015, Jonh Leech, um deputado, apresentou diversas propostas para que fosse concedido um perdão a Turing, dessa forma retirando as acusações contra seu “crime”. Depois de muita insistência, finalmente o governo retirou suas acusações e emitiu um perdão oficial. Turing também recebeu um perdão oficial da rainha em 2013, quando o casamento gay foi legalizado na Inglaterra. Além disso, uma lei intitulada “Lei Turing” entrou em vigor logo após, declarando perdão a todas as outras 75 mil pessoas que haviam sido condenadas por sua orientação sexual no país. Alan Turing foi um homem excepcional com uma mente brilhante, mas que teve sua história ofuscada durante muito tempo apenas pela sua orientação sexual. Nós, do Turing USP, não poderíamos deixar de prestar nossa homenagem em razão de um mês tão importante. Junho é reconhecido não somente pelo nascimento e pela morte de Alan Turing, mas também como o mês do orgulho LGBTQ+. Ano após ano ainda precisamos relembrar todos os esforços que ajudaram a comunidade LGBTQ+ a chegar onde estão, e lembrar constantemente que esse é apenas o começo da luta pela igualdade merecida. Em um país como o nosso, todo apoio a essa luta é pouco. Devemos sempre procurar fazer nossa parte para que o mundo seja um lugar acolhedor e pacífico para todos os representantes da comunidade, ajudando, por fim, na luta por um mundo onde Alan Turing merecia ter vivido. Para acompanhar o Turing USP siga a gente nas redes sociais: Facebook, Linkedin, Instagram, e servidor no Discord!"
https://medium.com/turing-talks/lan%C3%A7amento-reposit%C3%B3rio-de-aprendizado-por-refor%C3%A7o-19b395610fc1?source=collection_home---------22----------------------------,Lançamento — Repositório de Aprendizado por Reforço,Aprenda do zero a técnica de aprendizado de máquina que vem cada vez mais ganhando atenção,Nelson Alves Yamashita,849,5,2021-07-04,"Projeto desenvolvido por Ariel Guerreiro, Bernardo Coutinho, Eduardo Eiras, Felipe Machado, Fernando Matsumoto, Luís Henrique Fernandes, Nelson Yamashita e Rafael Coelho Bem vindo a mais uma edição do Turing Talks! Hoje iremos divulgar um pouco de um projeto que temos realizado no último ano, nosso Repositório de Aprendizado por Reforço! Ele está disponível neste link no nosso Github: O Aprendizado por Reforço (RL), em inglês Reinforcement Learning, é uma subárea do Aprendizado de Máquina que estuda programas que aprendem a realizar tarefas complexas por tentativa e erro, a partir do feedback que recebe de suas ações. Esse tipo de aprendizado se assemelha muito com o processo de aprendizado intuitivo dos seres humanos, no qual o indivíduo experimenta algo, e com base na resposta desse experimento, decide se ele vale a pena ou não. Quando uma criança encosta o dedo em uma superfície quente, por exemplo, ela recebe uma resposta negativa, e não repete a mesma ação novamente. As técnicas de Aprendizado por Reforço são muito poderosas, já que conseguem gerar comportamentos extremamente complexos, muito difíceis de serem programados, como fazer um robô caminhar ou até correr sem cair. Cumprindo com nosso pilar de disseminação do conhecimento, queríamos criar um material em português de nível acessível sobre RL, que contasse com explicações didáticas ao mesmo tempo que possuíam algum rigor sobre a teoria, e que também tivessem as implementações dos algoritmos descritos. Com isso compilamos informações de diversas fontes que são referência no estudo de aprendizado por reforço em inglês, como o fantástico livro do Sutton & Barto, a documentação do repositório Spinning Up in Deep RL da OpenAI, aulas do lendário David Silver, entre outros. Fizemos questão também que cada explicação de algoritmo seja acompanhada por uma implementação em um Jupyter Notebook. Para que dessa maneira você consiga acompanhar a explicação teórica junto com suas consequências no código. O repositório está dividido em três principais partes: a introdução, os algoritmos clássicos de RL e os algoritmos de RL profundo. Nessa seção explicamos alguns conceitos principais de RL, essenciais para o entendimento dos outros tópicos do repositório. Aqui você aprenderá sobre coisas como o que um agente, estado, recompensa, ação e política e como esse conceitos interagem entre si. Além disso, também incluímos a explicação sobre um tema que pode causar bastante confusão para aqueles que estão aprendendo RL pela primeira vez, a diferença em algoritmo On-Policy e Off-Policy. Aqui você poderá ler sobre alguns algoritmos de RL que apresentam os fundamentos teóricos da área. Dizemos que eles são “clássicos” por utilizarem mais métodos matemáticos-probabilísticos para garantir seu funcionamento, ao invés de redes neurais. Nele você encontrará explicações sobre o que é o problema do k-Armed Bandits (que até já falamos aqui no Medium), muitas vezes usado como exemplo introdutório, por conta da simplicidade teórica e de implementação, o que ajuda a mostrar os conceitos aprendidos na parte de introdução. Também explicamos sobre métodos de Monte Carlo em Aprendizado por Reforço com uma evolução natural do processo, os método de Temporal-Difference Learning. Este método é de grande importância para entender alguns algoritmos importantes de RL profundo. O Aprendizado por Reforço Profundo é a combinação do Aprendizado por Reforço com o Aprendizado Profundo (Deep Learning). Nesta área, são utilizadas redes neurais, potentes modelos de reconhecimento de padrões, para aprender e estimar importantes funções, como a política ótima de um agente ou a função de valor de um problema. Como todos os algoritmos serão construídos a partir de redes neurais, é recomendado utilizar algum framework de Deep Learning, como o PyTorch ou o Tensorflow. Neste repositório, todas as redes serão criadas usando o PyTorch. Caso não esteja muito familiarizado com esta biblioteca, recomendamos os seguintes materiais: Nosso Workshop de Redes Neurais com PyTorch e nosso post aqui no medium de Construindo uma Rede Neural do zero com PyTorch. Aqui você encontrará explicações sobre alguns dos algoritmos mais potentes de Aprendizado por Reforço, implementados de uma maneira didática para um melhor entendimento. Você verá algoritmos como Deep-Q Networks (que também já fizemos um texto aqui), uma evolução do algoritmo de Temporal difference de Q-Learning, Policy Gradient, que tentam estimar diretamente uma política ótima e os poderosos algoritmos de Actor Critic, que combinam estimadores de função de valor e estimadores de uma política ótima. Esperamos que você aproveite o repositório e consiga aprender e solidificar seu conhecimento nesta área que às vezes não é tão bem compreendida quanto os outros modelos de aprendizado de máquina! Pretendemos continuar atualizando o repositório com outros algoritmos e mais exemplos, então não esquece de deixar uma star nele! Caso você encontre algum erro ou possua alguma sugestão, ficaremos muito felizes com sua contribuição no repositório, seja ela um issue ou um pull request :D. Você também pode entrar em nosso Discord onde estamos constantemente realizando aulas abertas e publicando outros assuntos interessantes. Você também pode nos acompanhar em nossas redes sociais: Facebook, Linkedin, Instagram, Medium!"
https://medium.com/turing-talks/consegue-uma-intelig%C3%AAncia-artificial-escrever-como-guimar%C3%A3es-rosa-313d7d1b1acb?source=collection_home---------21----------------------------,Consegue uma Inteligência Artificial escrever como Guimarães Rosa?,Como usar uma Rede Neural Recorrente para construir um modelo de língua,Julia Pocciotti,1106,8,2021-08-01,"Talvez você já tenha visto alguns exemplos pela internet sobre IA' s ou bots treinados em milhares de textos para escrever como um autor específico ou replicar um gênero textual. Diferentes técnicas podem ser usadas para gerar esse tipo de resultado. Hoje, vamos ver como construir um modelo para ""escrever"" como João Guimarães Rosa utilizando Redes Neurais Recorrentes. Explicarei aqui o básico sobre esse tipo de Rede Neural, mas, se quiser se aprofundar sobre o assunto, recomendo o nosso Turing Talks sobre o tema: Se quiser conferir o projeto diretamente, você pode acessar o repositório do github com o código completo, ou então o site com o gerador de texto aqui. Antes de começarmos a falar sobre o nosso modelo, vamos antes ver um pouco melhor qual é a nossa tarefa em questão. Um modelo de língua possui uma tarefa bem simples: prever a próxima palavra ou caractere em uma sentença. Qualquer tipo de modelo que esteja cumprindo com essa tarefa, ou sendo treinado para isso, pode ser caracterizado como um modelo de língua. Você com certeza já encontrou modelos de língua pela internet, por exemplo, quando estamos fazendo uma busca no Google e aparecem sugestões para terminar a frase, isso é um modelo de língua. De maneira mais formal, um modelo de língua irá aprender a distribuição de probabilidades em uma sequência de palavras. Ou seja, dada uma sequência de palavras de tamanho m, o modelo atribui uma probabilidade P(w(m+1)|w(1), …, w(m)). João Guimarães Rosa é sem dúvida um dos maiores escritores da nossa literatura. Além de escrever contos e romances, Rosa foi também médico e diplomata brasileiro. Suas histórias são ambientadas em meio ao sertão brasileiro, e dentre vários aspectos formais presentes em sua obra, destaca-se o seu trabalho com a linguagem, esta, marcada por inúmeros neologismos e falas regionais. — Epa! Nomopadrofilhospritossantamêin! Avança, cambada de filhos-da-mãe, que chegou minha vez!…E a casa matraqueou que nem panela de assar pipocas, escurecida àfumaça dos tiros, com os cabras saltando e miando de maracajás, e NhôAugusto gritando qual um demônio preso e pulando como dez demônios soltos. (trecho do conto ""A hora e a vez de Augusto Matraga"") Para construir o nosso modelo, iremos usar todos os livros publicados pelo autor, são eles: Sagarana, Corpo de Baile, Grande Sertão Veredas, Primeiras Estórias, Tutameia, Estas estórias e Ave, palavra. Os PDFs de todos as obras podem ser encontrados no site LeLivros. Tendo os PDFs dos nossos livros baixados, precisamos agora convertê-los para um arquivo .txt. Para isso, iremos utilizar a biblioteca textract: Para simplificarmos o nosso código, iremos juntar todos os textos em apenas um único arquivo. Além disso, informações como notas iniciais da editora, informações de publicação e introduções feitas por outros autores foram retiradas manualmente no processo. Com o texto preparado, vamos ler o arquivo o nosso arquivo e verificar algumas informações básicas, como o número de caracteres: Diferente de nós, humanos, os computadores não conseguem abstrair informações das palavras diretamente. Sendo assim, precisamos pensar em algum tipo de codificação numérica para que a máquina possa processar o nosso texto. Neste caso, iremos criar dois tipos de vetores numéricos: Aqui, criamos três representações importantes: temos o char2idx para codificar o nosso texto em uma representação numérica, o idx2char para decodificar o texto e text_as_int , que representa o nosso texto já codificado. Ao lermos um livro, ou ao utilizar a linguagem de uma maneira geral, estamos levando em conta a ordem na qual os caracteres e as palavras são dispostas na página, ou seja, a ordem em que esses dados aparecem são extremamente importantes para a constituição do sentido do texto. Quando colocamos essa informação em conjunto, temos um texto coeso, do qual conseguimos extrair sentido. Além da linguagem, podemos pensar em outros tipos de dados que também obedecem uma sequência, como informações de um eletrocardiograma, as bases nitrogenas em um DNA, ou séries temporais, por exemplo. Quando temos dados que são dispostos em uma ordem específica, utilizamos modelos sequenciais, como uma Rede Neural Recorrente (Recurrent Neural Network, em inglês — RNN), por exemplo. Uma vantagem das RNNs é que com elas conseguimos treinar facilmente inputs e outputs com tamanhos diferentes e, principalmente, a nossa rede consegue aprender a generalizar as informações textuais independente da sua posição na frase, ou seja, com essa representação o modelo conseguiria prever que a palavra ""Augusto"" muito provavelmente teria a sua próxima palavra como ""Matraga"", independentemente de ""Augusto"" aparecer no início, meio, ou final da frase. Em uma RNN tradicional, temos a seguinte representação: Como comentamos anteriormente, aqui estaremos lidando com dados que obedecem uma determinada sequência. Sendo assim, representaremos os nossos dados em ""timesteps"", e para fazermos previsões sequenciais cada y<t> (output) levará em conta não apenas o valor do seu timestep x<t>(input), mas também usará a informação de seu estado anterior a<t-1>. Se você quiser saber mais informações sobre como funciona uma RNN, dê uma olhada no nosso Turing Talks sobre o tema aqui. Dado um caractere, ou uma sequência de caracteres, qual será o caractere mais provável? — Essa é a tarefa para a qual estaremos treinando o nosso modelo. Sendo assim, teremos uma sequência de caracteres como entrada do modelo e iremos prever o caractere seguinte. Para isso, iremos dividir o texto em sequência de exemplos, e cada sequência terá um número de caracteres de tamanho determinado pela variável seq_length . Para cada sequência de entrada, os targets terão o mesmo comprimento, com exceção de um caractere deslocado para a direita. Dessa forma, a nossa rede ficará assim: Para isso, vamos usar o método from_tensor_to_slices to módulo Dataset to TensorFlow para criar um um objeto desse tipo a partir do text_as_int , que já tínhamos definimos acima. Por fim, vamos separar os dados em batches para o nosso treinamento. Nessa representação, usaremos o comprimento de cada sentença de input limitada para 100 caracteres: Em seguida, precisamos criar tuplas dessas sequências para alimentarmos a nossa RNN: Por fim, vamos embaralhar o nosso dataset e dividi-lo em batches: Nessa representação, usaremos três camadas: Para treinar a nossa rede, usaremos o otimizador ""Adam"" e a função de custo ""Sparse Categorical Loss"". Além disso, para conseguirmos carregar os nossos pesos e salvar a nossa performance, iremos configurar um diretório para os checkpoints: Por fim, usaremos 10 épocas para fazer o nosso treinamento. Para isso, recomendo que você utilize uma GPU. Se você não possui uma GPU no seu computador, você pode usar o Google Colab, e em Runtime>Change runtime type selecione ""GPU"". Para ver a posição do nosso último checkpoint, utilizamos a seguinte notação: tf.train.latest_checkpoint(checkpoint_dir) Finalmente, podemos usar a função que definimos anteriormente para criar um novo modelo com batch_size=1 , e usar o método load_weights salvos do último checkpoint: Agora o nosso modelo está pronto para fazer previsões! Tudo o que precisamos é criar uma função para processar o texto da mesma forma que fizemos anteriormente quando treinamos o modelo: Agora basta escolher uma string e ver o seu modelo fazer previsões! Agora que temos um modelo treinado, podemos usar plataformas como o Streamlit e o Heroku para fazer a versão com o deploy do nosso modelo. Com o Streamlit conseguimos facilmente carregar o nosso modelo, construir uma interface para o usuário escrever a string inicial, e então fazer as previsões do texto. Por fim, utilizamos o Heroku para colocar o que construimos com o Streamlit em uma url compartilhável. Com essas duas plataformas, chegamos em um resultado final assim: Se quiser brincar com o modelo que construímos, basta acessar o site: http://gerador-texto-guimaraes.herokuapp.com/ . Nesse Turing Talks não vamos entrar em detalhes sobre como fazer o deploy do modelo (quem sabe em um próximo texto!). Mas se você quiser ver o código, ele está todo disponível neste repositório: É claro que os nossos resultados estão longe de gerar um texto como um conto ou obra de João Guimarães Rosa, mas justamente pela linguagem muito característica do autor, repleta de neologismos e marcada pelo falar popular do sertão, é curioso ver como o nosso modelo consegue replicar alguns desses padrões, gerando resultados, no mínimo, divertidos. Usando uma representação como essa, conseguimos criar vários outros modelos língua com corpus diferentes. Podemos ensinar uma IA a escrever como Shakespeare, J. R. R. Tolkien, e muitos outros! Basta termos um dataset e tratá-lo para essa tarefa. Se você gostou dessa abordagem, saiba que RNNs são apenas o começo dentre vários outros tipos de redes que podemos usar para construir modelos de língua. Hoje, temos modelos muito mais sofisticados para modelar a linguagem e gerar resultados ainda mais surpreendentes, como o GPT-2, GPT-3 e, o mais recente, Google MUM. Por fim, não deixe de acompanhar o Turing USP no Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Bons estudos e até a próxima! “O real não está na saída nem na chegada: ele se dispõe para a gente é no meio da travessia.” João Guimarães Rosa Agradecimento especial à Luísa Mendes Heise pela ajuda na coleta do corpus"
https://medium.com/turing-talks/seus-primeiros-passos-com-o-pca-ee411d4bc8d2?source=collection_home---------20----------------------------,Seus primeiros passos com o PCA,,Murilo Krebsky Hobus,652,5,2021-08-22,"Olá! Seja bem-vindo a mais um Turing Talks! Neste Turing Talks você vai aprender um pouco sobre como funciona a Análise de Componentes Principais (o famoso PCA), pra que ela serve e como fazê-la na prática com o sklearn. Então, sem mais delongas, vamos começar! Imagine que você esteja trabalhando com um dataset com 30 colunas numéricas. É claro que você não vai ser capaz de plotar essas informações em um gráfico, afinal só conseguimos enxergar em 3D, e o gráfico com as informações do dataset estaria em um espaço de dimensão 30! E aí queestá a primeira utilidade do PCA: visualização. A Análise de Componentes Principais é uma ótima ferramenta para conseguirmos visualizar essas informações em espaços de dimensão alta.Mas agora você pode estar se perguntando: mas como isso é possível? Eu não vou perder toneladas de informações? E agora temos a segunda utilidade do PCA: através dele conseguimos fazer um ”resumo”das informações presentes, porém não sendo necessária a utilização de todas as variáveis que temos (O nome bonito para essa utilidade é redução de dimensionalidade). Primeiramente, vamos relembrar um conceito muito importante para o entendimento do PCA: a Covariância. A covariância é uma medida de dispersão muito usada na estatística, que tenta expressar quão relacionadas estão duas variáveis aleatórias. Vamos tentar esclarecer um pouco mais esse conceito através de sua fórmula e do exemplo abaixo: Considere os seguintes vetores: x = {x₁, x₂, …, xₖ}, y = {y₁, y₂, …, yₖ}. A covariância entre x e y é definida como: são as médias dos valores de x e y respectivamente. Além disso, é importante notar que o valor: é a variância de x. Ou seja, duas variáveis possuem alta correlação se existe um “padrão”entre elas: quando uma cresce a outra também, ou o inverso. Mas o que isso tem a ver com PCA? Isso é que vamos ver a seguir. O objetivo do PCA é colocar os dados em um espaço onde coordenadas distintas possuam covariância igual a 0, e esse fato faz com que apenas as coordenadas que realmente possuem relevância fiquem evidentes. Para isso, o PCA encontra as componentes principais do novo conjunto de dados, ou seja, os eixos em que os dados estão nesse novo espaço(daí que vem o nome).O caminho que a Análise de Componentes Principais traça para chegar até esse resultado é um pouco longo e não será discutido nesse Turing Talks, mas só como spoiler, conceitos como matriz de mudança de base e matrizes simétricas são altamente utilizados (pensou que nunca iria usaraquela sua aula de álgebra linear né?!)Ufa! Agora que já temos um bom embasamento sobre a teoria, vamos ver como o PCA funciona na prática. Agora vamos ver como utilizar o PCA utilizando a biblioteca sklearn. Para isso, vamos utilizar um conjunto de dados que já está embutido na biblioteca, chamado breast cancer dataset, que é um conjunto de dados que rotula tumores como benignos ou malignos de acordo com uma série deatributos. Como podemos ver acima, para cada tumor observado, 30 propriedades foram coletadas. Vamos utilizar o PCA para reduzir a quantidade de variáveis e conseguir fazer uma visualização melhor dos dados.O primeiro passo é instanciar o PCA, e faremos isso utilizando a biblioteca sklearn. Feito isso, podemos transformar nossos dados com o PCA. Agora com os dados já transformados, podemos começar a explorar algumas das vantagens que o PCA nos traz. A primeira delas, é que conseguimos ver qual é a importância de cada direção principal (quanto maior a variância da componente, mais importante ela será). Pensando bem, isso faz total sentido, pois se olharmos na figura 1 novamente, podemos ver que todos os pontos têm valores extremamente próximos no eixo com menor variância, enquanto que o que os torna diferentes na verdade é o eixo com maior variância. Com o PCA já podemos ver quais são as direções com maior variância e qual sua representatividade em porcentagem: O PCA está nos dizendo que uma direção representa quase toda a informação dos dados! Isso é um pouco duvidoso não? A reposta é sim! Essa “explicabilidade”absurda por apenas uma direção é explicada por dois motivos: Esses são dois fatores que podem arruinar a sua Análise de Componentes Principais. Vamos então retirar esses outliers e colocar todos os dados na mesma escala. Para a remoção dos outliers iremos utilizar uma medida chama Z score, e para a normalização dos dados o StandardScaler do sklearn. Agora, calculando mais uma vez as variâncias e a variância explicada temos: Veja como nossa explicabilidade dos dados mudou! Agora a principal componente representa “apenas”cerca de 0.45 da informação total dos dados.Por último, mas não menos importante, vamos visualizar esses dados no espaço 2D: Veja como o tratamento dos dados fez uma enorme diferença quando tentamos agrupar as classes com apenas 2 atributos. Se você gostou do texto continue acompanhando os próximos Turing Talks, e nos siga em nossas redes sociais: Facebook, Instagram, Linkedin e nosso servidor no Discord."
https://medium.com/turing-talks/alphafold-2-entenda-seu-funcionamento-e-implica%C3%A7%C3%B5es-para-biologia-computacional-2ace80b1b70b?source=collection_home---------19----------------------------,AlphaFold 2 — Entenda seu Funcionamento e Implicações Para Biologia Computacional,,Nelson Alves Yamashita,842,9,2021-08-29,"Olá e bem vindo a mais um Turing Talks! Hoje falaremos sobre um assunto que ganhou bastante atenção recentemente: o fantástico projeto da DeepMind capaz de predizer estrutura de proteínas, o AlphaFold 2. Caso você não se lembre, em 2020 um time de pesquisadores de inteligência artificial da Google DeepMind venceu o campeonato bianual do Critical Assessment of Structural Prediction, um campeonato em que diversos biólogos computacionais tentam construir modelos que consigam predizer a estrutura de diversas proteínas determinadas em laboratório sem informação pública. O assunto voltou a ganhar atenção após o lançamento em julho do tão esperado artigo do modelo na Nature explicando como a ferramenta funcionava. O código fonte do projeto também foi disponibilizado publicamente. Inclusive, você pode até rodar uma versão um pouco mais simplificada no Google Colab agora mesmo! Primeiro, é importante lembrar a importância das proteínas. Elas são como pequenas máquinas em nossos corpos, responsáveis pelas mais diversas funções. Essas funções são definidas por sua estrutura tridimensional porém descobrir essa estrutura costumava ser um processo demorado e custoso, que envolvia diversos experimentos em laboratórios. Com o lançamento do AlphaFold 2 esse processo se torna muito mais fácil! Isso significa que caso um pesquisador queira saber a estrutura de uma proteína (apenas sabendo sua sequência de aminoácidos), em questão de alguns dias ou horas ele já terá informações sobre sua estrutura! Uma aplicação direta dessa descoberta é a possibilidade do design de proteínas. Como a funcionalidade de uma proteína está diretamente atrelada a sua estrutura, se queremos desenvolver uma proteína com uma funcionalidade específica precisaremos que ela também dobre de uma maneira específica. Como o processo de predição da estrutura de uma proteína é acelerado pelo AlphaFold 2, essas descobertas podem acabar vindo muito mais rápido! Um exemplo de uma aplicação desse design de proteínas pode ser uma proteína que consegue se fundir com a proteína de algum patógeno (como um certo vírus respiratório), que após a fusão é posta em uma vacina que pode fortalecer de maneira ainda mais eficaz o sistema imune do usuário. Além disso, muitos outros problemas e áreas de pesquisa que estavam restringidos pelo problema da predição da estrutura da proteína serão agora desenvolvidos, o que abre diversas novas oportunidades para novas linhas de pesquisa. Nota: algumas das explicações vão estar simplificadas, dando maior valor a intuição do que o a tecnicidade do modelo. Caso tenha interesse, recomendamos ler o material suplementar do artigo (em inglês) para poder entender profundamente seu funcionamento. O modelo funciona com três partes principais: Essa é a ideia básica por trás da ferramenta. Agora vamos explicar um pouco melhor cada uma delas. Primeiro vamos explicar o que são aqueles dois termos estranhos: Um MSA pode ser entendido como um tabelão de diversos sequenciamentos de aminoácidos (que são tratados como uma sequência de letras) parecidos, mas não idênticos de diversas espécies. Isso possibilita identificar aminoácidos que possuem uma tendência maior a sofrerem mutação ou que estão mais correlacionadas. Por exemplo, se temos dois aminoácidos A e B de uma proteína essencial como a hemoglobina que possuem uma interação. Se o aminoácido A sofre mutação, por conta da seleção natural, se o aminoácido B não sofrer uma mutação que conserve essa interação, ele não será selecionado, mantendo essa interação entre as espécies futuras. É uma análise que se baseia no conceito biológico de que, evolutivamente falando, muitos organismos possuem proteínas que executam funções similares (por exemplo, transportar oxigênio pelo sangue ou digerir açúcar) já que compartilham um ancestral em comum. A representação em pares (pair representation) usada no AlphaFold é um vetor de duas dimensões que tem como intuito mostrar qual aminoácido tem mais chance de entrar em contato com o outro. Essa representação em pares usa em grande parte templates chamados de distogramas, mapas que calculam a distância entre dois aminoácidos e as coloca numa espécie de matriz. A ideia por trás deles é similar ao dos MSA: como muitas proteínas desempenham funções similares em diversas espécies, elas devem ter estruturas similares, possibilitando essas predições. Então mesmo que suas sequências sofram diversas mutações, a estrutura (nesse caso mapeada como distância) se manteria. Dessa maneira, o AlphaFold procura por alinhamentos similares em um banco de dados genético para criar seu MSA, paralelamente, procura por distogramas similares em banco de dados de estruturas. Por conta de como o AlphaFold faz suas buscas, algumas vezes ele receberá distogramas que não são suficientes ou até não receberá distogramas, isso evita que o modelo sofra overfitting copiando a estrutura, tendo que realmente aprender a partir do sequenciamento. Finalmente o AlphaFold junta todas essas informações passadas: sequência de entrada, MSA, distogramas; e monta os dois produtos finais do pré-processamento, a representação do MSA e a representação em pares. A ideia principal do Evoformer é criar um fluxo de informação constante entre a informação evolutiva (o MSA) e a informação geométrica/distância (a pair representation), para que assim consiga não só aperfeiçoar as duas representações, mas que elas também consigam comunicar conhecimento entre si. Dessa maneira o modelo está usando sua noção atual da estrutura para melhorar sua informação sobre o alinhamento das sequências, que por sua vez é utilizado para melhorar a hipótese sobre a estrutura, e assim em diante. Um bloco do Evoformer recebe uma representação do MSA e uma pair representation como entradas, e essas acabam também sendo suas saídas. Acontece que cada uma dessas “linhas” é na verdade uma arquitetura de redes neurais que está cada vez mais mostrando resultados impressionantes, tratam-se de dois transformers. Infelizmente seria muito complexo explicar exatamente como funciona essa arquitetura nesse texto, já que ela merece um TT só dela ;). Mas a ideia principal é que dentro de cada bloco Após 48 desses blocos, a primeira sequência na representação do MSA é passada junto com a pair representation para o último bloco, aquele que irá finalmente predizer a estrutura da proteína. Finalmente chegamos a parte final da arquitetura do modelo, a parte da predição da estrutura! Temos nela 8 blocos de pesos compartilhados (similar ao de uma rede convolucional) que inicialmente recebe a representação de uma sequência do MSA, representando as informações sobre as variações na sequência, e a pair representation, representando as informações sobre a interação dos aminoácidos. Ela também introduz uma nova informação que será essencial para a construção da estrutura 3D, um “suporte principal” (backbone), pequenos triângulos que irão ditar a localização global, e rotação local do aminoácido. Essas transformações que serão aplicadas nos backbones são representadas por um objeto matemático que chamamos de matriz de transformação afim. A cada iteração o AlphaFold gera um conjunto dessas matrizes que representam a movimentação e rotação de cada aminoácido. O maior diferencial dessa parte da arquitetura do modelo é a utilização de algo que os autores chamam de “Invariant Point Attention”(Atenção a Pontos Invariantes— IPA) para atualizar a informação sobre o sequenciamento, importante já que ele é utilizado para prever a matriz que falamos anteriormente. O IPA é também um mecanismo complexo que acreditamos não valer a pena ser explicitado aqui, mas que em resumo, serve como uma ferramenta de data augmentation por desconsiderar informações sobre a rotação e translação locais quando passados para uma escala global na proteína. Em outras palavras, se o modelo sabe que qualquer rotação ou translação dos dados dão no mesmo resultado, ele precisará de muitos menos dados para retirá-los de modelos errados e poderá aprender de maneira muito mais eficiente. E finalmente, uma última rede rasa é adicionada no último bloco para predizer o ângulo de torção para todos os átomos que não estão na cadeia principal. Obtendo a coordenada final de todos os átomos na estrutura. Um último ponto importante é saber quais critérios que foram utilizados para treinar esse gigante que é essa rede neural. Em resumo existem quatro funções de perda principais: É interessante trazer essas funções de perda para mostrar como cada parte individual do modelo possui uma função específica essencial, e como todas elas juntas são relevantes no final da predição. O AlphaFold realmente é um marco gigantesco tanto para pesquisadores na área de IA quanto na biologia computacional e bioinformática. Seu desenvolvimento e possibilidade de utilização (bem, pelo menos se seu computador conseguir rodar) abre várias portas para os mais diversos tipos de descobertas na área. Lembre-se de nos acompanhar em nossas redes sociais! No Facebook, Linkedin, Instagram e, claro, nossos posts do Medium!"
https://medium.com/turing-talks/conceitos-de-lingu%C3%ADstica-para-processamento-de-linguagem-natural-78241ca2608c?source=collection_home---------18----------------------------,Conceitos de linguística para Processamento de Linguagem Natural,Você sabe como os conceitos de linguística se encaixam em NLP?,Daniel Frozi Brasil da Fonseca,911,11,2021-09-05,"Texto escrito por Daniel Frozi, Isabella Camareli, Renato Costa, Rian Fernandes e Vitoria Rodrigues. Olá, seja bem-vindo a mais um Turing Talks! Aposto que, se pensarmos em Sintaxe e Morfologia, você teria uma lembrança dos tempos da escola, não? Agora o que você talvez não saiba é que esses e outros níveis de análise linguística são importantes para estudarmos IA também. O nosso texto de hoje trata justamente desses conceitos de Linguística para Processamento de Linguagem Natural. Para falarmos de linguagem, devemos ter em mente que o assunto desperta o interesse da humanidade há muito tempo: datam do século IV a.C seus primeiros estudos, feitos por hindus devido às suas motivações religiosas. Com o passar dos anos, outros povos também foram fundamentais para a construção do conhecimento linguístico atual, como latinos e gregos. No entanto, é só no século XIX que as línguas vivas passam a ser analisadas como objeto de estudo. É nesse período que ocorre, para os estudiosos, a distinção entre língua literária e língua falada, já que a fala não possuía prestígio como objeto de estudo anteriormente. Vale ressaltar que, para a Linguística moderna, a escrita é importante, mas um dos princípios para os estudos é justamente a língua falada. Com isso em vista, concluímos que, até o início do século XX, o objetivo da Linguística era apenas descrever o desenvolvimento histórico das línguas. No entanto, tal situação mudou em 1916, com a publicação do livro “Curso de Linguística Geral”, de Ferdinand de Saussure, pai da Linguística moderna e fundador da corrente estruturalista. O livro foi organizado por dois alunos de Saussure, que integraram anotações de aulas do professor da Universidade de Genebra. É a partir das rupturas propostas neste período que a Linguística torna-se autônoma e passa a ser reconhecida como estudo científico baseado, primeiramente, na observação dos fatos da linguagem e a posterior definição de hipóteses. Com as alterações no estudo da linguagem e sua posterior categorização como estudo científico, é importante definirmos o que é essa disciplina. Diante disso, podemos dizer que, para Saussure, a Linguística é a ciência que possui como objeto de estudo a língua em todas as suas variações e níveis de análises estruturais. Para isso, devemos diferenciar os conceitos de linguagem e língua. Para Saussure, a linguagem é composta por duas partes: Agora que você já sabe o que é Linguística, deve ter em mente também que os estudos linguísticos ocorrem de forma descritiva, e não prescritiva, já que analisam a língua em sua observação, haja visto o que foi dito acima: um dos principais motores do estudo da Linguística moderna é a língua falada. No entanto, a língua é um objeto de estudo muito amplo e que, consequentemente, pode ser analisado por diversos pontos de vista e recortes. Devido a isso, para que esses estudos ocorram, a Linguística possui diferentes níveis de análise, os quais podem ser vistos na imagem a seguir: Um dos níveis de análise linguística mais relevantes para NLP é a morfologia. Apenas pelo nome (do grego “morphe” e “logos” que significam “forma” e “estudo”, respectivamente) já dá para perceber que essa é a área da linguística que estuda a forma e, consequentemente, a formação das palavras em si, visando classificar e estudar essas formas. Existem diversos processos que podem alterar uma palavra, por exemplo: Por isso, é legal pensar que as palavras mudam por causa de um pequeno segmento que atribui alguma diferença de significado a ela. Esse “pedacinho” é chamado de MORFEMA, e ele é o responsável por indicar gênero, número, tempo, modo, pessoa e classe gramatical de uma palavra. Vamos pensar na palavra “agradar”, por exemplo. Esse verbo na sua forma infinitiva pode variar de muitas maneiras para se adequar à conjugação que desejarmos, como “agradou” ou “agradamos”. Além disso, continuando com o segmento “agrad”, é possível formar adjetivos, “agradável” ou “agradecido”. Podemos formar um substantivo, como “agrado”, e até mesmo um advérbio, “agradavelmente”. Percebam a maneira com que uma mesma palavra pode ser alterada de diversas maneiras apenas ao acrescentarmos morfemas diferentes. São processos como esses que a morfologia estuda dentro da linguística! Certo, mas onde podemos aplicar conhecimentos de Morfologia em Python (ou em programação no geral)? Uma tarefa que utiliza esses conceitos é quando lematizamos ou stematizamos um texto. Caso você não se lembre o que é isso, vamos rever rapidamente: Quando passamos textos para modelos básicos de Machine Learning, precisamos realizar alguns métodos de pré-processamento, ou seja, precisamos “alterar” algumas coisas nele para que ele fique compreensível para a máquina. Uma dessas mudanças é a lematização e a stematização, que nada mais é do que pegar as palavras conjugadas e reduzi-las a sua forma normal, como por exemplo retornar os verbos conjugados à sua raiz — Isso porque quando usamos esses modelos simples a máquina pode interpretar palavras iguais, mas conjugadas de forma diferente, como palavras distintas, por exemplo andar ≠ andando. A diferença entre os dois processos é que ao lematizar convertemos as palavras exatamente à sua raiz, e ao stematizar apenas tiramos o radical, o que pode acabar gerando palavras que não existem. Para fazermos isso, podemos usar os lematizadores e os stematizadores da SpaCy ou da NLTK, que são os mais usados. Para ver mais detalhadamente essa aplicação, veja nossos textos de Introdução a NLP e de Ferramentas para NLP em português. Enquanto a morfologia trabalha com a estrutura das palavras isoladamente, a sintaxe vai estudar a estrutura da sentença, ou seja, a relação lógica entre as palavras na distribuição de uma frase. Essa área investiga como as palavras são organizadas para formar frases em uma língua natural. Essa organização entre palavras numa frase é controlada por regras e princípios básicos de ordenação e concordância. Vamos tratar um exemplo concreto para ficar mais claro. Uma frase simples, como “Maria comprou um computador novo ontem” pode ser reorganizada de várias maneiras: Porém, a frase “Ontem computador um Maria comprou” não faz sentido para nós, falantes de português. Assim, o principal objetivo da pesquisa em Sintaxe é desvendar as regras e princípios que controlam a formação de frases nas diferentes línguas humanas, determinando as possibilidades de associação das palavras. Em NLP, podemos aplicar conceitos de Sintaxe quando precisamos realizar tarefas que exigem tratamentos como Parsing e Tagging. Vamos ver o que são cada uma dessas coisas: Para ambas essas tarefas podemos utilizar a biblioteca SpaCy, que possui todas as ferramentas para extrair esses recursos linguísticos do texto. Para isso, dê uma olhada na parte de “Linguistic features” da documentação dela. O significado é a parte mais importante de qualquer diálogo, e é justamente a semântica que o estuda. Entender, ou melhor, analisar como o significado se forma em frases ou em outros modos de comunicação é uma tarefa que envolve primeiramente entender a relação presente entre significado e significante. Toda palavra é essencialmente um signo, que é formada por esses dois elementos: significado e significante. Significante é a imagem acústica que é atribuída a um significado. Imagine que um gringo com zero conhecimento da língua portuguesa chegasse no Brasil e escutasse a palavra “garrafa”, sua primeira reação seria analisar o som, e não o sentido, porque para ele “garrafa” não representa nenhum conceito, não tem nenhum significado, é apenas um som, uma imagem acústica. Já um brasileiro, ao escutar a palavra “garrafa”, pensa concretamente em uma forma, em um conceito: ele consegue identificar que o objeto existe, logo compreende o significado transmitido pelo significante. A importância de entender superficialmente essa relação é identificar melhor o que significado representa no mundo linguístico: mais que se referir a uma “coisa” ou ao puro “sentido”, significado é um conceito de ordem semântica. E é a partir dessa visão sobre significado que a semântica opera. Estudar o significado de uma frase por exemplo é analisar conteúdo e contexto, assim como as relações presentes na frase de cada significado e significante. Ou seja, semântica é o estudo do significado usado por seres humanos para se expressar através da linguagem. As aplicações de Semântica em NLP são principalmente duas: A Pragmática é a ciência do uso linguístico, ou seja, ela estuda as condições que envolvem e ditam o modo como utilizamos a linguagem. Toda frase, seja ela escrita ou falada, carrega consigo um significado, assim como visto anteriormente em semântica, mas não necessariamente apenas por ser linguagem que tudo sempre vai ter total sentido. Um dos campos da atuação da Pragmática no qual podemos claramente observar esse fenômeno é na enunciação, na produção de enunciados, que representam realizações linguísticas concretas. Novamente vamos imaginar: é um dia normal e andando na rua você escuta alguém dizer no telefone “Então amanhã nós vamos lá, naquele horário mesmo…isso… sim, boa e tu vai levar aquele então né? Ok, fechado.”. A princípio você parece entender o conceito básico de todas as palavras, mas não compreende totalmente o que vai ocorrer, existe um sentido geral mas essencialmente você não entendeu o verdadeiro significado dessa fala, porque ao fim não conhece as falas do outro enunciante e nem as condições presentes naquela comunicação, como: onde é o “lá”, quem está em “nós”, quando seria “naquele” horário e que “aquele” vai ser levado. Todos esses elementos linguísticos que indicam o lugar, tempo e quem são os participantes da produção do enunciado são conhecidos como dêiticos, ou seja, dêiticos são pronomes pessoais (como eu/tu), marcadores de espaço, advérbios de lugar, pronomes demonstrativos (como por exemplo, aqui, lá, esse, este) e marcadores de tempo (como, agora, hoje, ontem). Todos esses elementos, ou melhor dizendo os dêiticos, só podem ser perfeitamente entendidos dentro da situação de comunicação, voltando ao exemplo da ligação no telefone, você que estava fora da conversa não entendeu os sentidos dos dêiticos que foram utilizados no diálogo, já o homem que falava na conversa os entendia porque ele fazia parte daquela situação enunciativa. O mesmo fenômeno ocorre para textos escritos: imagine que você encontra na rua um bilhete com o seguinte enunciado “Hoje tenho que comprar banana, leite e carne lá no mercado”, novamente você consegue entender parcialmente sobre o que se trata a mensagem, mas não compreende os dêiticos, não sabe se “Hoje” se trata realmente do dia presente ou do dia anterior, também não sabe a qual mercado o autor se refere e nem quem é o autor do bilhete. Dessa forma, para que exista a compreensão total de um texto escrito, também é necessário explicitar a sua situação enunciativa na escrita: quando é “Hoje”, em qual mercado ele foi comprar e quem é o “eu” que fez a lista. Em suma, a pragmática é a ciência do uso linguístico porque estuda os fenômenos que atribuem o sentido e significado da frase no discurso, provocando situações que geram a interpretação que ocorre além da linguagem escrita e falada, as particularidades da enunciação e de muitos outros meios de comunicação e as condições linguísticas que envolvem a sua utilização. Por fim, essa área pode ser encontrada em NLP nas seguintes aplicações: Todas as tarefas acima podem ser feitas utilizando o BERT, um dos mais poderosos modelos de língua da atualidade para tarefas de NLP e Deep Learning, para conhecer mais dê uma olhada na documentação da biblioteca. Vamos fazer uma revisão e uma conclusão do que vimos neste texto? Veja se você consegue lembrar do aqui será revisto e, caso também tenha entendido o que aqui resumimos, pode considerar que você bem compreendeu sobre certos conceitos de Linguística para NLP! Bem, inicialmente verificamos que a língua viva e falada começou a ser estudada a partir do século XIX, ampliando tal situação com os preceitos de Saussure. Ainda, definimos Linguística e marcamos a diferença entre linguagem e língua, além de pontuarmos o comportamento descritivista dos estudos linguísticos, lembra? Assim percebemos a língua como um objeto muito amplo e apresentamos certos recortes e níveis possíveis de estudar sobre isso: divisões importantes também para o tema de NLP. Dessa forma, passamos, primeiramente, por Morfologia, ao comentar sobre a existência dos morfemas e o estudo da forma da palavra: informações importantes e interligadas com processos de redução de programação, né? Como o explicado sobre lematização e stematização. Depois, falando um pouquinho de Sintaxe, relembramos que esse recorte do estudo trabalha sobre a palavra dentro de uma sentença, pensando então sobre a questão da ordem dos termos e concordância de frases. Daí, vimos a relevância da Sintaxe ao ser aplicada em tratamentos como Parsing e Tagging. Seguindo — explicando Semântica — apontamos que o significado é a parte mais importante de qualquer diálogo e, assim, chegamos aos conceitos de signo, significado e significante: percebendo que as principais aplicações de Semântica estão na tarefa de Named Entity Recognition (NER) e na tarefa de Extração de relações. Por fim, citamos sobre a Pragmática: sintetizada como o estudo das condições envolvidas com o modo que utilizamos a linguagem e, por isso, definimos sobre enunciação, lembra? Que pensa na produção de enunciados e faz com que a gente perceba que dêiticos — outro termo explicado neste texto — só são perfeitamente entendidos na situação comunicacional e nos fenômenos importantes para o contexto. Finalmente, mostramos as aplicações de tal área de estudo nas tarefas de Intenção, Modelos de Q&A (Question and Answering) e Summarization. É isso! Esperamos que tenha entendido e que tenhamos te ajudado a entender um pouco mais o quanto conceitos de Linguística podem ser relevantes para estudos de NLP! Ah, não se esqueça de acompanhar o Turing USP no Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Entre também em nosso servidor no Discord :) Agradecimentos à Julia Pocciotti."
https://medium.com/turing-talks/%C3%A1rvore-linear-generalizada-uma-extens%C3%A3o-da-%C3%A1rvore-de-regress%C3%A3o-que-voc%C3%AA-deve-conhecer-53f9658f828c?source=collection_home---------17----------------------------,Árvore linear generalizada: uma extensão da árvore de regressão que você deve conhecer.,,Alberto Rodrigues,860,8,2021-09-12,"Olá, tudo bem com você? No Turing Talks de hoje vamos falar sobre um modelo de aprendizagem supervisionada bem interessante que com certeza você vai gostar! Mas antes disso, talvez seja interessante que você conheça um pouco sobre árvore de regressão e modelos lineares generalizados, porém não se preocupe se você não conhece, este Turing Talks fala sobre tudo o que precisa saber para entender bem o modelo. Então, vamos lá? Para entendermos um pouco melhor como o modelo de Árvore treina nossos dados temos que conhecer umas das mais famosas classes de modelos da estatística chamada de Modelos Lineares Generalizados. Exemplos particulares dos modelos lineares generalizados são a regressão linear múltipla e regressão logística, apresentados em Turing Talks anteriores. Então, se você conhece esses modelos, provavelmente sabe um pouco sobre os modelos lineares generalizados. Os modelos lineares generalizados, abreviados por simplesmente MLGs são uma classe de modelos da estatística que possibilita maiores escolhas de distribuições de probabilidade para o target ou variável resposta(y), ao invés de usar sempre a distribuição normal para o target(y) para estimar os coeficientes de regressão linear múltipla usando máxima verossimilhança. No entanto, nem sempre a distribuição normal é adequada para a variável target e isso pode acontecer por diversos motivos, desde de a variável target não ser aproximadamente simétrica em torno da média até a natureza da variável ser muito restrita, por exemplo ser sempre no intervalo(0,1). Logo, os MLGs entram na jogada para nos ajudar com esse problema, pois esta classe de modelos possui mais opções de distribuições de probabilidade para a variável target, alguns exemplos são: Normal, Gama, Normal Inversa, Binomial Negativa, Binomial, Poisson e Beta. Se você não conhece essas distribuições não se preocupe, apenas estamos dizendo que para algum conjunto de dados particular uma distribuição de probabilidade específica para a variável target pode se ajustar melhor e consequentemente teremos melhores predições para o nosso modelo. A ideia que motivou o desenvolvimento da Árvore Linear Generalizada é a mesma ideia de uma árvore de decisão/regressão comum: dividir o conjunto de dados inteiros em subconjuntos menores a fim de melhorar a previsão através de médias ou medianas, certo? Por exemplo, imagine que estamos querendo prever salário de uma pessoa com base na idade e anos de estudo. Então, poderíamos ter a seguinte árvore de regressão: No entanto, a predição é simplesmente fornecida por uma média ou mediana, que pode não ser adequada em algumas situações. Isso acontece porque para um dado subconjunto de uma folha a previsão fornecida é sempre a mesma. Logo, podemos pensar na mesma estrutura de uma árvore, porém ao invés prever o mesmo valor para o mesmo subconjunto a predição será fornecida por uma equação de regressão, logo a predição será diferente para as observações de um mesmo subconjunto da árvore. Logo, a pergunta que podemos fazer é: como obtemos essa equação de regressão? Uma possível escolha é sempre usar uma regressão linear múltipla em cada subconjunto, que é um modelo linear generalizado com distribuição normal e função de ligação identidade! Por exemplo, poderíamos ter uma árvore como na figura abaixo: Pois bem, apesar do modelo mencionado acima ser extremamente mais poderoso em termos de poder preditivo do que uma árvore comum, ele tem uma grande desvantagem: usa sempre o mesmo modelo e consequentemente a mesma estrutura para a predição de uma observação, uma equação de estimação linear e sempre será estimada com base na distribuição Normal usando o estimador de máxima verossimilhança. Na prática, isso pode acontecer, porém é muito difícil com grande maioria dos conjuntos de dados por conta da complexidade e quantidade de observações. Logo, a ideia de uma Árvore Linear Generalizada é ao invés de usar sempre a distribuição Normal com função de ligação identidade para a variável target são testadas várias distribuições com funções de ligação diferentes e verificar qual melhor se adapta a um subconjunto particular da árvore. Em particular, o modelo desenvolvido testa somente para variáveis target contínuas positivas, então as distribuições mais comuns utilizadas no MLGs são Normal, Gama e Normal Inversa juntamente as funções de ligação identidade, logarítmica e inversa, totalizando 9 modelos diferentes. Para entender como os modelos de subconjuntos são escolhidos precisamos entender como as divisões da árvore são escolhidas. Primeiro vamos conhecer um pouco melhor de como as divisões são realizadas e quais os motivos por trás da ideia da separação. A ideia da separação é determinar subconjuntos homogêneos de acordo com a variabilidade da variável target. Isso pode ser visto também com o objetivo de melhorar a predição de cada modelo particular de um subconjunto. Mais especificamente os subconjuntos são gerados através da seguinte fórmula: O objetivo de uma boa divisão é minimizar o desvio padrão da variável target de cada subconjunto levando em consideração as quantidades de observações. Logo, queremos maximizar essa expressão para obtermos subconjuntos com desvio padrão da variável target menor que o desvio padrão do target do conjunto inteiro e consequentemente melhorar a estimação dos modelos de cada subconjunto. Para cada divisão são testados diversos valores das features e o valor da feature que maximiza a expressão é escolhido e a divisão é realizada. Este processo é realizado até que pelo menos hiperparâmetro de parada seja acionado, como o número mínimo de observações para o subconjunto ou a profundidade máxima da árvore, hiperparâmetros clássicos de uma árvore de regressão. Com a árvore construída, agora é hora de escolher qual o melhor MLG para predizer nossa variável target para um determinado subconjunto. Para cada subconjunto, alguns MLGs são treinados e em seguida é verificado qual o modelo que obteve o menor erro de treinamento, por exemplo o erro quadrático médio ou o erro médio absoluto, de tal forma que esse modelo será escolhido para cada subconjunto. Esse processo é realizado recursivamente até os subconjuntos finais das folhas que serão responsáveis por predizer as observações. Uma importante observação é que não são todos os MLGs que são testados com o crescimento da árvore e sim somente alguns que obtiverem os melhores resultados fornecidos pelo subconjuntos anteriores, isso é realizado para evitar custo computacional e tornar a aprendizagem mais rápida. Após a escolha dos modelos para cada subconjunto da árvore é realizado o processo de poda, que é o principal componente do modelo que evita o overfitting. Isso é necessário porque pode acontecer que uma árvore ajuste um subconjunto muito específico e não entenda o padrão dos dados, assim ocasionalmente overfitting. Existem dois principais tipos de poda, nas duas podas um processo é realizado de baixo para cima da árvore para verificar se realmente a divisão realizada para as folhas sofreu de overfitting ou não. Na primeira, o próprio erro de treinamento é utilizado para verificar se as predições realizadas nos nós folhas sofreram overfitting. São calculados os erros de treinamento para os nós folhas e também para o nó pai(subconjunto acima dos nós folha). Com os três erros calculados , eles são comparados e é verificado se o erro de treinamento do nó pai é menor que uma ponderação dos erros de treinamentos dos nós folhas, se isso acontecer a divisão é “podada”, ou seja ela deixa de existir e o novo nó folha será designado para o nó pai. Esse processo é realizado recursivamente até o topo da árvore. Uma importante observação é que não é exatamente o erro de treinamento utilizado, mas sim o erro de treinamento com uma correção, isso é feito porque geralmente o erro de treinamento é subestimado, portanto essa correção tenta resolver esse problema. No segundo tipo de poda, é feito quase o mesmo processo explicado acima, porém com a diferença de que ao invés de usar o erro de treinamento é usado o erro do conjunto de poda, que é um conjunto de validação específico para a Árvore, que não é usado no processo de treinamento antes da poda. Com isso, terminamos o processo de aprendizagem e temos nossa Árvore Linear Generalizada, para fazer as predições para as observações que quisermos! Um exemplo é dado abaixo: Nesse caso, temos três subconjuntos e para cada um temos um MLG específico. A primeira equação(da esquerda para a direita) é uma regressão linear simples, que é um MLG com distribuição normal e função de ligação identidade para as observações que são menores ou iguais a 1,55, a segunda equação dada por um MLG com distribuição gama e função de ligação inversa quadrática para observações menores ou iguais que 1,95 e maiores que 1,55 e a terceira equação é fornecida por um MLG com distribuição normal inversa e função de ligação logarítmica para as observações maiores que 1,95. As equações de predição são fornecidas através da função inversa da função de ligação, mas não se preocupe, isso é um componente específico do MLG. As distribuições de probabilidade para cada subconjunto apesar de não serem visualizadas na árvore podem ser verificadas no código desenvolvido de todo o treinamento na linguagem de programação R. Com o treinamento realizado, agora é a hora de predizer observações que não foram vistas pelo modelo. Para este objetivo podemos simplesmente verificar em qual subconjunto final a observação a ser predita pertence com base na equação de predição do MLG escolhido para determinado subconjunto. Por exemplo, imagine que temos a seguinte Árvore Linear Generalizada já treinada com nossos dados e queremos fazer algumas predições. Com nossa feature denotada por x1. Podemos estar querendo fazer as predições para alguns valores de feature como por exemplo: 2.4, 1.75, 3.31 e 1.55. Logo basta olharmos em qual subconjunto estas observações pertencem e realizar a predição. As predições são dadas na tabela abaixo. Podemos notar que para cada subconjunto temos a predição baseada no MLG específico. Com isso, chegamos a mais um final do Turing Talks. Se você tiver interesse em aprender mais sobre os detalhes do modelo apresentado, veja o artigo original e diz pra gente o que achou :) Lembre-se de nos acompanhar em nossas redes sociais! No Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Entre também em nosso servidor no Discord :)"
https://medium.com/turing-talks/como-o-random-forest-nos-ajuda-a-entender-os-mist%C3%A9rios-do-universo-literalmente-3ecce1256af7?source=collection_home---------16----------------------------,Como o Random Forest nos ajuda a entender os mistérios do Universo? (literalmente!),Neste artigo mostraremos um exemplo de aplicação de Random Forest na área de Astronomia.,Lilianne,904,4,2021-09-26,"Na astronomia existe um problema clássico que é a classificação de objetos celestes. Esse assunto é extensivamente estudado ainda hoje devido à importância que uma boa classificação tem em qualquer e toda subárea na astronomia. Ter um vasto catálogo de objetos bem classificados pode nos ajudar a responder as seguintes perguntas em aberto (além de muitas outras!): E para entender como é feita a classificação, precisamos saber que existem duas formas de medir grandezas físicas de um objeto astronômico: Por conta do tempo necessário para se obter espectros, grandes mapeamentos do céu em espectroscopia são economicamente inviáveis. Por outro lado, dados fotométricos nos trazem menos informação física sobre os objetos, dificultando o processo de classificação. Um projeto chamado S-PLUS liderado por cientistas brasileiros visa imagear todo o céu austral em 12 diferentes bandas. Essas bandas permitem a obtenção do que chamamos de espectro fotométrico de baixa resolução. Se em um espectro temos milhares de pontos no gráfico, o espectro de baixa resolução do S-PLUS terá apenas doze. A figura abaixo mostra os espectros de alta resolução e os espectros fotométricos de baixa resolução do S-PLUS de diferentes objetos: quasar, galáxia, estrela em estágio intermediário de evolução e uma estrela em estágio final de evolução. Em geral, identificamos a classe do objeto a partir dos padrões de picos e declives nos espectros de alta resolução. Por exemplo, os quasares são identificados pelos picos alargados. A pergunta é: como treinar uma máquina que seja capaz de classificar rapidamente as centenas de milhões de objetos que serão observados pelo projeto S-PLUS utilizando apenas o espectro fotométrico de baixa resolução? L. Nakazono et al. 2021 tentaram responder a esta pergunta no artigo recentemente publicado no jornal Monthly Notices of the Royal Astronomical Society utilizando o algoritmo Random Forest. Lembra que eu disse que a forma mais confiável de classificar objetos é por espectroscopia? Pois bem, os autores utilizaram catálogos de quasares, estrelas e galáxias que foram classificados por espectroscopia para treinar, validar e testar os modelos. As features utilizadas para a construção do melhor modelo foram as 12 medidas fotométricas já mencionadas anteriormente e 2 medidas na região do infravermelho. Além delas, foram consideradas 4 outras medidas que definem a geometria (ou morfologia) aparente do objeto. Afinal, algumas galáxias são bem óbvias de se classificar pelo seu tamanho. Mas não se engane, pois nem toda galáxia é bem definida. O modelo final apresentou um macro averaged F-score de 97% na amostra teste. Foi usada esta métrica pois ela é menos sensível a dados não balanceados. O catálogo de estrelas, quasares e galáxias servirá de base para todos os projetos científicos que utilizarão os dados do S-PLUS, mostrando a importância do Random Forest na ciência. Por exemplo, a partir dessa classificação, C. R. Bom et al. 2021 conseguiram treinar um modelo de deep learning para classificar a morfologia das galáxias. E para quem tiver interesse em se aventurar com dados astronômicos, os dados do S-PLUS são públicos e os autores disponibilizaram todo o código no github. A classificação também pode ser facilmente obtida utilizando o pacote splusdata em Python. Para mais informações sobre o projeto: Lembre-se de nos acompanhar em nossas redes sociais! No Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Entre também em nosso servidor no Discord :)"
https://medium.com/turing-talks/impacto-da-intelig%C3%AAncia-artificial-na-sociedade-atual-bf119ad42577?source=collection_home---------15----------------------------,Impacto da Inteligência Artificial na sociedade atual,"Neste artigo veremos as suas aplicações e consequências não só no ambiente virtual, mas também real.",Iryna Mireia,513,6,2021-10-03,"Sabe aquela sensação de escutar uma nova música e pensar: “É exatamente isso que eu estava procurando!”? É raro encontrar um som que desperte essa impressão, principalmente se procurarmos manualmente por mecanismos de buscas na internet. Atualmente, em plataformas digitais, como o Spotify, existem recomendações de músicas que se baseiam no consumo do usuário, ou seja, se ele gosta de rap russo, indie australiano e synthwave, o algoritmo reconhecerá, aprenderá seu estilo musical e recomendará músicas desses mesmos gêneros — ou semelhantes — de outros artistas. Isso significa que as chances de encontrar uma nova banda favorita é maior, o que faz a experiência desse usuário na plataforma ser mais satisfatória. O exemplo anterior serviu para mostrar como a Inteligência Artificial está em nossas vidas de forma a facilitar o uso de muitas ferramentas que mexemos diariamente. Muito além dos trabalhos repetitivos e manuais, o foco é desenvolver máquinas que desempenham o raciocínio humano. A partir do ano de 1950 houve um avanço significativo na área. Foi com o Alan Turing, matemático britânico, que ocorreu uma visão completa da IA apresentando, em um artigo, o Teste de Turing, que mostra se uma máquina consegue “pensar” ou não, ou seja, o computador passará no teste se um interrogador humano, depois de propor algumas perguntas por escrito, não conseguir descobrir se as respostas vêm de uma pessoa real ou da Inteligência Artificial. Com isso, a pergunta que fica é: Existe IA capaz de desenvolver uma diálogo casual ou jogar algum jogo que dispute com um humano? Como ela está inserida atualmente? É o que veremos a seguir. Para exemplificar como a Inteligência Artificial está presente em nossas vidas, será necessário separá-la em três grandes áreas, as quais falarei com mais detalhes: IA Focada, IA Generalizada e IA Superinteligente. Na IA Focada, o algoritmo é desenvolvido para resolver problemas específicos de uma área, armazenando grande volume de dados e realizando tarefas complexas, como cálculos estritamente precisos, sempre focalizando no objetivo final para qual foi projetado. No fim, programas desse tipo simulam a inteligência, sem tentar aprender com o ambiente em volta. Os assistentes virtuais inteligentes como a Siri, da Apple, e Alexa, da Amazon, podem ser classificadas dentro dessa área, permitindo que as pessoas interajam com suas vozes e/ou textos e, assim, obterem respostas para as suas questões. Aqui no Turing já desenvolvemos um chatbot, batizado de ADA, que exerce a função de responder mensagens frequentes em nosso Facebook. Você consegue encontrar mais detalhes desse projeto no nosso repositório do GitHub. IA Generalizada A IA Generalizada, também conhecida como IA Forte, é capaz de realizar tarefas tipicamente humanas e é a que mais se iguala conosco ao realizar uma atividade intelectual. Por ter esse objetivo, torna-se mais difícil de ser desenvolvida e, em geral, os algoritmos usam técnicas de Aprendizado de Máquinas como uma ferramenta, além de compreender e reagir a diferentes tipos de estímulos. A Visão Computacional, por exemplo, é uma área dentro da IA Generalizada, a qual investiga maneiras de fazer uma máquina interpretar imagens do mundo real. Seu objetivo é a construção de programas que melhorem o desempenho por meio de exemplos. Para isso, é necessário um grande volume de dados, que guiarão o algoritmo. A recomendação de músicas e/ou anúncios feitos em sites e redes sociais se baseiam nesse aprendizado, o que pode gerar satisfação ao usuário, mas, ao mesmo tempo, colocá-lo em uma bolha social, que pode levá-lo a radicalização de ideias e opiniões. Ainda dentro do Aprendizado por Máquina, há três tipos de separação: Supervisionado, que aprende a partir de resultados pré-definidos, como a classificação de imagens, muito usada em sistemas de segurança e filtros do Instagram; Não Supervisionado, que ocorre quando exemplos são fornecidos sem rótulos e, com isso, o algoritmo tenta agrupá-los de alguma maneira, formando grupos. Quando precisamos determinar um mercado específico para um produto novo, o qual existe algumas informações, mas não o todo, precisando ter alguns direcionamentos a partir desses dados e agrupá-los, papel que o algoritmo irá desenvolver; por último, o aprendizado por Reforço, que aprende diretamente na interação com o ambiente, fornecendo uma resposta para cada ação tomada. Um exemplo disso é uma máquina capaz de vencer jogos de xadrez, desenvolvida pela IBM que, inclusive, venceu o Grande Mestre Garry Kasparov, campeão mundial xadrez de 1985 a 2000. Caso deseje saber mais sobre Aprendizado por Máquinas, já temos um artigo publicado aqui no medium. IA Superinteligente Por último, a IA Superinteligente corresponde a algoritmos que são capazes de superar a inteligência humana em todas as áreas, porém, ainda não foi desenvolvida nenhuma máquina capaz de tal ato, estando (ainda) em fase de estudos e na área ficcional. Na ficção, inclusive, é um tema bastante abordado, como no filme A.I. — Inteligência Artificial (2001) e O Exterminador do Futuro (1984). É nesta parte que nos perguntamos: será possível um ser humano desenvolver uma inteligência que o supere? Por enquanto — mas não por muito tempo — esse questionamento ainda fica na área do debate. Além de todos os exemplos mencionados aqui, a IA está presente em muito mais áreas da vida humana, como na medicina, no direito, na arte, entre tantos outros. Inclusive, muda diariamente a vida das pessoas, tendo vários estudos acadêmicos enfatizando efeitos sociais e éticos a respeito, destacando não só os benefícios, mas também os prejuízos que a área pode causar. Em geral, a Inteligência Artificial veio para facilitar nossas vidas e até salvá-la. Existe uma larga aplicação de máquinas no campo da saúde, a qual algoritmos identificam riscos em pacientes e ajudam a formular tratamentos personalizados. Porém, como nem tudo são flores, há também seus riscos, como por exemplo, perda de vagas de trabalho, agravando a desigualdade social existente não só no Brasil, como no mundo, principalmente de pessoas com baixa escolaridade. Além da área social, também há uma implicação moral e ética, que focam em questões como a privacidade dos usuários e transparência na utilização dos seus dados. Nestes últimos pontos, por exemplo, recentemente ocorreu o vazamento de dados contidos no Serasa de mais de 200 milhões de brasileiros, com informações como o CPF e renda, infringido o direito à privacidade. Outro problema já citado aqui é sobre a formação de bolhas nas redes sociais que usamos e como empresas podem se aproveitar disso para radicalizar as pessoas ainda mais e, assim, vender seus produtos/propósitos e lucrar em cima disso. O Facebook, junto com a empresa de marketing político Cambridge Analytica, foram alvos de investigações sobre a coleta de dados dos usuários, mesmo sem permissão deles, para influenciar nas eleições norte-americanas de 2016. Basicamente, anúncios que favoreciam apenas um dos candidatos eram mostrados para diferentes nichos de eleitores, com base nesses dados, a fim de convencê-los e radicalizá-los a favor de um político escolhido. Existem estudos para todos os problemas aqui apontados, sendo alguns já aplicados para a redução de danos, como um sistema de privacidade mais sofisticado, e na área social, projetos de lei que circulam todo o mundo com o foco de impor limites morais e éticos para as aplicações da IA na sociedade. Apesar da velocidade dos estudos para o desenvolvimento de algoritmos que pensem como nós serem mais rápidos do que debates e estudos para os desenvolverem de maneira saudável, passos estão sendo dados. Não há mais volta: a Inteligência Artificial é o que domina boa parte das nossas interações virtuais — e até pessoais — nas nossas vidas. Ela está por toda parte, mesmo que não as enxergamos ou prestamos atenção. Nosso comportamento é moldado pelas nossas redes sociais favoritas e sites que frequentamos, que utilizam da IA para que você permaneça neles, quase que o tempo inteiro. Cada vez mais as máquinas vão aprendendo como lidamos com o mundo em nossa volta e, cada vez mais, elas nos imitam e se autossustentam. Não é difícil imaginar que algum dia elas nos superem. Enquanto esse dia não chega, você pode conferir mais assuntos de IA aqui no nosso medium e acompanhar nossas redes sociais (Instagram, Facebook e Linkedin) para mais notícias e novidades a respeito. Entre também no nosso servidor no Discord! Melhor ficar bem-informado e preparado para esse dia. :)"
https://medium.com/turing-talks/aplica%C3%A7%C3%A3o-de-ia-em-mamografia-74d0bde2e73b?source=collection_home---------14----------------------------,Aplicação de IA em Mamografia,"No texto de hoje, abordaremos como é possível aplicar IA em imagens de mamografia para classificação de nódulos.",Felipe Augusto de Moraes Machado,592,6,2021-10-10,"Em outubro, há uma grande campanha nacional sobre a prevenção e o diagnóstico do câncer de mama, o Outubro Rosa. É o câncer mais frequente nas mulheres, atingindo milhões no mundo todo. Para entender melhor a gravidade da doença, o Grupo ARGO — Inovação em Saúde escreveu em suas redes um texto sobre o tema. Uma forma de auxiliar nessas tarefas é utilizando tecnologias, como Inteligência Artificial, e hoje mostraremos que é possível fazer um modelo simples mas que já apresenta um resultado razoável. O código utilizado na elaboração desse texto está disponível no Repositório do Turing! Para essa análise, será utilizado o CBIS-DDSM (Curated Breast Imaging Subset of DDSM) [1][2][3], um banco de mamografias disponíveis para pesquisa. Contém cerca de 1500 pacientes distintos, com presença ou de massa ou de calcificação em alguma região da mama, e sua classificação, sendo Benigno, Maligno ou Benigno Sem Retorno. A fim de tornar esse problema um de classificação binária, removemos os casos de calcificação e os casos de Benigno Sem Retorno, restando somente massas benignas (classe 0) e malignas (classe 1). Para cada caso, temos 3 imagens, a original, a segmentação e a região da segmentação. Abaixo, podemos visualizar um caso. Com os casos de interesse separados, precisamos pensar em como abordar essas imagens. Elas possuem tamanhos diferentes então não será possível utilizá-las da forma original. Além disso, queremos um modelo simples (como comentado na introdução). Dado esse cenário, há uma forma simples mas poderosa: extração de features. Serão extraídos 3 features diferentes: Todas as features serão extraídas somente da região da massa. Para a modelagem, será utilizado o algoritmo Gradient Boosting. As features de primeira ordem são simples de extrair, pois utilizam somente os valores dos pixels, sem analisar a vizinhança. O pacote numpy já possui funções necessárias para calcular todas as features. A primeira a ser calculada é a Energy, cuja formula está apresentada abaixo. As outras features calculadas são: média, máximo, mínimo, desvio padrão, Skewness e Kurtosis. Será que com essas já é possível treinar um bom modelo? Infelizmente, não. O AUC obtido no teste utilizando somente essas foi de 0.57, um valor baixo (lembrando que um modelo aleatório teria AUC igual a 0.5). Vamos então ver as próximas features! Nesse dataset, já existe uma coluna identificando o formato da massa. Há ao todo 18 formatos diferentes, e para a criação das features, foi utilizado one-hot-encoding. Será que agora já pode sair um bom modelo? Sim! O AUC, dessa vez, foi cerca de 0.77, um valor bem superior ao anterior e utilizando somente a informação do formato. Vamos ver, abaixo, o que o modelo está dando maior importância. É possível observar que o formato irregular é decisivo na classificação da massa. Podemos ver esse fato na tabela abaixo, onde é nítida a diferença entre as frequências de cada classe para esse formato. Enquanto as features de primeira ordem só analisam os valores de pixels, as features de segunda ordem analisam também a posição entre eles, possuindo informações da vizinhança. Essas relações podem ser calculadas por diversas técnicas: Gray Level Cooccurence Matrix (GLCM), Gray Level Run Length Matrix (GLRLM), Gray Level Size Zone Matrix (GLSZM), Neighbouring Gray Tone Difference Matrix (NGTDM), Gray Level Dependence Matrix (GLDM). Abordaremos somente o GLCM no texto de hoje. Como o próprio nome diz, o GLCM é uma matriz, cujos elementos buscam representar a distribuição espacial dos pixels. Ela precisa de duas informações: a direção e a distância (em pixels). O tamanho da matriz é NxN, onde que N é a quantidade total de níveis distintos que o pixel pode assumir. No arquivo DICOM (Digital Imaging and Communications in Medicine) que estamos utilizando, há ao todo 65536 níveis distintos. Com essa quantidade, é praticamente inviável manter uma matriz desse tamanho na memória, e ela teria a maior parte dos elementos igual a zero. Para corrigir esses problemas, costuma-se a dividir em discretizar em níveis menores. Abaixo, uma imagem exemplo com discretizações diferentes. Podemos observar que uma discretização com 32 níveis já é bem semelhante a original, visualmente. Com isso, nossa matriz GLCM terá um tamanho de 32x32. Mas como preencher os valores dela? Abaixo, uma imagem representando duas imagens e sua GLCM. A GLCM, como dito anteriormente, precisa de duas informações, a direção e a distância. O processo do preenchimento dessa matriz segue o seguinte passo: dado um pixel (P1), veja o pixel que está na direção e distância determinada (P2). Na GLCM, procure a linha com o nível de P1 e a coluna com o nível de P2 e adicione 1 nessa posição. Repita esse processo com todos os pixels. Na Figura 4, a direção escolhida foi direita e a distância foi de 1 pixel. Com a GLCM preenchida, é possível utilizar seus elementos para calcular diversas features, descritas aqui. Utilizando essas features, será que é possível criar um modelo bom? Infelizmente, dessa vez não. O AUC obtido com essas features foi na ordem de 0.59, melhor que de primeira ordem mas ainda sim baixo. A ultima modelagem feita no nosso código é juntando todas essas features que descrevemos ao longo do texto. Com isso, obtemos um AUC de 0.78, um valor considerável comparado à complexidade do problema e à simplicidade da nossa metodologia, que é conhecida na radiologia como Radiomics! Essa área consiste em extrair features de imagens médicas e desenvolver modelos de machine learning para predizer o desfecho clínico do paciente. No texto de hoje, aplicamos a metodologia de Radiomics para classificar nódulos em imagens de mamografia utilizando modelos simples de machine learning. Nosso resultado, embora modesto, mostra que essa técnica simples gera resultados interessantes. [1] Rebecca Sawyer Lee, Francisco Gimenez, Assaf Hoogi , Daniel Rubin (2016). Curated Breast Imaging Subset of DDSM [Dataset]. The Cancer Imaging Archive. DOI: https://doi.org/10.7937/K9/TCIA.2016.7O02S9CY [2] Rebecca Sawyer Lee, Francisco Gimenez, Assaf Hoogi, Kanae Kawai Miyake, Mia Gorovoy & Daniel L. Rubin. (2017) A curated mammography data set for use in computer-aided detection and diagnosis research. Scientific Data volume 4, Article number: 170177 DOI: https://doi.org/10.1038/sdata.2017.177 [3] Clark K, Vendt B, Smith K, Freymann J, Kirby J, Koppel P, Moore S, Phillips S, Maffitt D, Pringle M, Tarbox L, Prior F. The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository, Journal of Digital Imaging, Volume 26, Number 6, December, 2013, pp 1045–1057. DOI: https://doi.org/10.1007/s10278-013-9622-7 [4] Miranda Magalhaes Santos, J.M., Clemente Oliveira, B., Araujo-Filho, J.A.B. et al. State-of-the-art in radiomics of hepatocellular carcinoma: a review of basic principles, applications, and limitations. Abdom Radiol 45, 342–353 (2020). https://doi.org/10.1007/s00261-019-02299-3"
https://medium.com/turing-talks/introdu%C3%A7%C3%A3o-ao-lda-ab6338abef28?source=collection_home---------13----------------------------,Introdução ao LDA,,Murilo Krebsky Hobus,600,5,2021-11-16,"Olá, seja bem vindo a mais um Turing Talks! No TT de hoje vamos apresentar uma introdução ao LDA — Linear Discriminant Analysis — que nada mais é do que uma alternativa,(quase que um “irmão” do PCA (se você ainda não sabe o que é PCA, dá um pulo nesse outro Turing Talks em que introduzimos o PCA), e algumas de suas aplicações. Vamos dar uma ideia de como chegamos ao LDA, como ele funciona e quais são as suas diferenças em relação ao PCA. Então sem mais delongas, bora começar ! Vamos começar entendendo em que situações LDA funciona: Suponha que tenhamos pontos no plano cartesiano pertencentes a duas classes, e que queremos agrupá-los. Vimos que uma alternativa para isso era o PCA, porém veja os pontos abaixo e suas respectivas projeções: Podemos ver que o PCA não fez lá grande trabalho. Existem alguns pontos sobrepostos, o que vai totalmente contra o nosso objetivo inicial. Para situações como essas, a alternativa imediata é o LDA. Apenas para comparação, veja como o LDA se sai com o mesmo conjunto de dados acima: Vamos entender então porquê o LDA funcionou. Durante esse TT, consideraremos o caso bi-dimensional, para a simplificação da explicação e dos exemplos. Pense num cenário ideal, onde temos clusters de dados claramente separados, como na figura abaixo: Esse é o mundo dos sonhos, e quando isso acontece, duas coisas acontecem por consequência: O que o LDA faz é tentar encontrar uma direção onde as projeções possuam grande diferença entre medias e pouca dispersão. O primeiro passo para encontrar essa direção é expressar a média e a variância da projeção dos dados na direção a ser encontrada, como podemos ver abaixo: Onde w é a direção a ser projetada, C1 e C2 são duas classes distintas de pontos, n1 e n2 são respectivamente a quantidade de pontos em cada classe. A média projetada é simplesmente a média dos dados projetadas na direção de w, já a dispersão dos dados projetados é o valor absoluto da soma das diferenças entre os pontos e a média. Note que a dispersão dos dados na verdade é uma matriz quadrada de mesma dimensão do espaço dos dados. Desenvolvendo a dispersão projetada para um classe que chamaremos de 1, temos a seguinte expressão: Tendo isso em mãos, podemos finalmente escrever o Discriminante Linear de Fisher, que é dado pelo seguinte quociente: Note que maximizar esta função, de fato corresponde às nossas expectativas iniciais (aumento da distância entre medias e diminuição da dispersão) Podemos agora, explicitar a dependência do Discriminante de Fisher com relação a direção w, analisando e reescrevendo seu denominador e numerador: As matrizes SB e SW são chamadas de Scatter Matrix Between Class e Scatter Matrix Within Class. Essas matrizes nos dão a informação de quanto as classes estão separadas e espalhadas, respectivamente. Reescrevendo o Discriminante utilizando essas novas notações temos então: Portanto, o objetivo final de todo o algoritmo é encontrar o w que maximize a função acima. Uma vez encontrada esta direção, basta projetarmos os dados no espaço gerado pela mesma. Não entraremos nos detalhes seguintes do funcionamento do LDA, que requerem uma base matemática bem maior, porém, ao final do post teremos links de referência para consultas e também um código mostrando passo a passo como implementar o algoritmo. A primeira comparação que nos vem à cabeça é o PCA. Em alguns casos, o objetivo não é encontrar o espaço com as variáveis de maior explicabilidade dos dados, mas sim o espaço que nos oferece uma maior separabilidade dos mesmos. O algoritmo de PCA não é feito pensando em separabilidade, por isso em alguns casos pode ser que ele não apresente um bom resultado, como podemos ver na comparação feita abaixo: Perceba que no caso do PCA a clusterização não foi eficaz pois os 3 conjuntos de dados apresentam muitas sobreposições, principalmente na projeção na primeira componente principal. Vale ainda ressaltar que diferentemente do PCA, o LDA é um algoritmo de aprendizado supervisionado. Além de ser uma alternativa ao PCA, o LDA também pode ser usado como ferramenta de pré-processamento de dados. Veja abaixo um modelo de Árvore de decisão com e sem o processamento através do LDA: Como o objetivo do algoritmo de Árvore de decisão é um particionamento do espaço afim de maximizar alguma medida de ganho de informação, quando os dados já estão separados (justamente o que o LDA faz) essa tarefa fica muito mais fácil, aumentando sua precisão. Como prometido, aqui vão alguns materiais para você se aprofundar nesse tema tão interessante que é LDA: Primeiramente aqui vai um agradecimento aos meus colegas de graduação Murilo Henrique Gomes e Giuliano Pantarotto Semente que me ajudaram no desenvolvimento deste material e também do seminário que está acima. Agora por fim, muito obrigado por ter acompanhado até o fim! Se o texto te ajudou não se esqueça de nos acompanhar em nossos outros meios de comunicação: Facebook, Instagram, Linkedin e Discord!"
https://medium.com/turing-talks/debugando-aplica%C3%A7%C3%B5es-em-python-24ea73ae4403?source=collection_home---------12----------------------------,Debugando aplicações em python,Dizendo adeus ao print(“Passou por aqui”),Guilherme Salustiano,454,6,2021-11-21,"Olá, gente! Sejam muito bem-vindos a mais um turing talks. Entradas de usuários inválidas, aquele edge case que você não pensou, aquela função que você achou que tratava aquele caso mas lançou uma exceção, existem várias possibilidades e coisas medonhas que podem dar errado durante a execução de uma aplicação. Nesse momento quem nunca saiu colocando print(“Passei por aqui”), às vezes naquele loop gigante que você passa um tempo rodando o terminal até achar algo estranho. O print será nosso eterno companheiro, entretanto com o desenvolvimento de um maior ferramental ficou cada vez mais fácil utilizar de outras alternativas, mais rápidas, escaláveis e manuteníveis. Trouxe um problema que tive que debugar na última semana, pois o código não funcionou de primeiro, era mais ou menos assim: Testes são códigos que testam outros códigos. Substituindo o trabalho braçal de a cada alteração ter que testar todos os componentes do sistema relacionados (e frequentemente esquecendo alguma coisa). Eles nos ajudam a garantir o funcionamento do sistema, simplificando o processo de debug e facilitando alterações futuras no código. Há dois (algumas pessoas consideram três) principais testes: testes unitários — feitos para testar pequenas funções, com vários casos, e testes de integração, criados para testar a composição entre as funções. Há diversas bibliotecas que fazem isso em python, por se tratar de um projeto pequeno e apenas testes unitários estou usando o pytest. Ao rodar no terminal temos: Testes passando, um sentimento de paz chega junto, e vamos seguindo. Batalhar contra letras na frente do computador o dia todo é uma constante luta, e nem sempre ganhamos a primeira batalha, vamos olhar para esse caso introduzindo um bug: Nesse caso esquecemos de usar o separador passado como argumento e estamos sempre usando o ‘.’, rodando agora o teste teremos: BOOOOM! Explosões vermelhas no terminal, mas pelo menos graças aos testes conseguimos ter uma noção de quais funções e em quais casos estão apresentando erros, e, com pouca investigação, conseguimos nos isolar de onde o erro surge. Sabendo disso, vamos adentrar ainda mais ao código: O Python Debugger é um módulo padrão do python que assim comoo debugger de outras linguagens permite a você rodar seu código interativamente, passo a passo, ou definir breakpoints, pontos que o debugger começa a rodar passo a passo. Podemos rodá-lo diretamente num arquivo com `python -m pdb <script>.py` ou a partir do shell do python, que será mais útil para debuggar a partir de uma função: Dentro do pdb, temos alguns comando que podemos usar para interagir com o código, os que eu mais uso são: Você também pode chamar o pdb direto do pytest com: Ou ainda usando a função `breakpoint()` adicionada no python 3.7 Felizmente hoje em dia há diversos editores e ferramentas construídas em cima do pdb que tornam a vida no desenvolvedor muito mais feliz. No caso do VSCode, para rodar um script em debug podemos ir em Executar -> Iniciar a depuração -> Python File. Entretanto, como a minha função não está diretamente chamada, ou precisaria adiciona-lá com >> diff main << Porém, conforme o número de testes vão crescendo, isso se torna trabalhoso, portanto o VSCode fornece melhores ferramentas para gerenciar os testes, basta ativa-las em `python.testing`, no meu caso bastou definir ”python.testing.pytestEnabled”: true. Nessa janela podemos visualizar e rodar todos os testes, ou de arquivos ou funções específicas. Encher essa lista com testes verdes é um orgulho, e, mesmo quando estão vermelho, podemos roda-los em debug para descobrir o problema Vamos começar adicionando um ponto de interrupção no teste, basta clicar no espaço antes do número em um círculo vermelho que vai aparecer: Então podemos finalmente clicar para debugar aquela função e veremos que o editor destaca aquela linha, a qual muda ligeiramente de aparência: que já, já eu entro em mais detalhes. O que importa para nós agora é na parte superior, visto os diferentes plays e setinhas, que são respectivamente _continue_, _next_, _step_, _out_, os mesmos do pdb. A principal diferença é que o _continue_ vai rodar até um novo brakpoint ou uma exceção não capturada, o _next_ vai para a próxima linha, passando por cima de chamada de funções, enquanto o _step_ entra dentro das funções. Clicando no _step_ ou teclando F11, Voilà, estamos agora dentro da função. Clicando 5 vezes no _next_ (F10) podemos observar que a janela de variáveis, do canto superior esquerdo, começa a mudar. Ela representa todos os valores que estão no escopo daquela linha que está sendo analisada e assim já podemos começar a notas certas estranhezas entre o que era esperado e o que acontece. Logo abaixo dela temos a janela de _valores assistidos_: que permite ver como expressões, usando as variáveis disponíveis, se comportam. Muito útil para aquela expressão booleana que vai ser crucial para entrar naquele if que não está executando Similarmente, podemos querer executar códigos python mais complexos, para isso, na parte inferior, junto ao terminal, temos o console de depuração. Ele é um shell do python, mas que tem acesso a todas as variáveis que estão no escopo da linha analisada, permitindo importar e chamar funções, fazendo alterações e testando diferentes chamadas Na esquerda, abaixo da janela de _valores assistidos_, temos a pilha de chamadas. Quando uma função é chamada, o código pula para outro arquivo e precisa saber para onde voltar, por isso esse endereço de volta fica armazenado nesta pilha: e com o debbuger nós temos acesso a ela. Ao clicar, conseguimos navegar e visualizar as variáveis no momento que a chamada foi feita, ou a chamada da função que chamou a função foi feita: útil nesse caso por exemplo de uma função recursiva. E, por último, temos uma lista com todas as paradas, os breakpoints selecionados e a opção de também parar para analisar outros tipos de exceção Mesmo assim, o print acaba sendo muito útil para exemplificar um pouco mais do comportamento interno da aplicação, que pode servir tanto para entender melhor o que o programa faz quanto descobrir causas de problemas para simular localmente a aplicar as práticas já discutidas. Para esse tipo de problema, existem bibliotecas de logging, que, diferentemente do print, permitem a você definir níveis de verbosidade, horário, personalizar a formação… No python temos o módulo `logging` que faz parte da biblioteca padrão e seu uso é muito simples Que resultará em: Analisar o estado da sua aplicação é muito importante para entendê-la, consertá-la e modificá-la. Temos diferente ferramentas que podemos usar para alcançar esse propósito e espero ter conseguido apresentar novas formas de resolver esse antigo problema. Assim chegamos ao fim de mais um Turing Talks. Não se esqueça de nos acompanhar em nossas redes sociais! No Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Também temos um servidor no Discord , no qual divulgamos e discutimos assuntos de Inteligência Artificial. Um abraço e até a próxima!"
https://medium.com/turing-talks/ensinamos-uma-ia-a-jogar-mario-kart-e-mega-man-9e1d74d06a64?source=collection_home---------11----------------------------,Ensinamos uma IA a jogar Mario Kart e Mega Man!,Conheça nosso projeto que ensina IAs a jogarem jogos antigos!,Bernardo Coutinho,756,6,2022-02-13,"Projeto desenvolvido por: Nelson Yamashita, Rafael Coelho, Stephanie Urashima, Matheus Rezende, Fernando Matsumoto e Bernardo Coutinho. Bem-vindo a mais uma edição do Turing Talks! Hoje iremos divulgar um pouco de um projeto que temos realizado no último ano, o TuringRetro! O TuringRetro é um projeto da área de Aprendizado por Reforço no qual utilizamos Inteligência Artificial para ganhar jogos retrô, como o Super Mario Kart e o Mega Man 2. Ele está disponível neste link no nosso Github: O Aprendizado por Refoço, em inglês Reinforcement Learning (RL), é uma subárea do Aprendizado de Máquina que estuda programas que aprendem a realizar tarefas complexas por tentativa e erro, a partir do feedback que recebe de suas ações. Esse tipo de aprendizado se assemelha muito ao processo de aprendizado intuitivo dos seres humanos, no qual o indivíduo experimenta algo e, com base na resposta desse experimento, decide se ele vale a pena ou não. Quando uma criança encosta o dedo em uma superfície quente, por exemplo, ela recebe uma resposta negativa e não repete a mesma ação. As técnicas de Aprendizado por Reforço são muito poderosas, já que conseguem gerar comportamentos extremamente complexos, muito difíceis de serem programados, como fazer um robô caminhar ou até correr sem cair. Nosso principal objetivo foi o de aplicar os conhecimentos que obtivemos estudando diversos algoritmos de RL em ambientes diferentes dos convencionais. Decidimos que jogos antigos poderiam oferecer resultados interessantes, além de que jogos, por sua natureza, têm uma boa base para serem aplicados em ambientes de RL, já que possuem recompensas para feedback (pontos) e objetivos bem definidos. Além disso, jogos historicamente foram usados para testar algoritmos de RL, como no artigo em que apresentaram o algoritmo Rainbow, uma combinação de técnicas em aprendizado por reforço profundo que conseguiu resultados espetaculares em jogos de Atari. Outros exemplos são o Five da OpenAi com Dota 2 e AlphaZero para Go, Xadrez e Shogi. Um dos dois jogos iniciais que utilizamos para o projeto foi o Super Mario Kart. Este é um jogo de corrida no qual o jogador compete com sete outros corredores controlados pelo computador, podendo acelerar, dar ré, pular e até usar itens. Neste desafio, tentamos ensinar nosso programa a completar cinco pistas diferentes. Para o treinamento da IA, estabelecemos três regras principais: Com essas regras, apesar de tornamos o jogo muito mais difícil para o programa, também criamos um bom ambiente para seu aprendizado. Após o treinamento dos jogadores, eles conseguiram terminar todas as cinco voltas em duas das pistas escolhidas, como mostrado abaixo: Já nas pistas mais complexas, como a Rainbow Road, o programa consegue completar algumas voltas, mas acaba saindo da pista em algum momento: Apesar de não conseguir concluir todas as pistas, o resultado do agente foi bem positivo, com um desempenho melhor que a maioria dos participantes do grupo. Mega Man 2 é um jogo de ação e plataforma de grande sucesso do console NES, conhecido no Brasil como Nintendinho. Originalmente o objetivo do jogo era que o personagem passasse por 8 fases, derrotando um inimigo final poderoso chamado de Robot Master no fim da cada uma delas. Primeiramente tentamos treinar a IA para que conseguisse chegar ao final da fase, tendo que aprender a passar por diversos obstáculos, como buracos, espinhos e inimigos, e depois conseguisse derrotar esse chefe final, ou seja, uma IA bem generalizada. Rapidamente vimos que isso não daria certo, então focamos em um dos objetivos, que foi o de aprender a derrotar cada um desses chefes. Normalmente, no jogo, você pode optar por utilizar diversos tipos de equipamentos diferentes, ao invés do tiro convencional do personagem, e alguns chefes possuem fraquezas — ou seja, levam mais dano — de certos equipamentos. Para tornar a IA mais desafiadora, fizemos com que o personagem só pudesse usar seu equipamento básico. Para fazer a engenharia das recompensas, fizemos com que o agente seja punido (receba um feedback negativo) toda vez que recebe dano e quando morre, sendo que, quando ele morre, a simulação do episódio é terminada. Analogamente, ele recebe uma recompensa toda vez que consegue causar dano ao inimigo. Ajustamos as recompensas de maneira que ele recebesse mais pontos ao acertar o inimigo do que perdesse quando receber dano, dando à IA uma característica mais agressiva. Os resultados foram bem positivos, sendo que, dos 8 chefes, a IA conseguiu aprender a derrotar consistentemente 6, incluindo o infame Airman ;). Como o TuringRetro foi um dos nossos projetos mais complexos, optamos por nos basear em algumas ferramentas de Aprendizado de Máquina para auxiliar o desenvolvimento: Para treinar as IAs, utilizamos a biblioteca de Aprendizado por Reforço do Ray chamada RLLib, que disponibiliza diversos algoritmos de Aprendizado Profundo e de fácil uso. Já para a interação da IA com os ambientes de jogos retrô, utilizamos a biblioteca da OpenAI chamada Gym Retro, especializada justamente nessa funcionalidade. Por fim, para treinar os jogadores de maneira rápida, utilizamos os créditos da AWS concedidos pela Amigos da Poli para alugar máquinas na nuvem. O TuringRetro possui dois scripts principais: um para treinar um jogador e outro para rodar um jogador já treinado. Para rodar o projeto, é necessário primeiro instalar as ferramentas necessárias, que pode ser feito por meio do Docker, como explicado nesta seção do repositório. Em seguida, para rodar o projeto, basta utilizar os comandos explicitados nesta seção. Não se esqueça de nos acompanhar em nossas redes sociais! No Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Também temos um servidor no Discord no qual divulgamos e discutimos assuntos de Inteligência Artificial."
https://medium.com/turing-talks/uma-breve-introdu%C3%A7%C3%A3o-%C3%A0-justi%C3%A7a-algor%C3%ADtmica-6f53af7e4ec?source=collection_home---------10----------------------------,Uma breve introdução à Justiça Algorítmica,,João Pedro F.G.,460,4,2022-03-06,"Seja bem vindo a mais um Turing Talks! Lendo o título desse artigo, você deve ter se perguntado: “O que é Justiça Algorítmica?” e, ainda mais relevante, “Por que isso importa?”. Ao longo deste texto, buscaremos responder estas perguntas e entender um pouco mais sobre este novo campo de estudo. Imagine que você dirige o setor de Recursos Humanos de uma grande empresa de tecnologia. Por ser parte de uma empresa tão grande, seu setor recebe um número massivo de currículos a serem avaliados todos os dias e, por ser uma tarefa relativamente tediosa e repetitiva, não faz sentido que você tenha um grande número de funcionários alocados para a avaliação de currículos. Assim, com a ajuda do setor de Inteligência Artificial da empresa, você desenvolve uma ferramenta que realiza um filtro dos currículos. Parece uma boa solução, não é? Talvez até mesmo uma maneira mais justa de realizar contratações, visto que vieses humanos inconscientes não existiriam nesse processo. Bom, em 2014, a Amazon fez algo semelhante ao que foi descrito acima, mas os resultados não foram tão positivos. Um ano após a implementação dos algoritmos de aprendizado de máquina para a contratação de funcionários, percebeu-se que o sistema selecionava candidatos para vagas técnicas com um viés de gênero, isto é, tendia a contratar mais homens do que mulheres. A razão pela qual esse viés ocorreu é que os modelos utilizados nesse sistema foram treinados a partir dos currículos enviados para a empresa ao longo de um período de 10 anos. Assim, a base de dados utilizada era enviesada, gerando, portanto um sistema também enviesado. A situação descrita acima é apenas uma das maneiras pelas quais um modelo de aprendizado de máquina pode se enviesar, outras razões podem ser: Assim, é evidente que vieses em algoritmos podem prejudicar pessoas pertencentes a grupos minoritários. Entretanto, a correção desse tipo de viés não é algo trivial, já que ela passa pelo estabelecimento de uma série de parâmetros do que entendemos como justo e de como deve ser o tratamento dado às pessoas. Portanto, uma pergunta de alto nível de complexidade emerge ao discutir este tema: O que é justiça? A discussão sobre o que torna uma ação ou determinado arranjo social justo é algo que está presente na filosofia desde seu início. Podemos traçar diversas linhas de pensamento dentro desse grande debate, passando por diversas concepções distintas de justiça. Por exemplo, pode-se entender a visão de justiça de John Rawls, filósofo político do século XX, como uma perspectiva que visa a igualdade de oportunidades, independentemente de fatores arbitrários, como contexto socioeconômico. Entretanto, outros filósofos, apesar de também buscarem a igualdade de oportunidades, entendem que o contexto socioeconômico e outros atributos arbitrários devem ser levados em conta na formação dessa concepção igualitária. É evidente que mesmo duas visões com objetivos semelhantes podem ter premissas substancialmente diferentes. Assim, quando o assunto é avaliar o quão justo um algoritmo é, a escolha de qual concepção de justiça deve ser adotada é extremamente relevante, não só porque elas serão operacionalizadas de maneiras diferentes, mas também porque isso terá um impacto direto nas suas métricas de justiça e nas suas decisões de como melhorar o algoritmo. Caso você tenha se interessado por essa discussão filosófica e sua interação com a computação, recomendo a leitura deste e deste artigo. Após essa breve contextualização sobre justiça algorítmica, vamos partir para uma aplicação prática! Iremos analisar o quanto um modelo é influenciado por determinadas características de uma base de dados. A base de dados analisa será o dataset utilizado pela ProPublica, uma organização de jornalismo investigativo, para analisar e auditar os algoritmos utilizados por juízes americanos na análise do quão provável é que um criminoso seja reincidente. Para realizar a análise, utilizaremos a biblioteca FairML, criada para verificar vieses em modelos de aprendizado de máquina. Aqui, iremos analisar quais são os vieses de uma regressão logística ao tentar prever a probabilidade de reincidência. Para isso utilizaremos o seguinte código: Assim, obtemos como resultado, a seguinte imagem: Nesta imagem, temos as variáveis do dataset e o quanto o modelo de regressão é dependente delas. É possível perceber que, mesmo em um modelo tão simples quanto o apresentado aqui, há um viés racial claro contra afro-americanos. Neste artigo, você teve uma breve contextualização do amplo e recente debate sobre justiça algorítmica e também viu como essa discussão pode ser aplicada na prática. O tema é extremamente amplo, então encorajamos que os leitores interessados busquem mais sobre o tema, incluindo a literatura indicada nas referências deste texto. Weapons of Math Destruction Algorithmic Fairness An Economic Perspective on Algorithmic Fairness How We Analyzed the COMPAS Recidivism Algorithm FairML: Auditing Black-Box Predictive Models Não se esqueça de nos acompanhar em nossas redes sociais! No Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Também temos um servidor no Discord no qual divulgamos e discutimos assuntos de Inteligência Artificial."
https://medium.com/turing-talks/pos-tagging-da-teoria-%C3%A0-implementa%C3%A7%C3%A3o-eafa59c9d115?source=collection_home---------9----------------------------,POS Tagging — da teoria à implementação,Entenda como funciona uma das tarefas mais importantes para o Processamento de Linguagem Natural,Rian Fernandes,612,11,2022-04-17,"Olá, leitor! Seja bem-vindo a mais um Turing Talks. Suponhamos que, ao ler este texto, você esteja ouvindo suas músicas favoritas e uma delas é “1999”, da Charli XCX com o Troye Sivan. Você ouve, então, os seguintes versos em determinado instante: I just wanna go back, back to 1999Take a ride to my old neighborhood Aparentemente, você consegue, com clareza, entender a mensagem central que a cantora deseja passar: a de voltar à sua vizinhança antiga. Mas por que o último verso não te gera uma ambiguidade? Vejamos que há uma possibilidade para isso: a palavra “ride”. Por que, ao realizar sua leitura, você não pensou que este termo fosse um verbo, mas sim um substantivo? Sua resposta provavelmente seria dizer que um verbo não pode estar antes de um artigo, no caso, “a”, o que está certíssimo. As classes gramaticais, ou, no inglês, Part of Speech, são fundamentais para entendermos o funcionamento de uma língua e evitarmos, por exemplo, as ambiguidades. Portanto, não diferentemente, para o Processamento de Linguagem Natural, este tema também é fundamental, por isso trabalharemos com ele hoje! Bora lá? O que é POS Tagging e para quê serve? Você já sabe que Part of Speech (sigla “POS”) diz respeito às classes gramaticais, aquelas mesmo que aprendemos na escola. Podemos dizer, então, que POS Tagging é a tarefa de associarmos cada palavra em uma sentença à sua classe gramatical, assim como vemos no exemplo a seguir: As POS tags estão localizadas embaixo de cada palavra (acima das palavras, estão as dependências, assunto para um futuro Turing Talks sobre Parsing). Podemos, neste exemplo, perceber a existência de algumas tags: PRON (pronome), VERB (verbo) e PROPN (nome próprio), mas sabemos que existem várias outras (recomendo dar uma olhada no site das Universal Dependencies). Agora que sabemos o conceito de POS Tagging, podemos nos perguntar: para quê isso serve? Bom, além dos mesmos motivos que podemos pensar na importância das classes gramaticais para nós, humanos, essa tarefa também é base para outras tarefas de NLP, como: Técnicas de POS Tagging Pela importância da POS Tagging, podemos deduzir que é uma tarefa feita há um bom tempo. No entanto, no passado, a anotação das tags era feita manualmente por todo o corpus, o que, graças ao avanço das técnicas de NLP e, consequentemente, de IA, teve seu panorama alterado. Atualmente, a anotação é feita automaticamente por ferramentas que são capazes de anotar uma tag, dado um determinado contexto. Vale ressaltar que ainda existe anotação manual de tags, mas especificamente para conjuntos de dados pequenos, ou para a criação de dados de treinamento para o desenvolvimento de novos POS Taggers automáticos. As técnicas automáticas de POS Tagging são várias, como: Neste Turing Talks, daremos ênfase aos modelos probabilísticos. Para isso, primeiramente, devemos, então, entender o que são os Modelos Ocultos de Markov e o porquê de eles serem a melhor solução para POS Tagging. Modelos Ocultos de Markov Para entendermos a ideia por trás dos Modelos Ocultos de Markov, vamos imaginar uma situação: você, como estudante, possui um dia a dia extremamente corrido e, por isso, consegue estar em dois estados, “com sono” ou “sem sono”. Temos que ter em mente que o estado é algo não observável, ou seja, neste caso, não conseguimos observar o seu interior para saber se você tem sono ou não, o que podemos observar é justamente se você dormiu muito ou dormiu pouco (o que é uma evidência para dizermos se você estava com sono ou não). Essa situação pode ser esquematizada da seguinte forma: A ideia é que, dado um determinado dia inicial (começo), você possui, por exemplo, 40% de chance de estar com sono e 60% de chance de estar sem sono. Além disso, dado um dia em que você tinha sono, você tem 70% de chance de ter sono no próximo dia também e, claramente, 30% de não ter sono. Como os números são muitos, podemos organizá-los nas duas tabelas a seguir: Agora, com as probabilidades organizadas, talvez fique mais fácil trabalhar com alguns conceitos e informações. Primeiramente, é importante afirmar que as probabilidades de transição correspondem ao que chamamos de Estados Ocultos, pois não observamos sua ocorrência, isto é, não conseguimos observar se uma pessoa tem sono ou não. Por outro lado, as probabilidades de emissão correspondem aos Estados Observáveis, pois são justamente o que conseguimos observar: neste caso, se a pessoa dormiu muito ou pouco; podemos dizer que a emissão é uma evidência para a inferência do estado, ou seja, se uma pessoa dormiu muito, provavelmente ela estava com sono (ou, com outro exemplo, se vemos alguém com olheiras, provavelmente a pessoa está com sono). A ideia por trás dos Modelos Ocultos de Markov é que existem alguns estados ocultos sobre os quais não sabemos, mas que influenciam diretamente os estados observáveis. Outro conceito importante é que a probabilidade de um determinado estado ocorrer depende unicamente do estado anterior, ou seja, não leva em conta o resto do histórico, o que é conhecimento como a suposição de Markov. No nosso exemplo, podemos compreender que o fato de uma pessoa ter sono ou não em um dia depende exclusivamente se ela teve sono ou não no dia anterior; junto a isso, o fato de uma pessoa dormir muito ou pouco depende apenas se ela tinha muito ou pouco sono em seu estado anterior. Matematicamente, essa suposição é representada assim: Entendido isso, você deve estar se perguntando: mas o que isso tem a ver com POS Tagging? Calma lá, que é justamente isso que vamos ver agora. A ideia por trás dos Modelos Ocultos de Markov é extremamente importante para entendermos a funcionalidade do algoritmo Viterbi, responsável justamente pela nossa tarefa de NLP. Algoritmo Viterbi Para entendermos a essência do algoritmo, vamos utilizar um trecho de outra música da Charli XCX, “Vroom Vroom”(já anotada por um POS Tagger): Podemos entender que, em uma sentença, as palavras são os estados observáveis que possuímos no nosso modelo. Também percebemos que as POS tags são os estados ocultos, já que não observamos sua ocorrências, mas são elas que condicionam a existência de seu estado subsequente. Para que possamos trabalhar com nosso POS Tagger, é válido ressaltar que necessitamos de, além das palavras já tokenizadas, de um tagset, que nada mais é do que um conjunto de dados com sentenças já anotadas com as POS tags. É esse tagset que gera para nós as probabilidades de transição e emissão necessárias para os Modelos Ocultos de Markov e, consequentemente, para o nosso algoritmo. Neste exemplo do trecho, poderíamos definir, hipoteticamente, as probabilidades do nosso tagset da seguinte forma: Cada linha nesta matriz soma, ao total, 1, pois, dado um estado, todas as possibilidades de tags somadas resultam em 100%. Como o nosso exemplo é hipotético, não foram colocadas todas as tags, mas, para que se some 1, é necessária a existência de todas elas na matriz. Nesta matriz, a soma de cada linha já não é equivalente a 1. É fato que as probabilidades somadas de, dado uma determinada tag, tal ocorrência ser uma palavra x, uma palavra y, ou uma palavra z (e assim por diante…), devem ser 1, mas como o nosso tagset apresenta milhares de palavras, é um pouco complicado que coloquemos todas em exposição (isto é, uma matriz com milhares de colunas); para além disso, as palavras que não ocorrem no nosso trecho não nos importam muito. Para fazer a interpretação adequada das matrizes e, consequentemente, das probabilidades, devemos ler da seguinte forma: E assim poderíamos fazer com todas as possibilidades para todas as palavras. No entanto, surge um problema: existem várias tags e, além disso, só neste nosso pequeno trecho, possuímos 8 palavras. A quantidade de probabilidades que deveríamos calcular para maximizarmos a nossa probabilidade final seria muito grande, principalmente em um corpus maior. Visando minimizar isso, uma possível solução é que poderíamos levar em conta apenas as probabilidade de emissão, analisando a ocorrência de cada palavra isoladamente. No entanto, ao fazermos isso, ignoramos o fato de que as ocorrências ou não de determinadas tags são condicionadas por outras tags: neste caso, “ ‘ve” seria classificado como VERB e não como AUX, o que seria incorreto. É nesse momento que entra o nosso maravilhoso algoritmo Viterbi: O Viterbi funciona da seguinte maneira: devemos multiplicar a probabilidade de transição pela probabilidade de emissão, ou seja, os primeiros passos são os seguintes: Dito isso, podemos entender o porquê de o algoritmo ser bem melhor do que a temida “brute force”: ele só leva em conta a probabilidade da palavra anterior que maximiza a probabilidade da palavra atual. Enquanto, para olharmos todas as ocorrências (brute force), teríamos que andar Pˡ, sendo “P” a quantidade de POS tags e “l” a quantidade de palavras na nossa sentença, com o Viterbi, devemos olhar apenas L.P². Entendido o funcionamento do algoritmo, podemos, agora, ver sua implementação :) Implementação em Python Para a implementação do POS Tagging em Python, podemos utilizar bibliotecas como NLTK e Spacy, mas, hoje, criaremos nosso próprio POS Tagger! Primeiramente, vamos importar as bibliotecas e fazer os downloads necessários. Nosso tagset a ser usado é proveniente da biblioteca NLTK: Podemos ver que nosso tagset é uma lista de listas, sendo cada lista interior uma sentença; além disso, dentro de cada lista, há tuplas contendo cada token e sua tag: Podemos, então, dividir nossos dados em treino e teste. Após isso, dividiremos nossas tuplas em treino e teste também: Também é interessante que vejamos a quantidade de tags presentes no nosso tagset e seus nomes: Agora, definiremos uma função para calcular a a Probabilidade de Emissão e outra para a Probabilidade de Transição: Podemos, então, calcular uma matriz contendo as nossas probabilidades de transição: Para ela ficar mais compreensível, podemos transformá-la em um DataFrame: Agora, definiremos, então, nossa função para rodar o Viterbi. Após isso, definiremos algumas sentenças aleatórias para testarmos nosso modelo :) Por fim, testaremos nosso modelo: Vimos, então, que conseguimos uma acurácia de quase 94%! Quem diria que nosso modelo se comportaria tão bem :) E por hoje era isso! Se você se interessa por Processamento de Linguagem Natural ou qualquer outra vertente do ramo da Inteligência Artificial, não deixe de ler outros Turing Talks e nos acompanhar em nossas redes sociais, como Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Entre também em nosso servidor no Discord. Muito obrigado por chegar até aqui! Bons estudos e até! :) POS (Part-of-Speech) tagging with Hidden Markov Model | Great Learning (mygreatlearning.com) Hidden Markov Model : Data Science Concepts — YouTube Part of Speech Tagging : Natural Language Processing — YouTube The Viterbi Algorithm : Natural Language Processing — YouTube"
https://medium.com/turing-talks/clustering-conceitos-b%C3%A1sicos-principais-algoritmos-e-aplica%C3%A7%C3%A3o-ace572a062a9?source=collection_home---------8----------------------------,"Clustering — Conceitos básicos, principais algoritmos e aplicações",,Felipe Azank,751,23,2022-05-01,"Escrito por Felipe Azank e Gustavo Corrêa Olá, sejam bem vindos a mais um Turing Talks, hoje falaremos sobre uma das atividades mais realizadas na Área de Ciência de Dados: os processos de Clustering ou Agrupamento. Esse texto tem como objetivo explicar as principais técnicas de Clustering, assim como o funcionamento geral de um algoritmo de aprendizagem não-supervisionada. Para isso, seguiremos a seguinte estrutura: Clustering, ou agrupamento, consiste na implementação de técnicas computacionais para separar um conjunto de dados em diferentes grupos com base em suas semelhanças. Diferentemente de algoritmos de classificação e regressão, o Agrupamento faz parte do universo da Aprendizagem Não Supervisionada, na qual os algoritmos devem entender as relações entre dados sem estarem rotulados a nenhuma categoria prévia. No universo de Data Science, as técnicas de agrupamento costumam ser utilizadas para múltiplas funcionalidades, entre as principais temos: Agora que já temos uma ideia geral do significa de Clustering e onde esses processos podem ser aplicados, vamos aos diferentes tipos de algoritmos de agrupamentos de dados. Existem diversos tipos de algoritmos de Clustering, a grande diferença entre eles está em como sua complexidade aumenta com o crescimento do dataset. Dessa forma, entender os diferentes tipos de algoritmos mostra-se interessante para escolher aqueles que agregam os melhores resultados para os seus dados. Esses tipos podem ser agrupados em em 4 grupos: Clusterização Baseada em Centróides, Clusterização baseada em Densidades, Clusterização, Clutserização baseada em Distribuições e Clusterização Hierárquica. As técnicas de agrupamento baseadas em centróide apresentam a missão de, partindo de uma quantidade determinada de grupos, encontrar quais são os centróides (centros geométricos) os quais representam o “meio” de cada cluster e, a partir deles, identificar qual cluster cada um dos pontos pertence com base nas suas distâncias para cada um dos centróides. Uma vez que a identificação dos centróides é finalizada, a classificação de todos os pontos nos clusters torna-se muito simples, uma vez que é baseada apenas no cálculo da distância dos pontos em relação aos centróides, portanto, algoritmos desse tipo costumam ser eficientes. Contudo, como desvantagem, acabam por ser mais sensíveis a condições iniciais (uma vez que, para se encontrar os centróides, partem-se de um conjunto de pontos aleatórios, que são ajustados a cada iteração, até se encontrar um conjunto final) e funcionam melhor quando os dados estão bem separados entre si. Ademais, graças à formulação dos algoritmos, esse tipo de clusterização não costuma lidar adequadamente com outliers, os inserindo no cluster de centróide mais próximo. Mais a frente, veremos representantes desse tipo de algoritmo (K-Means e Mini Batch K-Means), cujos procedimentos serão explicados mais detalhadamente. Algoritmos de clustering baseados em densidade têm como objetivo identificar regiões de alta concentração de pontos e os conectar em agrupamentos, identificando, dessa forma, os clusters. Esse tipo de método permite que seja possível a identificação de clusters com geometrias arbitrárias e também permite que seja possível identificar outliers. Contudo, esse tipo de abordagem não costuma se sair bem com dados que apresentam agrupamentos com densidades distintas entre si. O representante mais conhecido desse tipo de clusterização é o DBScan, que terá seu funcionamento e aplicação explicados mais a frente. Algoritmos desse tipo tentam assumir diferentes distribuições a partir dos dados e definir cada distribuição encontrada como um agrupamento diferente. Em outras palavras, os algoritmos tentam ajustar diferentes distribuições aos dados, de maneira a relacionar cada distribuição com um cluster. Com base nisso, a classificação de cada ponto nos grupos pode ser feita de modo probabilístico, no qual, dada as distribuições de cada grupo, pode-se estimar a probabilidade do ponto pertencer a essa distribuição. Esse tipo de algoritmo encontra diversos problemas, uma vez que a criação de cada uma das distribuições que descrevem os grupos pode sofrer com overfitting caso não exista condições que limitem o ajuste de parâmetros. Ademais, é possível também notar que esses algoritmos não apresentam grande utilidade quando não se sabe qual o tipo de distribuição dos dados ou quando, por algum motivo, os clusters de dados pertencerem a diferentes tipos de distribuições (Gaussiana, Poisson, Pearson e etc). Um exemplo de utilização desse tipo está no algoritmo Gaussian Mixture, que tenta adequar diferentes modelos gaussianos aos dados. OBS: Muitas vezes, quando queremos exemplificar ou testar algoritmos de clustering, geramos dados aleatórios a partir de distribuições, usando comandos como “make_classification” do sklearn. Se olharmos com atenção, esse processo de selecionar aleatoriamente dados de diferentes distribuições para gerar um dataset acaba por ser o processo reverso do que esse algoritmo baseado em distribuições realiza, o que gera, em muitos casos, uma performance excelente desse quando usamos esses dados gerados aleatoriamente, criando assim, uma sensação de engano sobre a performance dos algoritmos em dados reais. Algoritmos desse tipo têm como objetivo agrupar dados similares entre si usando ferramentas como a distância entre eles. A diferença desse tipo de algoritmo para os demais está na criação de diversos clusters (alguns agrupamentos dentro de outros), o que, por fim, acaba gerando uma árvore de clusters, na qual um dado pertence a grupos menores e maiores, criando, dessa forma, uma hierarquia. Tendo essa hierarquia de clusters, podemos, então, controlar o quanto queremos que os dados sejam semelhantes entre si a ponto de pertencerem a um mesmo grupo, o que fará com que o algoritmo “suba” ou “desça” na hierarquia dos agrupamentos. Ademais, podemos, inversamente, informar quantos clusters desejamos ter, levando a uma configuração da hierarquia que fornecerá esse número. Contudo, esses algoritmos apresentam duas desvantagens principais: Os principais exemplos desse tipo são o Agglomerative Clustering e o BIRCH, ambos explicados mais ao fim. Após entendermos como os métodos de Clusterização podem ser divididos, assim como suas vantagens e desvantagens em geral, vamos analisar mais em detalhes os principais algoritmos utilizados nessas tarefas. Como este Turing Talks consiste em um texto introdutório ao universo de clustering, todos estes métodos também estão bem documentados e apresentam uma aplicação simples através de bibliotecas como o scikit learn. Os algoritmos que conheceremos agora serão: Um dos algoritmos mais conhecido assim como bem antigo (proposto em 1967), esse algoritmo baseado em centróides, tem como objetivo, encontrar os agrupamentos nos dados nos quais a variância dentro de cada cluster seja mínima. Parâmetros principais de entrada: O funcionamento do algoritmo tem como base inserir K centróides aleatórios (sendo K o número de clusters) e, a partir desses pontos, ajustá-los com base nos dados, a fim de encontrar o centro dos clusters. De maneira mais detalhada, o algoritmo realiza as seguintes etapas: O funcionamento ilustrado permite entender facilmente cada uma das etapas: http://shabal.in/visuals/kmeans/2.html Para a aplicação do K-means e de todos os outros algoritmos desse Turing Talks, foi-se utilizado o Mall Customer Segmentation Data, uma base de dados didática e simplificada de usuários de um shopping. A base contém 4 colunas: “Gênero”,”Idade”, “Renda Anual (em milhares de dólares)” e “Spending Score” (uma classificação de 0 a 100 dada pelo shopping com base no comportamento de gasto do cliente). Como esse texto se trata de um texto introdutório e mais focado nos algoritmos e conceitos, vamos relevar alguns processos realizados nos dados e aplicar os algoritmos de Clustering considerando apenas duas features: Renda Anual e Pontuação de Gastos, que podem ser vistos no gráfico a seguir. Por fins de simplicidade, vamos definir o número de clusters para 5, uma vez que visualmente esse parece ser o caso (existem formas matemáticas para a definição disso, que abordarem em próximos textos). Para implementar o algoritmo de K-Means, basta utilizarmos a biblioteca sklearn.clusters, que contém diversas implementações dos algoritmos de Clustering mais populares na área de Ciência de Dados. Adiciona-se que, para que seja possível obter resultados mais consistentes, é necessário a normalização das features. O código gera o seguinte gráfico: Como pode ser visto, o algoritmo identificou 5 clusters bem definidos, sem apresentar dificuldade com os dados esparsos. Contudo, podemos perceber também que alguns dados bem distantes foram inseridos em agrupamentos, uma vez que esse algoritmo não identifica outliers. Esse algoritmo entra como uma alteração do K-means, com o objetivo principal de otimizar o processo de clusterização e utilizar menos memória em comparação com a abordagem clássica. Parâmetros principais de entrada: Vantagens e desvantagens Por assemelhar-se muito com o K-Means, esse algoritmo apresenta as mesmas vantagens e desvantagens em comparação com os demais, entretanto, é válido ressaltar alguns contrastes: Utilizando o scikit learn, a aplicação pode ser vista abaixo: gerando o gráfico: Como já esperado, é possível notar que o resultado se assemelha muito com o K-means, contudo, um fato curioso a se notar está na ordem nos quais os “labels” do gráfico estão dispostos, o que demonstra que a ordem com que se foi elaborado os clusters e direcionado os dados é diferente do algoritmo anterior. Partindo agora para os algoritmos baseados em densidade, o DBScan (Density-Based Spatial Clustering of Applications with Noise) visa encontrar áreas de alta densidade no domínio e expandir essas áreas de forma a encontrar os clusters. Esse algoritmo é mais recente do que o K-Means, tendo sua primeira publicação em 1996. Ao contrários dos métodos ja vistos, este por sua vez não exige que seja inserido, a priori, o número de clusters, uma vez que elabora os grupos com base no número de vizinhos a um ponto e no raio da vizinhança. Parâmetros principais de entrada: Em linhas gerais, o algoritmo busca criar uma “rede” de pontos vizinhos, de maneira a entender quais pontos estão “juntos” entre si e, portanto, fazem parte do mesmo cluster. O funcionamento mais detalhado pode ser entendido pelas seguintes etapas: Um exemplo ilustrado pode ser visto abaixo. Nele, temos minPts = 4, ou seja, vemos que os pontos em A, para seus raios de vizinhança, contêm 4 pontos ou mais em seus arredores, desta forma, todos os pontos em vermelho são identificados como Pontos Centrais (Core). Os pontos em B e C, por sua vez, não apresentam uma quantidade maior ou igual a minPts em seus arredores, contudo, são vizinhos de Pontos Core, o que faz com que eles sejam identificados como pontos de fronteira do cluster (amarelo). Por fim, o ponto N não apresenta nenhum vizinho para seu arredor de raio ε, o que faz com que seja identificado como um Outlier (azul). gerando o gráfico: O resultado, diferentemente dos anteriores, mostra a label “-1”, evidenciando a presença de outliers, dados que não pertencem a nenhum outro grupo. Ademais, vemos claramente a presença da última desvantagem listada: na qual a diferença de densidade entre os clusters ocasionou na criação de grupos menores. É uma abordagem para a clusterização baseada em grafos em que não é necessário especificar o número de clusters. Seu funcionamento se dá a partir da comunicação entre os datapoints (como uma rede), o loop de troca de informações ocorre da seguinte forma: Parâmetros principais de entrada: Para facilitar ainda mais o entendimento do algoritmo segue o seguinte exemplo de seu funcionamento, dividido em: Construção da matriz de similaridade A similaridade entre duas pessoas é calculada como a negação da soma dos quadrados das diferenças das coordenadas deles: Exemplo da célula entre Bob e Edna A tabela final fica no seguinte formato O número de cluster gerados será menor quanto menor for a diagonal, então preencheremos ela com o menor número de todas as células, -22. Construção da matriz de responsabilidade Todas as células começam com 0 e para preencher as células usamos a seguinte fórmula: A reponsabilidade da pessoa A para a pessoa B é a similaridade da pessoa A para B menos a máxima similaridade da pessoa B. Depois de calcular todas as responsabilidades obtemos: Construção da matriz de disponibilidade Primeiramente preenchemos a diagonal principal com a soma das responsabilidades da pessoa, que são os valores acima de zero excluído a responsabilidade dela com ela mesma. Depois disso, para preencher a disponibilidade de uma pessoa A para uma pessoa B, somamos a responsabilidade de A com A com os valores positivos restante das responsabilidades de A desconsiderando a responsabilidade de A para B. Depois de calcular as disponibilidades obtemos: Construção da matriz de critério É a soma da matriz de responsabilidade com a de disponibilidade célula a célula, isto é, a célula (1,2) da matriz de critério é a soma das células (1,2) das matrizes de responsabilidade e disponibilidade. O resultado é: Para definir os grupos pegamos o critério mais alto de cada coluna e usamos como exemplar para juntar as pessoas em grupos. Daí: Visto isso, os grupos gerados são: (Alice, Bob e Cary) e (Doug, Edna) Como listado nas desvantagens, o algoritmo recorre a construção de clusters esféricos ou elípticos, gerando resultados nem sempre ideiais para dados complexos. Ademais, é possível perceber que a quantidade de clusters encontrados no exemplo pode ser controlada com o parâmetro preferences, uma vez que, quanto menor esse número, menor a quantidade de clusters que o modelo tende a encontrar. Portanto, podemos ver que, no caso do Affinity Propagation, é necessário uma estimativa clara dos parâmetros para atingir bons resultados. A maior parte dos algoritmos de clustering hierárquico não são eficientes, isso gera um problema quando você tem que trabalhar com um número grande de dados com recursos limitados. Assim surgiu um método hierárquico chamado BIRCH, o qual, além de visar resolver os desafios da complexidade elevada, foi um dos primeiros algoritmos a lidava com outliers. Conceitos fundamentais Antes de entendermos o funcionamento em si do algoritmo, vale entender alguns conceitos fundamentais para esse algortmo de Clustering, esses conceitos são: Clustering Feature e Clustering Feature Tree. Um Clustering Feature consiste nas informações a respeito de uma subdivisão dos dados criada pelo BIRCH, um subcluster. O CF é definido por 3 valores principais: N, LS e SS: A partir deles pode-se calcular outras características dos clusters, como: Assim, um CF é um agrupamento dos dados que contém essas características. Por fim, o conjunto dos CF em forma de árvore é conhecido como CF Tree, que consiste em uma árvore na qual cada folha contém um sub-cluster. Cada entrada da árvore é um CF por si só e, caso não seja uma folha, apresenta um ramo que aponta para os nós filhos. Parâmetros principais de entrada: O processo de funcionamento do BIRCH tem início com a criação da CF Tree. Para entender esse passo vamos utilizar de um exemplo, criaremos uma CF Tree com base num threshold de 2 e no seguinte conjunto de pontos: Tendo isso definido, o processo completo de funcionamento do algoritmo se dá de duas formas: Construção da árvore e Clustering global Possui 3 fases obrigatórias e uma opcional: Para cada decisão de clustering não é necessário analisar todos os data points e clusters existentes. Para isso, ele explora a ideia que o espaço dos dados não é geralmente uniformemente ocupado e, por isso, nem todo datapoint é importante. Como pode ser visto no resultado aplicado, apesar de identificar facilmente alguns clusters evidentes, a parte superior direita acaba por dividir os clusters de maneira estranha. Isso ocorreu possívelmente pela esparsialidade dos dados, o que pode ter deslocado o cluster 2 de local, fazendo com que o aglomerado de dados com label 1 (verde) na parte superior direita tenha sido relacionada com os dados do centro, e não da extremidade. É um método de clustering hierárquico que funciona tratando cada datapoint como um cluster (chamado de folha) e aglomerando eles em pares gerando um cluster maior (chamado de nó). O processo segue até que todos os pontos estejam aglomerados em um único cluster, obtendo assim, uma árvore hierárquica chamada de dendrograma. Exemplo de Dendrograma gerado a partir dos dados Parâmetros Iniciais de Entrada Como descrito anteriormente o algoritmo se dá da seguinte forma: A proximidade dos pontos é definida pelo cálculo da sua distância, sendo o método mais comum para isso a distância euclidiana. Isso é bem lógico para os casos entre pontos, mas como isso funciona com os nós? Para juntar os nós existem alguns diferentes critérios de ligação (Linkage Criterion): Depois de terminado o dendrograma, faz-se um corte horizontal de acordo com o número de clusters finais desejados. Problemas de Clusterização estão presentes cada vez mais e podem servir como chave para diversos setores e empresas que desejam potencializar suas decisões e análises com o uso de dados. Como visto hoje neste Turing Talks, esse universo compreende diversos algoritmos, cada um com suas vantagens e desvantagens, cabendo a nós estudarmos e compreendermos quais deles são as melhores opções para a resolução de cada problema. Este texto apesar de bem carregado com muito conteúdo, ainda toca na superfície dos problemas e algoritmos de Clusterização, abrindo portas para muitas oportunidades de aprofundamento nos próximos Turing Talks. Estamos nos primeiros passos de um longo caminho :) Esperamos que você tenha gostado e muito obrigado por chegar até aqui. Se quiserem conhecer um pouco mais sobre o que fazemos no Turing-USP, não deixem de nos seguir nas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Para acompanhar também mais de perto e participar de nossas discussões e eventos, entre no nosso servidor do Discord. Até a próxima!"
https://medium.com/turing-talks/diagnosticando-a-sua-regress%C3%A3o-linear-4c35fcdf3b9d?source=collection_home---------7----------------------------,Diagnosticando a sua Regressão Linear,Verificando a robustez do seu modelo com statsmodels em Python,Luísa Mendes Heise,603,8,2022-06-05,"Olá, querido leitor, seja muito bem-vindo a mais um Turing Talks. Imagino que você, que não perde nenhum dos nossos textos, já deve estar familiarizado com um dos modelos estatísticos mais clássicos de todos os tempos, a regressão linear. No entanto, caso o conceito esteja um pouco nebuloso, você pode sempre pode reler o nosso texto aqui no Turing Talks sobre o tema. A proposta de hoje é se aprofundar um pouco em alguns métodos de diagnóstico de uma regressão linear e, assim, verificar a robustez do modelo. Podemos entender robustez de diversas formas, mas aqui, especificamente, damos foco na capacidade do modelo de explicar os dados e de prever de forma segura observações que estejam no seu escopo. É importante pontuar que a regressão linear é muito utilizada não só para prever, mas também para explicar os nossos dados. Nesse sentido, ainda que o nosso modelo produza métricas de negócio satisfatórias, precisamos ficar atentos aos pressupostos e às anormalidades, para que não cheguemos a conclusões erradas sobre os nossos dados. Bom, começando, quando ajustamos um modelo de regressão linear, assumimos: 2. Tais erros são normalmente distribuídos. 3. Os erros das observações são independentes entre si, não havendo nenhuma estrutura temporal, espacial ou outra correlação entre eles. 4. Os erros tem variância constante, ou seja, se estou prevendo preços de casas, a variância do erro numa faixa de 100k-200k BRL não deve ser diferente da variância do erro da previsão de uma casa na faixa de 1M-1.5M BRL. Lembrando que esses pressupostos permitem a construção de procedimentos de inferência, mas não são necessários para uma convergência numérica estimativas de mínimos quadrados (OLS). Por último, antes de falarmos em detalhe sobre esses pontos, é importante relembrar que não temos acesso ao erro em si, mas ao chamado resíduo, que é uma aproximação do erro calculada pela diferença do valor previsto e o valor real: Para ilustrar esse Turing Talks, vamos utilizar um dataset clássico, o California Housing Prices. Utilizaremos apenas uma covariável, a medium income, de forma a tornar mais simples as análises. Quando o erro segue a distribuição normal, podemos garantir que nossos parâmetros beta estimados com mínimos quadrados são iguais a uma estimativa gerada via máxima verossimilhança. Entretanto, a recíproca não é verdadeira, ou seja, a não normalidade dos resíduos não nos permite afirmar que, com certeza, nossos parâmetros estão errados, inclusive, com um número grande de dados, as estimativas podem estar adequadas. Ainda assim, é importante estar atento a esse ponto, já que uma distribuição muito distante do esperado pode significar que um modelo linear não é suficiente para explicar os dados e, nesse caso, vale a pena inserir transformações nos dados (como log) para tentar resolver o problema. Se isso ainda assim não resolver a questão, talvez seja melhor utilizar outros métodos que podem lidar com a distribuição real. Para verificar a normalidade dos resíduos, nós podemos utilizar alguns métodos, tanto gráficos como testes estatísticos. Vamos ilustrar com o nosso exemplo. Primeiro, vamos começar ajustando um modelo de regressão linear, com a ajuda da lib statsmodels: O gráfico QQ (quantil-quantil) é um gráfico de probabilidade, que é um método gráfico para comparar duas distribuições de probabilidade plotando seus quantis um contra o outro. Nesse caso, a linha é a distribuição teórica (normal) e os pontos são referentes a distribuição encontrada dos resíduos. Olhando para o gráfico, vemos que nossos resíduos se aproximam de uma distribuição normal nos quantis mais centrais, entretanto, parecem ter uma cauda longa, o que os distanciam do caso ideal. Outra possível estratégia é plotar o histograma dos resíduos e verificar “no olho” se a sua distribuição se aproxima da normal. Nesse histograma, fica clara a existência de uma cauda longa na nossa distribuição. Por último, uma opção mais rigorosa é utilizar um teste estatístico. Você pode utilizar qualquer teste de normalidade que quiser. Aqui vamos utilizar um que convenientemente já está implementado na lib que estamos utilizando. O teste rejeita a hipótese nula, o que indica que os dados violam a suposição de normalidade dos resíduos. Ainda assim, vamos continuar com esse modelo, já que o gráfico QQ e o histograma tem um aspecto considerado satisfatório. Caso seja observada heterocedasticidade nos resíduos, isto é, caso a sua variância não seja constante, isso pode significar que observações incertas têm muita influência sobre as estimativas. Além disso, os intervalos de previsão estarão, com certeza, errados, já que a eles são calculados utilizando diretamente com a variância dos resíduos. De novo, podemos verificar se esse é o caso de uma forma gráfica e também com um teste estatístico. Aqui começa a ficar mais claro que a “linha” de valores no topo do gráfico “atrapalha” o nosso modelo. Essa é uma forma imediata e visual de verificar se existe alguma tendência nos resíduos em relação ao valor previsto. Aqui claramente vemos que há uma tendência sistemática, em forma de linha. A lib que utilizamos, novamente, contém um teste estatístico para que verifiquemos se a variância dos nossos resíduos são constantes. O teste é significativo, o que significa que os dados violam a suposição de homocedasticidade, ou seja, a heterocedasticidade está presente nos resíduos. A última coisa que queremos verificar em relação aos resíduos é se eles tem alguma correlação. Um jeito de verificar isso é com um gráfico de auto correlação. Basicamente, interpretamos os resíduos como uma série temporal. Então calculamos a correlação entre as observações em função do intervalo de “tempo” entre elas, ou seja, trasladamos a série em um delta T e calculamos a correlação da série trasladada com a original. Obviamente, quando o lag é zero, a correlação será 1, já que é a correlação da série com ela mesma, isto é, do valor de cada resíduo com ele mesmo. Vemos aqui que os resíduos não parecem estar correlacionados. A faixa azul representa um intervalo de confiança de 95% ao redor do zero. Agora vamos falar de algumas técnicas mais “avançadas” que podem te ajudar a identificar pontos estranhos na sua regressão linear. De forma simplificada, podemos entender que os valores Y_hat previstos em uma regressão linear são uma combinação linear dos valores Y utilizados para ajustar esse modelo. O leverage é uma medida que nos indica quanto que uma observação influi na previsão do seu próprio valor. Nesse sentido, as observações com alto leverage não conseguem ser explicadas de forma eficiente pelas demais. Um ponto com leverage elevado não necessariamente é um outlier, mas um dado que tem um valor fora do escopo dos dados de treinamento e, por isso, os demais pontos “falham” em descrevê-lo. Aqui vemos que as observações com previsão mais alta tem leverage mais alto. Todos os erros estão incluídos em na estimativa da variância dos resíduos, de modo que um grande resíduo contribuirá para uma grande variância, afetando todos os outros resíduos padronizados. Isso pode ser mitigado usando os resíduos studentizado. Um resíduo studentizado é o quociente resultante da divisão de um resíduo por uma estimativa de seu desvio padrão. Quando este valor elevado, isso pode gerar suspeitas em relação ao comportamento daquela observação em específico. De forma geral, é uma forma de “superar” o problema da heterocedasticidade dos resíduos na hora de verificar os pontos de atenção, ou seja, aqueles que tem um resíduo “normalizado” pela variância elevado. Em código, temos: A distância de Cook é uma expressão que avalia o impacto que a retirada de uma observação na estimação dos parâmetros beta da seguinte forma: Ou equivalentemente: Sem entrar muitos nos detalhes, intuitivamente, a distância de Cook mede globalmente quanto que os parâmetros se alteram com a retirada de um ponto. Em código, temos: Aqui verificamos quais pontos estão causando maior mudança nos parâmetros estimados. Poderíamos tentar entender o que difere esses pontos dos demais. DFBETAS é uma medida que, assim como a distância de Cook, mede a influência dos pontos nos parâmetros, mas de forma individualizada. Assim, numa regressão múltipla, podemos ver a influência de cada observação em betas específicos, e não como uma medida global. No nosso exemplo, vamos verificar a influência das observações no beta correspondente a renda média. Nesse texto, você pôde descobrir algumas formas de diagnosticar sua regressão linear e identificar pontos que podem ser outliers ou devem ser melhor estudados. Esperamos que você tenha gostado e agradecemos por chegar até aqui. Se quiserem conhecer um pouco mais sobre o que fazemos no Turing-USP, não deixem de nos seguir nas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Para acompanhar também mais de perto e participar de nossas discussões e eventos, entre no nosso servidor do Discord. Por hoje é só, até mais!"
https://medium.com/turing-talks/uma-breve-introdu%C3%A7%C3%A3o-de-web-scraping-com-beautiful-soup-f762d191ec44?source=collection_home---------6----------------------------,Uma breve introdução de web scraping com Beautiful Soup,,Hugo R. V. Angulo,545,8,2022-08-14,"Texto escrito por: Hugo R. V. Angulo, Antonio Freire, Igor Augusto Martins Sousa, Bruno Vidigal Para além dos muitos datasets que conseguimos achar em sites como o Kaggle e que são comumente construídos nas empresas, existe uma quantidade imensa de informações úteis e interessantes que estão esparsas pelos websites alheios que acessamos de vez em quando. Essas informações não estão estruturadas nem prontas para uso… Logo, não podemos usá-las… certo? É aí que entra a importância da sabedoria de Web Scraping. Web scraping significa a extração de informações e dados direto de um website. Isso pode ser feito manualmente, por exemplo com um copy & paste. Contudo, para possibilitar essa extração, essa extração, ferramentas para a automação são necessárias, como softwares específicos ou scripts utilizando diferentes bibliotecas focadas em web scraping. No nosso caso, vamos explorar mais a fundo como fazer Web Scraping com BeautifulSoup, uma biblioteca para extração de dados de páginas estáticas em HTML e XML. BeautifulSoup é uma biblioteca que fornece ferramentas para extração de informações contidas no HTML de uma página estática. Uma página pode ser estática ou dinâmica, mas o que isso quer dizer? - Estática: um request é processado pelo servidor do site e uma resposta é devolvida diretamente pelo servidor.- Dinâmica: exige que o request passe por outros programas para devolver a resposta. O foco do presente artigo será fazer uma aplicação de Webscraping em páginas estáticas, as páginas dinâmicas serão tratadas em futuros trabalhos. Sobre o tal de “request”, mencionado acima, é por meio deles que a internet tem seu funcionamento, mais especificamente requests HTTP. Esses são os “pedidos” que seu navegador faz ao servidor da página pela qual está navegando. Com esses “pedidos” é que se renderiza a página web para que o usuário a veja e interaja com ela. Requests HTTP podem ser feitos pelo navegador e também em Python, usando a biblioteca Requests, que devemos usar conjuntamente ao BeautifulSoup para conseguirmos extrair as informações que buscamos. Com a biblioteca Request, podemos interagir com as APIs dos sites em que desejamos fazer Web Scraping e, utilizando alguns de seus métodos, podemos extrair, adicionar, deletar e alterar informações, tudo isso fazendo um pedido ao servidor da página. No caso usando a biblioteca Requests como suporte ao BeautifulSoup, usamos o método GET para pegar ou extrair informações da página, interagindo com o seu servidor para que isso seja possível. Só assim conseguimos retirar os preciosos dados contidos na estrutura HTML de páginas estáticas. No esqueleto do site, há partes do arquivo HTML que contém informações atreladas a algum produto ou item ali disposto, por exemplo, e com comandos específicos da biblioteca que exploraremos, podemos extrair esses dados e compilá-los em um dataset com essas informações. E a partir daí, fazer o que já sabemos, analisar esses dados. Então vamos para o código! Para exemplificar a utilização do web scraping para a construção de datasets, iremos utilizar dados do site airBnB. Nosso objetivo é fazer uma comparação simplificada entre os preços e avaliações dos apartamentos em algumas capitais brasileiras. O primeiro passo é fazer a importação das bibliotecas que utilizaremos neste projeto: Após a definição das cidades do buscador diretamente no site, podemos obter as URLs de cada uma das cidades das quais queremos coletar informações (colocaremos aqui as urls resumidas para não ocupar muito espaço): O próximo passo será clicar no botão “inspecionar” para visualizar no arquivo html do site e analisar sua estrutura. Posteriormente iremos selecionar um elemento da página para conseguir acessar de forma mais direta às classes e tags do arquivo html. Aqui, mostramos como exemplo o elemento que armazena o preço da diária da locação. Uma vez feito isso podemos visualizar o arquivo html de cada um dos itens de interesse. Também é importante inspecionar a tag do item para passar à próxima página para posteriormente coletar os dados de cada uma das páginas. Também utilizaremos a biblioteca BeautifulSoup e request para obter o arquivo html da url do site com a estrutura a seguir: O comando find permite obter o html de uma tag, assim utilizaremos este comando: O comando find_all permite obter uma lista do html de cada uma das tags onde esta tag se repete em conjunto com a class: Antes de construir as listas com as informações que iremos coletar, utilizaremos a função a seguir para obter uma lista com as URLs de cada uma das páginas das cidades. O comando .get(‘href’) permite obter uma string contendo a url de uma determinada tag e class, assim utilizaremos este comando para obter a url do item para passar à próxima página. Para deixar o código mais limpo, vamos tirar elementos do código que não são necessários e obter apenas segmentos do texto do html. Iremos utilizar os comandos .get(‘href’) para obter links presentes no html e .get_text() para obter strings. Além disso, iremos utilizar um for loop do Python para aplicar os comandos .get(‘href’) e .get_text() em cada um dos elementos da lista gerada pelo comando soup.find_all(tag, class) e desta forma criaremos listas de cada um dos elementos de interesse. Assim, a função a seguir permite obter uma lista com cada um dos títulos, links, preços e avaliações de cada uma das acomodações da cidade definida pelo URL do buscador e construir um dataset com as informações coletadas do site. E voilá! Um dataset novinho pra você poder analisar o que achar interessante! Se quiser entender um pouco melhor como funcionam as funções do BeautifulSoup, a documentação está disponível nas referências! Vamos agora visualizar estes dados rapidamente: Podemos ver com o gráfico acima que as cidades com mais locações baratas são Manaus e Belo Horizonte, e que as mais caras aparentam estar no Rio de janeiro, vamos analisar melhor esse comportamento olhando as médias desses valores. Aqui vemos que a média de preço de Manaus é maior que a de Belo Horizonte, isso se dá provavelmente por conta de termos locações com preços mais caros em Manaus, como pode ser visto na cauda do gráfico que fizemos anteriormente. Curiosamente, vendo a matriz de correlação acima podemos notar que não há correlação entre preços e avaliações, a maioria das locações tem avaliações altas independentemente de seu preço. Agora que extraímos dados de uma página usando BeautifulSoup, demos o primeiro passo para trabalharmos com dados contidos em websites. Entretanto, com web scraping, conseguimos ir muito mais longe em termos de fontes para extrairmos dados, até mesmo podemos extrair dados do Facebook, Instagram e outros sites com estrutura complexa. Mesmo sendo páginas dinâmicas e até mesmo se tiverem anti-bots, se Captchas aparecerem no meio do caminho para impedir extrações automatizadas e, enfim, muito mais. Porém, é claro que também nesses cenários se torna muito mais desafiador achar formas de extração dos dados, já que exige uma robustez muito maior do método de extração para contornar os obstáculos que aparecem. Também é necessário ter em mente que algumas barreiras são intransponíveis do ponto de vista de permissão de acesso a certos dados! Nos websites, há como acessar quase todo tipo de informação com a devida sabedoria, mas isso não significa que seja legal, então busque se informar a respeito daquilo que se tem ou não direito de acessar e extrair. Existem questões éticas que são importantes quando trabalhamos com web scraping! Tenha em mente que apesar de públicos, os dados extraídos não são de nossa autoria (em geral), portanto, é sempre interessante citar de onde estão vindo as informações e o que se pretende fazer (ou foi feito) com elas! Ademais, se tomarmos dados com muita frequência de um mesmo site podemos causar problemas em seu servidor, por isso, sempre devemos agir com cautela! Alguns sites até desenvolveram APIs para evitar essas inconveniências, disponibilizando apenas algumas informações e controlando o acesso ao servidor de seus sites. Web scraping pode ser feito de muitas formas: por meio de softwares e extensões de navegadores, por meio de scripts prontos que podem ser rodados diretamente ou por meio de scripts de produção própria, os scrapers podem vir com diferentes User Interfaces que podem trazer diferentes níveis de dificuldade e aplicações conforme o scraper e também podem ser rodados de forma local usando o próprio servidor ou de forma cloud-based, utilizando um servidor externo. Para além do exemplo que demos na prática, há muitas aplicações para as quais podemos usar web scraping! Aqui vão alguns exemplos: Praticando aos poucos, partindo de métodos mais simples de web scraping e subindo os níveis de dificuldade, muitas informações valiosas podem estar nas suas mãos para fazer análises cada vez mais poderosas. Esperamos que você tenha gostado e muito obrigado por chegar até aqui. Se quiserem conhecer um pouco mais sobre o que fazemos no Grupo Turing, não deixem de nos seguir nas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Para acompanhar também um pouco de nosso projetos, acesse nosso GitHub."
https://medium.com/turing-talks/projeto-presidenci%C3%A1veis-1cff43f4007b?source=collection_home---------5----------------------------,Projeto Presidenciáveis,,Luísa Mendes Heise,802,5,2022-09-25,"Usando tweets para medir a similaridade entre os candidatos Olá, caro leitor, bem-vindo a mais um Turing Talks. Com as eleições chegando, hoje vamos apresentar o mais novo projeto concluído do Turing USP: o projeto “Presidenciáveis”. A ideia principal do projeto era coletar tweets de candidatos à presidência da República e utilizar algum método para medir as similaridades entre os textos coletados, por tema abordado. Você pode ver o resultado final no site: http://www.presidenciaveis.turingusp.com.br/ (ou, alternativamente https://presidenciaveis-turing-usp.netlify.app/). Começando pela coleta de dados, nós utilizamos a API do Twitter para uso acadêmico, isso possibilitou a extração de tweets mais antigos dos candidatos. Para esse projeto, nós decidimos coletar tweets escritos desde 2012. O uso da API é relativamente simples e mais detalhes sobre ela podem ser encontrados em sua documentação. Com os tweets em mãos, começamos nossa análise: desde visualizações mais simples até, de fato, desenvolver a análise que possibilitaria a comparação dos discursos dos presidenciáveis. Método para comparação de discursos: Divisão dos temas O primeiro passo para iniciar a comparação de discursos por tema foi dividir os tweets em temas. A metodologia utilizada para isso foi simples: apenas delimitados alguns tema e, para cada um deles, foram definidas algumas palavras-chave: Com isso feito, essas palavras-chave foram procuradas nos tweets e, caso fossem achadas, isso implicava na classificação daquele tweet com o tema correspondente. Aqui, cabe ressaltar que um único tweet poderia ser classificado com mais de um tema. Cálculo das similaridades O cálculo das similaridades foi feito com base no uso de embeddings, mais especificamente, embeddings feitas pelo modelo BERT em português, o BERTimbau, disponível no repositório do HuggingFace. De forma muito genérica, um embedding é uma representação vetorial de uma palavra. Ou seja, uma palavra é traduzida em uma lista de números. Esses números por si só não têm um significado; uma interpretação só pode ser feita ao comparar dois embeddings ou mais entre eles. A forma como esses vetores são obtidos faz com que eles tenham a propriedade de capturar relações semânticas e sintáticas entre as palavras. Isso você pode entender melhor neste texto. Em específico, o modelo BERT é um dos modelos estado da arte para cálculo de embeddings em português, ele gera embeddings contextualizados, ou seja, uma mesma palavra pode ter representações vetoriais distintas a depender de seu contexto. Como já dito anteriormente, os embeddings são fundamentalmente representações de palavras. No entanto, podemos, a partir de vetores de palavras, extrapolar vetores para uma sentença, um texto ou até um conjunto de textos. Isso pode ser feito de muitas maneiras, desde uma concatenação até uma média. No nosso caso, um embedding de um tweet é a média dos embeddings das palavras contidas nele. E o embedding de um tema é a média dos embeddings do tweets que estão classificados com tal tema. Agora que temos nossas representações de temas por candidato, o que resta é comparar esses vetores e chegar à similaridade entre os discursos sobre determinado assunto. Para tal, utilizamos a similaridade de cossenos, que pode ser definida como um produto interno normalizado entre os vetores. Mais sobre o porquê de utilizar essa métrica pode ser lido no nosso texto sobre embeddings. Além disso, alguns ajustes foram feitos: para maior consistência do método, foram removidos candidatos em comparações cujo embedding do tema foi feito com o uso de menos de 10 tweets; também, para melhor visualização das diferenças entre os candidatos, uma vez calculadas as similaridades duas a duas, interpolamos esses valores entre 0 e 1, de forma que a menor similaridade correspondeu a 0 e a maior a 1. Vale ressaltar que, por mais que utilizemos técnicas de Inteligência Artificial para calcular esses valores, eles podem não condizer com a realidade, haja vista que utilizamos apenas palavras chaves escolhidas arbitrariamente para a seleção de tweets. Além disso, os tweets geralmente são administrados não apenas pelo presidenciável que o usa, o que não reflete integralmente sua opinião. Desta forma, algumas opiniões do próprio candidato podem não estar explícitas em seus tweets. Geração das visualizações Agora com as similaridades calculadas e interpoladas, elas foram colocadas em uma matriz que as expõe. Para a produção dos gráficos utilizados ao longo do projeto, usamos a biblioteca Plotly e, após os construirmos, geramos um arquivo html para adicionar ao nosso site. O site foi feito utilizando o framework ReactJS e optamos por carregar os gráficos em html como um iframe. Vale ressaltar que existe uma implementação de plotly compatível com React, em que podem ser passados jsons com informações sobre o gráfico como props. Esses jsons podem ser gerados em Python com o método .to_json() aplicado à figura. Com o código do site feito, o app netlify foi utilizado para o deploy do site. Por fim, deixamos aqui nossos agradecimentos a todos os membros do grupo que auxiliaram na construção do projeto: Rian Pereira Fernandes — rianpf@usp.br Luísa Mendes Heise — luisa.heise@usp.br Antonio Cunha Freire Júnior — antoniocunha@usp.br Daniel Scaramelli — danielscaramelli03@usp.br Daniel Frozi Brasil da Fonseca daniel.frozi.brasil@usp.br Felipe Azank dos Santos — felipeazank@usp.br Gustavo Azevedo Corrêa — guazco@usp.br Hugo R. V. Angulo — hugovangulo@gmail.com Igor Augusto — igor.augusto42@usp.br Isabella Soares Camareli — isacamareli@usp.br Iryna Miréia Nunes Azevedo — iryna.mireia@usp.br João Pedro de Freitas Gomes — joaopedrofg@usp.br Julio Manuel — juliomanuelpamplonaosorio@usp.br Kauã Fillipe — kauafillipe@usp.br Lilianne Nakazono — lilianne.nakazono@usp.br Lucas Leme Santos — lucaslssantos99@usp.br Maria Fernanda Fernandes Rezende — mariarezende1313@usp.br Noel Viscome Eliezer — noel.eliezer@usp.br Raul Granja — raul.granja@usp.br E por hoje era isso! Se você se interessa por Ciência de Dados, Processamento de Linguagem Natural ou qualquer outra vertente do ramo da Inteligência Artificial, não deixe de ler outros Turing Talks e nos acompanhar em nossas redes sociais, como Facebook, Linkedin, Instagram e, claro, nossos posts do Medium! Entre também em nosso servidor no Discord. Muito obrigado por chegar até aqui! Bons estudos e até! :)"
https://medium.com/turing-talks/introdu%C3%A7%C3%A3o-%C3%A0-feature-engineering-para-previs%C3%A3o-com-s%C3%A9ries-temporais-bf8bd3d0397d?source=collection_home---------4----------------------------,Introdução à Feature Engineering para Previsão com Séries Temporais,,Kauã Fillipe,660,12,2022-10-09,"Texto escrito por: Kauã Fillipe e Letícia Falconer Olá, leitor! Seja bem-vindo a mais um Turing Talks! No texto de hoje, trataremos sobre Feature Engineering para Séries Temporais, assunto bastante importante dentro da área de Ciência de Dados. A proposta deste texto é ser uma introdução ao entendimento de como se dá o processo de feature engineering no contexto das séries temporais. Mas, para isso, primeiramente, vamos definir o que são séries temporais, feature engineering e features de uma série temporal. Partindo disso, podemos entender melhor qual a relação entre esses dois conceitos. Para isso, seguiremos a seguinte estrutura: Vamos lá? Uma série temporal pode ser definida como um conjunto de observações relacionadas a uma variável disposto como uma sequência no tempo. Para exemplificar este conceito, vamos utilizar o dataset monthly-airline-passengers.csv, do Datasets repository, fornecido pelo jbrownlee. Nele, temos o registro do número de passageiros em uma linha aérea por mês, dado da seguinte forma: Veja que estamos trabalhando com um dataset que tem apenas uma variável e que os dados costumam ser registrados de maneira periódica. No nosso exemplo, como dito anteriormente, o registro é realizado considerando o intervalo de 1 mês. Abaixo, uma visualização dos nossos dados: E outro conceito extremamente importante para o texto de hoje é a Feature Engineering. Do que ela se trata e para qual propósito ela serve? Bom, ela é um processo de seleção dos dados originais do nosso dataset e transformação deles em novas features, as quais podem ser utilizadas no aprendizado de máquina, simplificando e melhorando a acurácia do nosso modelo. Um exemplo muito utilizado desse conceito é o de dados faltantes em um dataset e como lidar com eles, se o correto seria apagá-los ou reescrevê-los, e de que maneira fazer isso. Mas o exemplo do texto de hoje não é esse, e sim sobre manipulação de colunas para melhor aproveitamento dos nossos dados. Vejamos o dataset a seguir, Climate change dataset — Sheet1.csv, que contém coordenadas de cidades e se elas são afetadas pelas mudanças climáticas. Como podemos ver nas primeiras colunas do dataset, as suas coordenadas estão divididas em latitude e longitude, separadamente. Contudo, essas informações não nos são úteis por si só, mas sim como um conjunto, e existem maneiras melhores de representar esses dados. Mas para relacionarmos um dado com o outro, basta concatenarmos as coordenadas? Na verdade, existem maneiras mais fáceis de serem interpretadas pelo nosso modelo. Uma possível abordagem para solucionar esse problema seria transformar as colunas em coordenadas polares! Assim, vamos criar uma coluna que representa a distância da cidade em relação à coordenada zero, e outra com a inclinação, em graus. Agora que temos os conceitos iniciais bem introduzidos, podemos dar um passo à frente e discutir como tudo isso pode ser aplicado em nossa série temporal. Como dito anteriormente, podemos utilizar a Feature Engineering para criar colunas em nosso dataset que nos ajudem a fazer poderosas previsões sobre o futuro, mas por onde devemos começar? Apesar de existirem alguns métodos que podem nos ajudar a criar features que nos beneficiem, como as de DateTime, as LagTime e as Window Features, os quais serão explicados adiante, não existe uma fórmula que nos diga qual a melhor abordagem para chegarmos no resultado esperado. Pelo contrário, é extremamente importante que tenhamos noção do nosso objetivo final, e levemos em consideração os dados que temos disponíveis para criar features específicas para o nosso problema. Seria correto criar todas as features que conseguirmos, independente de nosso dataset e da previsão futura desejada? Seguramente não. Vejamos o dataset a seguir, cinemaTicket_Ref.csv, com informações a respeito de vendas de ingressos em diferentes cinemas. Primeiramente, podemos observar que existem 246 cinemas e 48 filmes diferentes em nosso dataset. Desejamos prever as demandas futuras de compras de ingresso, mas como existem diferentes localidades e diversos filmes, suas demandas variam muito, e não é interessante analisarmos todos esses dados simultaneamente. Conhecendo o dataset, devemos considerar o histórico de cada um desses cinemas separadamente, para podermos prever cada um deles futuramente. Tendo tudo isso em mente, qual o próximo passo a ser tomado? Bom, o correto seria analisarmos cada região de forma independente, verificando a maneira com que seu clima, seus feriados e até mesmo épocas do ano influenciam nas vendas de ingressos. E ainda, poderíamos colocar essas informações externas como novas features em nosso dataset, garantindo uma acurácia ainda melhor para nosso modelo. E isso tudo criando a quantidade mínima necessária de colunas novas em nosso dataset, evitando deixá-lo poluído. São as features que podem ser extraídas dos dados relacionados ao carimbo de data/hora. Nesse caso, temos horário, dia, mês, ano, etc. A partir disso, é possível determinar novas relações interessantes entre os dados e gerar insights importantes para a nossa análise de dados. As features de data e hora, ou time features, permitem modelar a dependência temporal, ou seja, se é possível fazer uma previsão a partir do tempo determinado em que o evento ocorreu. A forma como se extraem as features vai depender da qualidade do dataset com o qual se trabalha. Voltando ao dataset com a quantidade de passageiros em uma linha aérea por mês, por exemplo, veja que podemos extrair features de Ano e Mês da coluna Month. Como o tipo dessa coluna é string e suas informações não estão no formato Timestamp ou dateformat do Pandas, poderíamos obter essas duas novas colunas aplicando o método split para dividir as duas informações de tempo da coluna Month, da seguinte maneira: Com isso, poderíamos construir gráficos que nos informem sobre o comportamento da quantidade de passageiros ao longo dos meses dentro de cada ano: Uma ferramenta bastante útil nesse contexto é a função to_datetime, que transforma argumentos no formato datetime. Ao padronizar o formato de datas do nosso dataset, podemos utilizar uma maior gama de ferramentas disponíveis na biblioteca Pandas, o que nos permite criar outras visualizações, também. Com o auxílio de bibliotecas interativas como o Plotly, podemos continuar a construir gráficos bem interessantes nesse caso, como o seguinte: Nele, podemos ver que existe, de fato, uma relação entre o nosso alvo, a quantidade de passageiros e o tempo. Nesse gráfico, traçamos uma linha de tendência, disposta através da média móvel dos dados, que indica o comportamento geral da quantidade de passageiros ao longo do tempo. Assim, é possível identificar uma sazonalidade no seu comportamento, de tal modo que, independente do ano, nota-se um aumento significativo da quantidade de passageiros nos meses de julho e agosto em relação aos demais - muito provavelmente por conta do período de férias. Além disso, vale mencionar outras técnicas igualmente úteis. Como exemplo, vamos utilizar um novo dataset, da biblioteca Plotly, que representa o preço de 6 ações de empresas de tecnologia em um período de tempo. É possível extrair diretamente de objetos do tipo Timestamp features de Ano, Mês, Dia e Hora, da seguinte forma: Mas poderíamos ter aplicado para o caso dos passageiros, se quiséssemos. Com isso, seria possível construir gráficos com a quantidade de passageiros ao longo dos meses de cada ano novamente, e ter uma visualização melhor do que o caminho escolhido no início. Dessa vez, conferimos, com maior clareza, o aspecto sazonal dessa distribuição nos meses de um ano e, também, o aumento do número de passageiros que trafegam por essa companhia com o decorrer dos anos, o que pode indicar um sucesso na imagem da empresa. Observe: As features de data e hora também podem vir em outros formatos, como o binário, e serem extraídas por meio de outras estratégias. Fica a cargo do cientista de dados decidir qual a melhor abordagem. Essas são features muito importantes em uma série temporal, mas não as únicas. No próximo tópico, trataremos das lag time features. Utilizamos lag time features quando queremos estudar a dependência entre a nossa variável alvo e seus valores em instantes de tempo passado, os quais são chamados de lag. Essa abordagem é muito interessante em cenários como o da previsão de preços de ações de uma empresa. Nesse contexto, podemos tentar prever uma queda ou uma subida no preço de uma ação com base nos valores que ela assumiu em dias anteriores, por exemplo. Diferente das features de data e hora, as lag time features ajudam a determinar a dependência serial, ou seja, se uma observação dessa série temporal depende e pode ser predita a partir de uma observação anterior no tempo. Se utilizarmos lag 1, estamos adotando uma defasagem de 1 unidade de tempo - 1 dia ou 1 mês atrás, por exemplo. Lag 2 se refere a uma defasagem de 2 unidades de tempo, lag 3, a 3 unidades de tempo, lag 7, a 7 unidades de tempo. Assim, 2 dias atrás, 3 semanas atrás ou 7 meses atrás, respectivamente, dependendo da unidade de tempo com a qual se estiver trabalhando. Mas, antes de mais nada, precisamos tomar uma decisão: qual o período de atraso que devemos considerar? Usamos 1, 2 ou 8 dias para estudar essa relação? Para isso, existem algumas ferramentas que podem ser úteis para ajudar a determinar esse valor, como a autocorrelação. A autocorrelação de uma lag (uma defasagem) l pode ser definida como a correlação de todos os pares de observação (v_t, v_t-l) que estão separados por l unidades de tempo, em que v é a nossa variável alvo, e t é um instante de tempo. Essa medida nos informa o quanto a observação de nossa variável alvo em um instante de tempo se relaciona com suas observações passadas, dada uma defasagem l e, portanto, permite ter uma ideia de qual valor adotar para esse atraso. Podemos analisar a correlação por meio de uma função de autocorrelação (ACF), que pode ser representada graficamente através de um correlograma. Vamos voltar ao dataset de preços de ações de mercado utilizado anteriormente. Preste atenção que os registros dos preços são feitos a cada semana. Dessa forma, a unidade de tempo que vamos adotar para as lags é de semanas, e não dias, como citado antes. Podemos criar uma lag utilizando a função shift, indicando o tamanho da defasagem. Veja o exemplo abaixo, demonstrando como poderíamos obter lag 1 dos preços da empresa Amazon: Note que, devido à defasagem de 1 semana que estamos realizando, há 1 dado faltante na nossa lag feature. Se fizéssemos um shift(5), ou seja, uma defasagem de 5 unidades de tempo (5 semanas, nesse caso), teríamos 5 dados faltantes na coluna de lag feature, e assim por diante. Bom, foquemos na análise da empresa Netflix. Suponha que você queira predizer o preço das ações da Netflix com base no preço de semanas anteriores, mas não tenha certeza se é melhor predizer o preço com base no preço de 2, 3 ou 10 semanas atrás. Para ter mais segurança da sua escolha, você então utiliza a função plot_acf da biblioteca statsmodels em Python para criar um correlograma e estudar a autocorrelação do preço das ações dessa empresa. Perceba que a autocorrelação decai lentamente conforme o aumento do tamanho das lags e que, como esperado, o preço das ações da Netflix tem correlação com os preços da semana anterior mais do que com o preço de semanas mais distantes no tempo, o que faz bastante sentido. O gráfico abaixo confirma que, de fato, a melhor escolha é utilizar uma defasagem de 1 semana no seu modelo. Semelhante às Lag Features, utilizamos as Window Features para relacionar a nossa variável target com as features do passado. Contudo, não estamos mais falando em um instante único de tempo, mas sim um intervalo até o momento atual. Podemos pensar que ela é algo como um resumo de todas as Lag Features até o presente, podendo ser a média desses valores, seu mínimo, seu máximo ou qualquer outra estatística que desejemos utilizar. Ainda, dependendo se a janela escolhida for fixa ou móvel, as Window Features podem ser categorizadas em Rolling Window e Expanding Window Features. Rolling Window Features As Rolling Window Features, como diz no próprio nome, são janelas que se deslocam junto com a série temporal, mas permanecem com um tamanho fixo. A imagem abaixo demonstra esse processo com mais clareza: em laranja temos a janela, com um tamanho de 3, que percorre a série temporal, calculando a soma dos valores. Além disso, podemos perceber que os primeiros dois valores da nossa soma se encontram nulos. Isso ocorre, pois como escolhemos uma janela de tamanho 3, não haviam dados o suficiente nos dois primeiros instantes de tempo! E como aplicar esse conceito em nossa série temporal? No Pandas existe uma função chamada rolling(), que faz todo esse processo por nós. Para demonstrá-la, iremos utilizar o mesmo exemplo da foto acima, mas agora em código! Observe que a função rolling retorna as janelas com o tamanho desejado, de forma que tivemos que aplicar a função sum() nela para obtermos o resultado esperado. Dessa forma, também poderíamos utilizar qualquer outra estatística, como a média (popularmente conhecida como moving average) ou até mesmo o valor mínimo ou máximo da janela! Abaixo temos um exemplo de como ficaria essa Rolling Window utilizando a média: Expanding Window Features Já as Expanding Window Features, são janelas que não possuem tamanho fixo, de modo que incluem todos os dados anteriores ao atual. Na imagem abaixo podemos observar esse processo: em laranja temos a janela, com um tamanho variável, que calcula a média dos valores. Abaixo temos um exemplo de como utilizar as Expanding Window Features com python no dataset acima, para calcular a média, utilizando uma função do Pandas chamada expanding(): Além disso, assim como nas Rolling Window Features, também poderíamos utilizar qualquer outra estatística, como o valor mínimo ou máximo da janela! Abaixo temos outro exemplo, agora utilizando o valor máximo da janela: Lidar com séries temporais é uma das tarefas mais comuns e importantes dentro da área de Ciência de Dados, sendo um ramo com diversas aplicações em empresas. Antes de construir um modelo preditivo com séries temporais, é essencial garantir um bom processo de feature engineering, manipulando os dados do nosso dataset e assegurando que ele possua todas as informações necessárias para que seja feita a previsão e, com isso, melhore a performance do algoritmo usado. Podemos adaptar nossos dados de diversas maneiras, levando sempre em consideração a melhor abordagem para nosso problema específico. No Turing Talks de hoje, explicamos sobre as features de data e hora, lag time e window features, além de alguns métodos que podem ser usados para gerá-las. Criamos o mapa mental abaixo com os conceitos básicos para te ajudar a relembrar a teoria. Mas não se confunda, esses métodos não são uma regra! O tema de Feature Engineering vai muito além do contexto das séries temporais e é utilizado na abordagem de outros cenários de modelagem. Caso se interesse em saber mais sobre séries temporais e como manipulá-las utilizando a biblioteca Pandas, recomendamos a leitura do Turing Talks “Manipulação de Séries Temporais com Pandas”. Pode ser que você encontre alguns conceitos e funções que utilizamos aqui também! Esperamos que você tenha gostado, e muito obrigado por chegar até aqui. Se quiser conhecer um pouco mais sobre o que fazemos no Turing-USP, não deixe de nos seguir nas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Para acompanhar ainda mais de perto e participar de nossas discussões e eventos, entre no nosso servidor do Discord. Um abraço, e até breve!"
https://medium.com/turing-talks/teste-de-hip%C3%B3teses-o-que-significa-quais-s%C3%A3o-os-tipos-e-como-realizar-um-teste-de-hip%C3%B3teses-1decda841bcd?source=collection_home---------3----------------------------,"Teste de Hipóteses: o que significa, quais são os tipos e como realizar um teste de hipóteses",Texto escrito por Iryna Mireia e Raquel Ruiz.,Iryna Mireia,256,6,2022-10-18,"Olá, seja bem-vindo a mais uma edição do Turing Talks! Hoje, explicaremos conceitos introdutórios sobre o tema, respondendo sobre o que é um teste de hipótese e os conceitos estatísticos relacionados a ele, como hipótese nula, nível de significância e valor-p. Hipótese estatísticaPara entender o que seria um teste de hipóteses, ou teoria da decisão, primeiro precisamos saber a definição de hipótese na estatística. Ela se define como qualquer afirmação sobre uma medida numérica calculada a partir de uma amostra, como por exemplo, a média, proporção etc. Uma afirmação do tipo “a concentração sérica do colesterol difere em crianças” não é uma hipótese estatística. Para tal, a afirmação deve ser: “a diferença entre a média de concentração sérica do colesterol de crianças e adultos é nula”, pois menciona o valor do parâmetro. As hipóteses sempre serão tiradas de parâmetros populacionais, os quais os mais comuns a serem estimados são a média, o desvio padrão e a proporção. O que é um teste de hipóteses?Segundo Bussab & Morettin (2017), o objetivo de um teste de hipóteses é “fornecer uma metodologia que nos permita verificar se os dados amostrais trazem evidências que apoiem ou não uma hipótese (estatística) formulada”. Ele nos auxilia a tomar decisões sobre uma ou mais populações baseado na informação obtida da amostra, associado a um risco máximo de erro.Suponhamos que temos uma determinada população e, dentro dela, retiramos uma amostra com alguns elementos. Após a análise dessa amostra, criamos hipóteses sobre a população, ou seja, fazemos um processo de inferência estatística. Por exemplo, eu quero descobrir se a média de idade da população de uma determinada cidade é 35 anos. Dependendo da cidade e do tamanho dela, seria uma tarefa muito trabalhosa e demorada saber a idade de todos os seus habitantes em um determinado período do tempo. Com isso, retiramos uma amostra x de pessoas, recolhemos a idade de todos e descobrimos que a sua média de idade é 37 anos. Faria sentido dizer que a média de idade dessa cidade é 35 anos, sendo que na amostra deu 37? É justamente nesse tipo de situação que usaremos o teste de hipótese. Quando obtemos resultados através de uma amostra de alguma população e fazemos inferências sobre os valores obtidos, estamos sujeitos ao erro estatístico, ou seja, um erro previsto e calculado, pois nunca obteremos respostas perfeitas, com 0% de erro, afinal, não calculamos em cima do todo, mas sim da sua amostra. Como é feito o teste de hipóteses?A hipótese que queremos testar é chamada de hipótese nula, matematicamente representada por H0: θ = θ0. Em geral, ela é formulada a partir de dados pré-concebidos ou valores de referência historicamente aceitos. Já a hipótese alternativa, HA, é aquela que suspeitamos que seja verdadeira, e pode ser escrita de 3 formas: A formulação da hipótese alternativa irá depender do problema em questão e do conhecimento que se tem sobre ele. A primeira alternativa é a mais geral, e é chamada de bilateral, enquanto a segunda e a terceira são hipóteses unilaterais. Uma vez definidas as hipóteses, é coletada de uma amostra aleatória o que chamaremos de evidência do parâmetro, θobs, e essa evidência irá fornecer a informação que precisamos para rejeitar ou aceitar a hipótese nula. É aí que entra o tão conhecido valor-p, que mede quão forte é a evidência contra a hipótese nula. Formalmente, o valor-p é definido como a probabilidade de obtermos um valor tão ou mais extremo do que θobs, considerando que H0 seja verdadeira. Ou seja, ele pode ser entendido como a plausibilidade da hipótese nula frente aos dados que obtivemos: se o valor-p é “pequeno”, a evidência fornecida pelos dados é forte, e do contrário ela é fraca. Mas, afinal, o que define qual o valor limite que o valor-p deve ter para que possamos estar seguros em rejeitar a hipótese nula? Para discutir isso, precisamos definir um conceito muito importante: o nível de significância do teste, a partir da análise dos tipos de erro que podemos cometer. Existem dois tipos de erros no teste de hipóteses: Erro de tipo I: rejeitar a hipótese nula, quando ela é verdadeira (“falso positivo”)Erro do tipo II: não rejeitar a hipótese nula, quando ela é falsa (“falso negativo”) Assim, podemos definir duas quantidades, a partir da probabilidade de cometermos cada um desses erros: Vemos que quanto maior o valor de α, menor o valor de β, e vice-versa, então não é possível controlar essas probabilidades independentemente para que ambas sejam muito pequenas, e não cometamos erros. Por padrão, α é fixado, e este valor é o nosso nível de significância, que pode ser entendido como o risco que consideramos ser aceitável de cometer erros do tipo I. Em geral, o valor de α é adotado entre 1% e 10%, mas ele varia a depender do problema e do quão “sério” é cometer um erro do tipo I. Por fim, calculado o valor-p e definido o nível de significância, o que queremos é o seguinte: Se valor-p ≤ α: rejeitamos H0Se valor-p > α: não rejeitamos H0 Vamos interpretar essas afirmações. Dizer que valor-p ≤ α é dizer que a probabilidade de cometermos um falso positivo (α) é maior ou igual à probabilidade de a nossa evidência θobs ser observada em um cenário em que a hipótese nula é verdadeira, então, devemos rejeitar H0, dado o nosso limite α para cometermos erros do tipo I. Ou seja, se a hipótese nula fosse verdadeira, a evidência que obtivemos seria extremamente rara — ou pelo menos, tão rara quanto nosso limite α permite — , então é muito difícil “acreditar” em H0. A nossa evidência é forte, e aponta para um cenário em que a hipótese alternativa HA é verdadeira. No caso contrário, valor-p > α, a probabilidade de estarmos cometendo um falso positivo ao rejeitar a hipótese nula é menor do que a probabilidade de observarmos θobs caso a hipótese nula seja verdadeira, então é mais seguro não rejeitar H0 — a nossa evidência não se mostrou tão convincente assim! E ainda é preferível acreditar em um cenário em que a hipótese nula é verdadeira, e permitiu que a evidência fosse observada. Nesta edição do Turing Talks, você teve uma introdução aos conceitos principais de inferência estatística e pôde aprender como funciona um teste de hipóteses. Mas este é só o começo! Se você quer saber, na prática, como aplicar um teste de hipóteses em python, fique ligado no próximo Turing Talks dessa série. Nele, falaremos sobre os testes de hipóteses mais conhecidos: o Z-test e o T-test; e como implementá-los no seu código. Esperamos que você tenha gostado, e muito obrigado por chegar até aqui. Se quiser conhecer um pouco mais sobre o que fazemos no Turing USP, não deixe de nos seguir nas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Para acompanhar ainda mais de perto e participar de nossas discussões e eventos, entre no nosso servidor do Discord. Agradecimentos especiais para Felipe Azank, Lilianne e Kauã Fillipe. Referências“Estatística Básica” — Bussab & Morettin (2017)"
https://medium.com/turing-talks/web-scraping-em-p%C3%A1ginas-din%C3%A2micas-com-python-e-selenium-2839f5e5871d?source=collection_home---------2----------------------------,Web Scraping em páginas dinâmicas com Python e Selenium,,Renata Leite Leandro,401,19,2022-11-27,"Texto escrito por: César Augusto, Davi Félix, Renata Leite e Rodrigo Marcolin. Olá, caro leitor! Seja bem-vindo a mais um Turing Talks! No texto de hoje, abordaremos, novamente, o tópico Web Scraping. No entanto, dessa vez o nosso foco recairá sobre páginas dinâmicas e a utilização da biblioteca Selenium, complementando o texto publicado no dia 14/08, que apresentava páginas estáticas e a biblioteca Beautiful Soup. Para alcançar o objetivo proposto, este Turing Talks será estruturado da seguinte forma: Aqueles que já leram o nosso último Turing Talks sobre Web Scraping, sintam-se livres para pular a introdução e ir direto ao que interessa! Mas, caso você queira refrescar a memória ou ainda seja novo ao assunto, fique tranquilo: começaremos do começo. Vamos lá? A ideia por trás do tópico de hoje é simples: o Web Scraping consiste na “raspagem” de dados de uma página da web através da construção de um agente capaz de coletar, organizar e armazenar informações úteis, tudo de forma automatizada. Apesar de uma pessoa ser capaz de copiar manualmente dados interessantes de um site e colá-los numa planilha ou num documento qualquer para guardá-los, como talvez você mesmo, caro leitor, já tenha feito enquanto planejava uma viagem e comparava os preços e acomodações de diferentes hotéis, é óbvio que, quando precisamos realizar uma “raspagem” de sites em larga escala ou de forma rápida, é muito mais eficiente utilizar um web scraper, capaz de automatizar e simplificar esse processo que poderia ser bastante demorado. Assim, basicamente, o Web Scraping serve para coletar um grande volume de informações num curto período de tempo, cometendo menos erros e sendo mais eficiente do que um humano poderia ser. Portanto, podemos observar que essa é uma ferramenta bem poderosa, sendo, por isso, usada por empresas de mídia para verificar quais assuntos estão em alta no momento, por sites de busca que comparam automaticamente o preço de remédios ou outros produtos, por empresas de marketing, de comércio eletrônico, de agências de viagem… Enfim, os usos são muito variados. Mesmo os serviços Google têm o seu próprio indexador, o Googlebot, que visita as páginas de internet existentes, reunindo e indexando as informações ao banco de dados do buscador. É assim que o Google vai saber quais páginas mostrar ao usuário de acordo com o que foi escrito na barrinha de pesquisa! Interessante, não? Agora que já definimos o Web Scraping e mostramos também alguns de seus possíveis usos, podemos introduzir o segundo termo-chave presente no título deste texto, que é a ideia de páginas dinâmicas. Como dissemos anteriormente, já publicamos um texto sobre Web Scraping de páginas estáticas. Mas, afinal, qual a diferença entre esses dois tipos? Tal classificação diz respeito ao modo de disponibilidade do conteúdo do site. Funciona assim: quando você acessa um site, basicamente o servidor recebe sua requisição e envia o conteúdo. Nas páginas estáticas, o conteúdo já vem pronto do próprio servidor, não havendo novas requisições a partir das interações do usuário dentro de uma mesma página. Enquanto isso, em páginas dinâmicas, o conteúdo muda a partir de novas requisições ao servidor quando o usuário interage com a página de alguma maneira (por exemplo, um botão apertado pode fazer com que novos dados sejam “buscados” no servidor e apareçam na tela). Isso porque o arquivo não está simplesmente pronto em HTML, precisando de todo um trabalho de acessar o banco de dados para montá-lo. No Web Scraping, essa diferença tem consequências importantes: se as páginas estáticas podem ser raspadas de maneira simples (acessamos, baixamos seu HTML e usamos ferramentas como BeautifulSoup para coletar os dados), as dinâmicas exigem um arsenal diferente de ferramentas, já que só o BeautifulSoup, por exemplo, não permite simular essa interação humano-computador com a página web. Portanto, faz-se necessária uma ferramenta que permita automatizar essa interação com os navegadores. Muito falamos das diversas vantagens do Web Scraping, mas devemos reconhecer também os seus pontos negativos — e, até mesmo, os seus perigos. Infelizmente, esse tipo de coleta de dados não interessa apenas àqueles que querem estudar e pesquisar sobre um determinado assunto ou à companhias que pretendem utilizar os dados coletados de uma forma ética e honesta, mas também a empresas que não veem limites para o aumento dos lucros, mesmo que isso signifique a ultrapassagem de certas barreiras que deveriam garantir a privacidade e a segurança de usuários de internet. Hoje em dia, é fato que, para praticamente qualquer organização, ter acesso a um grande volume de dados significa ter vantagens, seja conseguindo um lucro maior, uma melhor estratégia de negócio ou uma maior eficiência. Com isso em mente, já é esperado, portanto, que o Web Scraping seja uma prática bastante comum dentro dos mais diversos projetos; entretanto, é essencial refletir sobre todas as questões éticas envolvidas, por exemplo, quando uma empresa coleta dados de sua concorrente para decidir quais produtos colocará em promoção, obtendo maiores ganhos financeiros que sua adversária — questão que pode ser, inclusive, ainda mais grave quando há o desrespeito dos códigos de conduta de proteção de dados, elaborados para proteger, por exemplo, informações pessoais ou sensíveis desse tipo de atitude. É importante lembrarmos que, quando fazemos Web Scraping, estamos programando um robô para extrair da internet informações que julgamos pertinentes. Então, antes de acessar e tomar algum dado, pode ser interessante (e ético) confirmar quais são os dados que o proprietário do site pede para que não acessemos. Mas, caso você esteja se perguntando como fazer essa verificação, não tema! Existe uma ferramenta bastante útil para isso: Podendo ser acessado pela simples inclusão de“/robots.txt” ao fim do URL desejado (exemplo: “www.youtube.com/robots.txt”), esse arquivo tem por finalidade excluir dos buscadores páginas que não deveriam ser expostas para o público geral ao se pesquisar por aquele site, tais como arquivos essenciais para o bom funcionamento do site ou imagens e gráficos autorais. Mais do que isso, porém, essa ferramenta também contribui para regular o fluxo de visitas, buscando impedir um acesso frequente de robôs a partes internas importantes para o funcionamento do site, o que poderia acarretar uma sobrecarga do servidor e derrubar a página. Embora o arquivo robots.txt não seja taxativo — ou seja, o seu conteúdo não é uma ordem, mas apenas uma recomendação — , é aconselhável não acessar os arquivos que o proprietário não deseja que sejam acessados. Por essa razão, pode ser frustrante tentar fazer Web Scraping de uma dessas páginas “proibidas”, pois, provavelmente, o bot do programa que você está usando irá se negar a continuar com o acesso, em respeito às restrições impostas naquele domínio. Já explicamos o que é o robots.txt, mas, para realizar um Web Scraping de sucesso, precisamos ainda de muitas outras ferramentas. Buscaremos explicá-las neste tópico. Primeiramente, tratando-se de uma página dinâmica, é necessário escolher um caminho: podemos buscar o conteúdo do site diretamente em sua programação em JavaScript, o que só é possível sabendo como o site organiza as informações que estão contidas nele, ou podemos programar um bot para fazer isso por nós. Vamos começar pela primeira opção: Se você optar por buscar o conteúdo no JavaScript, então o AJAX será a ferramenta que tornará isso possível. AJAX é um acrônimo para Asynchronous JavaScript And XML. Basicamente, o AJAX não é uma linguagem de programação, mas sim uma técnica que torna possível uma atualização dessincronizada entre o navegador do usuário, que tenta acessar a página na web, e o servidor em que esta página está hospedada. Assim, conforme o usuário navega no site, novas informações são inseridas na página que está sendo mostrada em seu dispositivo, que poderá ser constantemente atualizada sem que o usuário precise recarregá-la. Um exemplo bastante elucidativo de uso do AJAX são as mídias sociais com o scroll infinito, em que é possível rolar a tela infinitamente por conta da alimentação incessante de conteúdo. O que seria o final da página, portanto, jamais é atingido. Utilizando o AJAX, faz-se uma engenharia reversa do JavaScript. Buscamos pela resposta enviada pelo servidor para a requisição feita a partir do dispositivo do usuário. Geralmente, essa resposta está contida em um arquivo JSON. Esse arquivo, então, é lido com o uso de Python e transcrito em um arquivo de texto, a partir do qual, enfim, será possível obter os dados. Pelo método aqui descrito, a “raspagem” da página torna-se bem mais complicada, demandando, por isso, um maior nível de dedicação e conhecimento para ser realizada. Agora, se você optar por utilizar um bot para fazer a coleta de dados das páginas, sorria, pois há várias opções que podem te auxiliar nesse processo! Antes de tudo, porém, precisamos esclarecer que as ferramentas selecionadas serão utilizadas em Python. Você, então, poderá se questionar: mas, por que Python? Para fazer Web Scraping, existem várias opções de linguagem de programação disponíveis. Entretanto, a depender da escolha que o programador faça, a tarefa de obter os dados da internet pode acabar se tornando ainda mais complicada do que já é. Dessa forma, levando em conta a presença de diversas bibliotecas em Python que permitem a automatização da raspagem dos dados, mediante a criação de bots que imitam o comportamento do usuário, por exemplo, optamos por fazer a raspagem de páginas da web a partir dessa linguagem. Além disso, o Python é bastante versátil, contando com ferramentas como a biblioteca Pandas para a análise e, principalmente, para a organização dos dados obtidos da internet. Dessa maneira, além de facilitar e agilizar a criação e execução de um bot scraper, optar pelo uso do Python possibilita ainda a unificação de todo o trabalho em uma única plataforma. Observe algumas das ferramentas para fazer Web Scraping de forma automatizada utilizando o Python: Finalmente, chegamos ao que é, sem dúvida, um dos artifícios mais famosos em seu ramo: o Selenium. Com esta ferramenta opensource, podemos não apenas automatizar as tarefas realizáveis em um navegador, como também realizar todas as demais funções exercidas pelos programas anteriores, isto é: se conectar a uma página na web, receber os seus dados, traduzi-los, interagir com a página, etc. Entretanto, o Selenium faz uso de uma tecnologia um pouco diferente para isso. Com a API WebDriver, conectada ao servidor do Selenium, é possível reproduzir todas as ações praticadas por um usuário humano, com a vantagem de poder reproduzi-las em múltiplos navegadores ao mesmo tempo! Para realizar essas tarefas, o WebDriver faz uso das ferramentas dos navegadores que já estão disponíveis no dispositivo do usuário. Então, basicamente, a API intermedeia a comunicação do dispositivo com o servidor, tomando, ainda que momentaneamente, o papel do navegador. O script escrito pelo usuário gera uma requisição que, convertida para o formato JSON, chega ao Selenium, sendo, então, repassada ao servidor como se fosse uma requisição gerada pelo próprio usuário durante a navegação. Essa requisição é recebida e tratada no servidor, retornando como ações a serem realizadas no navegador. Quando essas ações são realizadas, o navegador avisa o servidor, que retorna, ao Selenium, um arquivo com o resultado daquela execução. Essa troca de arquivos é que caracteriza a navegação na web. Todos esses passos, é necessário ressaltar, ocorrem em back-end. Contudo, o usuário pode visualizar a interação do bot com as páginas vistas como se estivesse observando alguém navegando em seu computador. Por utilizar o próprio navegador, o WebDriver torna o processo de interação com as páginas muito mais rápido e natural, de forma bastante similar ao que o próprio usuário faria, visto que a API consegue simular comportamentos típicos como duplos cliques, preenchimento de campos, ações de arrastar e soltar, cliques comuns e vários outros procedimentos. É por estas razões que optamos por utilizar o Selenium. Para aqueles que estão ávidos por utilizar esta poderosa ferramenta, incluímos também um pequeno tutorial de instalação. Por sorte, o Selenium pode ser acessado como qualquer outra biblioteca do Python, a partir do gerenciador de pacotes pip. Basta digitar o comando pip install selenium. No entanto, como já explicado, ainda precisamos de um webdriver para fazê-lo funcionar, e sua instalação pode ser feita de duas principais formas diferentes. A maneira mais recorrente para a obtenção de um webdriver seria a instalação da versão específica para o seu computador de modo que ele esteja disponível em linha de comando (no PATH do sistema). Por ser uma instalação mais complexa, usaremos uma forma alternativa a partir da biblioteca webdriver-manager, que também é instalada a partir do gerenciador de pacotes do Python, o pip. Para isso, utilize o comando pip install webdriver-manager. O uso dessa biblioteca permite mais versatilidade no versionamento dos webdrivers e a instalação pode ser feita dentro do código, sem a necessidade de alterar o PATH do seu sistema. No nosso exemplo prático, usaremos o chromedriver, correspondente ao navegador Google Chrome. No entanto, existem drivers para outros navegadores, como o geckodriver para o caso do Firefox. Usando o webdriver-manager, a instalação é feita a partir das seguintes linhas de código, como fizemos em nosso projeto: E chegamos a tão aguardada demonstração prática! Para aplicar os conhecimentos sobre Web Scraping, fizemos um projeto relacionado a previsão do tempo, tendo como base o site Climatempo. Vamos, agora, explicar os objetivos de tal proposta e sua implementação. Caso queira ver o projeto em detalhes, é possível acessar o repositório com o nosso código. Tomamos como objetivo a extração de previsões de temperatura para diferentes locais a fim de que pudéssemos comparar essas previsões com a temperatura real do dia. Para isso, utilizamos, por alguns dias, Web Scraping para retirar do site do Climatempo a previsão hora a hora para o dia atual — que consideramos como a temperatura correta— e para os próximos dois dias. Tais informações foram armazenadas em um banco de dados relacional SQLite. Com os dados em mãos, fizemos uma análise para determinar os acertos e inconsistências nas previsões de temperaturas. Nos próximos tópicos, iremos detalhar as principais etapas do projeto. No mundo da ciência e da engenharia de dados, o Web Scraping é uma das ferramentas que nos permitem extrair informações úteis de websites. Entretanto, por si só, sua utilidade acaba sendo limitada. Imagine que tenhamos um algoritmo de Web Scraping que acessa um site e extrai informações para uma determinada finalidade (ex. montar visualizações, treinar um modelo, etc). Se não armazenarmos esses dados em algum local, teremos que rodar o algoritmo para coletá-los toda vez que necessitarmos deles. Além disso, imagine que o conteúdo de um site é atualizado a cada dia. Caso não armazenemos os dados, teremos acessos somente àqueles extraídos no dia da execução do algoritmo, o que limitaria nossas análises. Portanto, o verdadeiro poder do Web Scraping é desbloqueado com sua utilização junto a um banco de dados. Nesse cenário, o robozinho do scraping é responsável, também, por trazer os dados e armazená-los no banco, onde eles ficarão disponíveis para consulta e manipulação sempre que quisermos. Há alguns tipos de banco de dados, como os relacionais (SQL) e não relacionais (NoSQL), ambos com infinitas utilidades. Mas esse é um tema para um outro TuringTalks! Neste, focaremos na utilização de um banco no contexto da nossa aplicação de Web Scraping. A ideia é que, durante um determinado período de tempo, extraíamos diariamente as temperaturas daquela data e as previsões para os dois próximos dias, armazenando essas informações no banco para que, posteriormente, pudéssemos comparar as temperaturas previstas com as medidas consideradas corretas. O banco de dados utilizado foi o SQLite, que é relacional e, apesar de ser bem simples e não escalável, é extremamente útil para uma grande variedade de aplicações. O Python nativamente possui uma biblioteca para interação com bancos SQLite desde sua versão 2.5. Para podermos tirar proveito disso, é necessário entender que, em um banco relacional, as informações são armazenadas em estruturas parecidas com tabelas, onde cada coluna representa um atributo e cada linha, um objeto. Tome, por exemplo, a seguinte imagem: Ela representa a seguinte tabela: Ademais, as tabelas podem relacionar-se umas com as outras. Por exemplo, observe a imagem abaixo, que representa as tabelas que criamos para esse projeto, bem como o relacionamento entre elas: As tabelas todas possuem um atributo chamado “id”, que é um identificador único de cada entrada. Não pode haver duas linhas com o mesmo id. Além disso, cada acesso (tabela central) é definido pela data em que ocorre (atributo data_acesso), pela data da qual prevê as temperaturas (atributo data_previsao), e pelo local das previsões (atributo id_local). Um local é, por si só, uma tabela (à direita), e cada local possui um nome e uma url. Assim, as tabelas “acesso” e “local” possuem um relacionamento entre si, pois cada acesso tem somente 1 local, mas cada local pode pertencer a vários acessos. A tabela acesso faz referência a qual local ela possui através do id do local em questão. Analogamente, cada previsão (tabela à esquerda) pertence a um e somente um acesso, mas cada acesso pode possuir mais de uma previsão, que é definida pelo horário em que ocorre e a temperatura que prevê. A explicação da razão pela qual armazenamos os dados dessa maneira e não simplesmente em uma tabela única é extensa e não cabe nesse TuringTalks. Porém, munidos com esse entendimento inicial do que está acontecendo no banco, fica fácil entender o que o robô coletor de dados faz! Primeiro, definimos quais são os locais cujas previsões iríamos analisar a fim de armazenar, no banco de dados, o nome da cidade e sua respectiva URL no Climatempo, sendo que novos locais podem ser adicionados a partir de um arquivo .csv por meio de uma função que criamos. Segue, na imagem, a página correspondente à cidade de São Paulo. No entanto, os gráficos que interessam para o projeto estão na página de previsão para 15 dias. Então, para acessá-la, precisamos interagir dinamicamente com o página, o que evidencia a necessidade de utilizar o Selenium. Com tal ferramenta, navegamos para essa página por meio do botão “15 dias” e encontramos os gráficos correspondentes ao dia atual e aos próximos dois dias, todos seguindo o mesmo formado da seguinte imagem: Para encontrarmos o código fonte do gráficos e obtermos os dados mostrados por ele, usamos a ferramenta de inspecionar elemento, conforme a próxima imagem: Isso nos levou a uma tag html div contendo um atributo chamado data-infos, sendo que o valor desse atributo corresponde aos dados mostrados no gráfico. Convertendo esse valor para JSON, obtivemos uma lista que contém um item para cada hora do dia (das 00h às 23h) junto com a temperatura correspondente prevista para cada um desses horários. Esses dados foram encontrados na seguinte forma: Os dados para o gráfico do dia atual estão na div de classe wrapper-char-1, enquanto os dados para os próximos dois dias estão na div de classe wrapper-chart-2e wrapper-chart-3. Com essas informações, temos tudo que precisamos para realizar a “raspagem” de dados. Em resumo, deve-se seguir o seguinte roteiro: Para analisar os dados colhidos a partir da raspagem, exportamos os dados do banco para um arquivo .csv. Finalmente, chegou a hora de analisar os frutos de todo esse trabalho! Recordando, retiramos dados de previsão e dados que consideramos como as medições da temperatura para aquele dia do site Climatempo. Com isso em mãos, foi possível, finalmente, realizar a comparação que era nosso objetivo desde o início. Foram escolhidas duas cidades, São Paulo e Salvador, das quais iríamos retirar esses dados. Após definidas as cidades, reunimos a previsão de dois dias antes e um dia antes de uma certa data e a temperatura registrada naquele mesmo dia. Então, por exemplo, para o dia 30 de agosto, obtivemos as previsões do dia 29, dois dias antes, e do dia 28, um dia antes. Então comparamos com a temperatura que o site informava para o dia 30 no próprio dia 30 de agosto e consideramos essa temperatura como a medição mais correta. Então, essas diversas previsões foram dispostas em gráficos para que pudéssemos analisar visualmente o comportamento da previsão conforme os dias passavam. Além disso, calculamos a temperatura média em cada uma das previsões e as comparamos para obter uma espécie de erro médio entre as previsões, ou seja, as temperaturas antes do dia, e a medição, a temperatura no dia. Os resultados vocês podem ver a seguir. Começaremos por São Paulo: Pela análise do gráfico acima, notamos que, de modo geral, a temperatura correta do dia 30 de agosto foi maior do que aquela que fora prevista 2 dias antes para a data, enquanto a realizada 1 dia antes é quase completamente correta. Assim, temos que o desvio médio entre a previsão de dois dias antes e a temperatura medida foi de 0.05%; já o desvio médio entre a previsão de um dia antes e a temperatura medida foi de 0.04%. Agora, vemos que as curvas referentes aos dias 30 (previsão) e 31 (medição) de agosto estão perfeitamente sobrepostas. Já a do dia 29 diverge da temperatura medida no dia 31, pois, de modo geral, esperava-se que o último dia do mês seria mais frio do que efetivamente foi. Portanto, calculamos que o desvio médio entre a previsão de dois dias antes e a temperatura medida foi de 0.06%; enquanto o desvio médio entre a previsão de um dia antes e a temperatura medida foi de 0.00%. Para o dia 1º de setembro, as previsões mantiveram uma coerência entre elas, pois ambas as curvas referentes às previsões realizadas nos dias anteriores a tal data estão sobrepostas. A temperatura registrada, no entanto, foi superior à esperada, em geral. Encontramos, assim, que o desvio médio entre ambas as previsões e a temperatura medida foi de 0.04%. As situações observadas em São Paulo foram, de certa forma, bastante semelhantes. Vamos analisar, então, se Salvador mantém um padrão de erro semelhante: O gráfico representado acima, que compara as diferentes previsões para o dia 30 de agosto em Salvador, apresenta um comportamento inédito nesta análise: tanto as previsões quanto a medição são exatamente iguais. Portanto, para esse dia, temos que o desvio médio entre ambas as previsões e a temperatura medida foi de 0.00% — ou seja, 100% de acerto! No dia 31 de agosto , porém, não tivemos a mesma sorte. Pelas previsões, era esperado que o dia 31 fosse bem mais quente do que efetivamente foi, com diferenças de até 4 graus. Calculamos, portanto, que o desvio médio entre a previsão de dois dias antes e a temperatura medida foi de -0.04%; e o desvio médio entre a previsão de um dia antes e a temperatura medida foi, também, de -0.04%. Ao analisarmos as informações do dia 1º de setembro, notamos que a previsão realizada dois dias antes, isto é, no dia 30 de agosto, foi bem mais assertiva que aquela realizada um dia antes. Por isso, vemos que o desvio médio entre a previsão de dois dias antes e a temperatura medida foi de 0.01%; já o desvio médio entre a previsão de um dia antes e a temperatura medida foi de 0.03%. Ufa! Percorremos um longo caminho! Explicadas as ferramentas utilizadas, a forma como fizemos o Web Scraping e o objetivo dessa raspagem de dados, podemos também tirar algumas conclusões a partir desses dados extraídos e analisados — afinal, para que coletaríamos dados, se não para usá-los? Apesar de termos sido bastante minuciosos nas checagens de temperatura, devemos levar em conta que, na maior parte dos casos, as previsões feitas com pelo menos dois dias de antecedência apresentam desvios menores do que 1%. Portanto, podemos considerar que as previsões do clima tempo são, em geral, capazes de refletir com grande exatidão as condições climáticas para um dado dia. Agora você já sabe, né? Quando estiver em dúvida sobre qual roupa escolher para a sua viagem daqui a um ou dois dias, cheque no Climatempo! Agora, se a sua viagem for apenas em 2 meses, que tal nos ajudar a aprofundar este pequeno projetinho introdutório sobre Web Scraping com uma raspagem de dados mais completa, que contemple também as necessidades dos mais ansiosos e curiosos para saber até onde podemos confiar nas previsões climáticas? Esperamos que você tenha gostado, e agradecemos por chegar até aqui! Se quiser conhecer um pouco mais sobre o que fazemos no Turing USP, não deixe de nos seguir nas redes sociais: Facebook, Instagram, LinkedIn e, claro, ler nossos posts no Medium. E se preferir acompanhar ainda mais de perto e participar de nossas discussões e eventos, você também pode entrar no nosso servidor do Discord. Um abraço, e até a próxima!"
https://medium.com/turing-talks/ambientes-virtuais-em-python-60924a4bf4f?source=collection_home---------1----------------------------,Ambientes virtuais em Python,,Kauã Fillipe,351,6,2022-12-04,"Olá, querido leitor! Seja bem-vindo a mais um Turing Talks! Imagine que você está trabalhando simultaneamente em vários projetos de programação, seja na sua empresa, grupo de extensão universitária ou uma iniciativa própria, utilizando a linguagem Python. Cada projeto possui suas próprias dependências, ou seja, cada um deles utiliza diferentes bibliotecas do Python ou ainda emprega diferentes versões de uma mesma biblioteca. Nesse caso, não é nada prático instalar uma versão X de uma biblioteca para desenvolver um dos projetos e, depois, atualizá-la, repetidamente, para outras versões Y, Z, etc., que devem ser utilizadas em outros trabalhos. Sabendo que alguns projetos são elaborados para a versão X da dependência e, desse modo, não conseguem ser rodados utilizando as versões Y, Z, etc., você percebe que não dá pra utilizar apenas uma versão dessa biblioteca para todos eles a não ser que adapte cada código para essa versão. Além disso, as dependências de um projeto costumam ser atualizadas frequentemente, gerando, assim, sempre novas versões disponíveis de bibliotecas terceiras e superiores a quaisquer outras versões que você possa estar usando no seu trabalho agora. Com isso, você descarta a possibilidade anterior de usar apenas uma versão das bibliotecas para todos seus projetos. Teria, então, alguma ferramenta que possibilite utilizar bibliotecas distintas e diferentes versões de uma mesma em vários projetos do seu trabalho? Para sua felicidade, sim! Os ambientes virtuais existem justamente para isso. Nesse texto, trataremos brevemente sobre o que é um ambiente virtual do venv, o principal módulo do Python para criar ambientes virtuais, e de como utilizá-lo em seus projetos. O que é um ambiente virtual? Um ambiente virtual é um ambiente do Python utilizado para controle de dependências e isolamento de projetos. Nesse sentido, ele possui seu próprio conjunto de dependências e seu próprio binário Python (uma cópia da versão do Python utilizada para criar o ambiente virtual). Dessa forma, as bibliotecas instaladas nele estão isoladas tanto das bibliotecas instaladas na versão do Python do sistema como das instaladas em outros ambientes virtuais. Isso ocorre porque a instalação é feita localmente em um diretório próprio do ambiente virtual utilizado e não globalmente no sistema (como aconteceria se você instalasse bibliotecas sem utilizar essa ferramenta). Portanto, cada ambiente virtual existe para uma versão específica do Python e para versões específicas das bibliotecas instaladas nele. Isso resolve a situação apresentada anteriormente, pois permite que vários projetos em Python utilizem versões diferentes dessa linguagem de programação e de pacotes instalados, sem que haja conflito, desde que cada projeto utilize um determinado ambiente virtual com suas respectivas especificações. Vale ressaltar que o que é feito em um determinado ambiente virtual não afeta outros ambientes virtuais. Assim, você pode atualizar a versão de uma biblioteca, se for necessário, em um certo projeto, sem necessariamente atualizar a versão utilizada da mesma biblioteca em outros projetos. Criando um ambiente virtual com o módulo venv O principal módulo utilizado na criação de ambientes virtuais em Python é o venv (mas, claro, você pode utilizar outros se preferir). Ele permite que essa tarefa seja feita de modo fácil e prático e já vem instalado com o Python. Para criar um ambiente virtual, basta fazer o seguinte: Isso deve ser feito dentro do diretório do seu projeto e você pode nomear seu ambiente virtual da forma como quiser, mas, geralmente, utiliza-se ven ou .ven como nomes. Dessa forma: É possível ver que, dentro do diretório do ambiente virtual criado, há algumas pastas e um arquivo: Em sistemas operacionais Linux e MacOS, existe ainda a pasta bin, que contém os arquivos executáveis (como o do pip e o interpretador do Python) e scripts de ativação do seu ambiente virtual. Alguns sistemas Linux podem ainda conter lib64, usada para instalar versões diferentes de bibliotecas com base em suas arquiteturas. Depois da criação do ambiente virtual, é necessário ativá-lo. Para isso, basta: Para o Power Shell, certifique-se de que a execução de scripts está habilitada no seu sistema. Do contrário, não será possível ativar o ambiente virtual, devido às suas políticas de segurança. No nosso caso, teríamos: Para sair do ambiente virtual, basta desativá-lo com o comando deactivate. Arquivo requirements.txt Após a ativação, todas as instalações de pacotes serão realizadas no ambiente virtual ativado e as bibliotecas ficarão armazenadas na pasta Lib. A instalação das dependências pode ser feita utilizando o sistema pip. Em grandes projetos, principalmente se forem colaborativos, é interessante ter uma documentação das dependências do projeto para que os demais membros envolvidos, por exemplo, consigam reproduzir seu trabalho utilizando as bibliotecas necessárias nas versões específicas para as quais você o desenvolveu. Essa situação é resolvida pelo arquivo requirements.txt, o qual lista as dependências de um projeto e as versões específicas de cada biblioteca utilizadas nele. Para gerar esse arquivo, uma possibilidade é, dentro do ambiente virtual, digitar o seguinte comando: E, para instalar os pacotes necessários a um projeto, a partir de um arquivo requirements.txt: A criação de um ambiente virtual é fortemente recomendada quando se está trabalhando em um projeto no qual se programa usando Python. Ela resolve questões que dariam muita dor de cabeça se esse recurso não fosse empregado. Um ambiente virtual permite isolar versões específicas das dependências de um projeto, inclusive a versão do Python utilizada, sem afetar os pacotes globais do sistema ou as bibliotecas utilizadas em outros projetos. Isso é extremamente positivo uma vez que possibilita um maior controle das configurações do seu ambiente Python e facilita o desenvolvimento de projetos colaborativos. Nesse sentido, a criação de um arquivo requirements.txt é igualmente sugerida, pois permite a reprodução do projeto da forma como foi desenvolvido por outros colaboradores sem muito esforço. Por hoje é só! Espero que você tenha gostado. Se quiser conhecer um pouco mais sobre o que fazemos no Turing-USP, não deixe de nos seguir nas redes sociais: Facebook, Instagram, LinkedIn e, claro, acompanhar nossos posts no Medium. Para acompanhar ainda mais de perto e participar de nossas discussões e eventos, entre no nosso servidor do Discord. Um abraço, e até mais!"
